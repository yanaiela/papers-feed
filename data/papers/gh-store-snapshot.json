{
  "snapshot_time": "2025-11-26T14:51:12.242765+00:00",
  "repository": "yanaiela/papers-feed",
  "objects": {
    "interactions:arxiv.2411.00640": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.00640",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-10T23:48:26.492Z",
            "data": {
              "session_id": "session_1749599306052_fap3gto",
              "source_id": "arxiv",
              "paper_id": "2411.00640",
              "start_time": "2025-06-10T23:48:18.158Z",
              "end_time": "2025-06-10T23:48:26.052Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:33:25.086Z",
            "data": {
              "session_id": "session_1750977204673_9k6mbqz",
              "source_id": "arxiv",
              "paper_id": "2411.00640",
              "start_time": "2025-06-26T22:33:18.549Z",
              "end_time": "2025-06-26T22:33:24.673Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 4,
        "object_id": "interactions:arxiv.2411.00640",
        "created_at": "2025-06-10T23:48:27+00:00",
        "updated_at": "2025-06-26T22:33:51+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.00640": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.00640",
        "url": "https://arxiv.org/pdf/2411.00640",
        "title": "Adding Error Bars to Evals: A Statistical Approach to Language Model\n  Evaluations",
        "authors": "Evan Miller",
        "abstract": "Evaluations are critical for understanding the capabilities of large language\nmodels (LLMs). Fundamentally, evaluations are experiments; but the literature\non evaluations has largely ignored the literature from other sciences on\nexperiment analysis and planning. This article shows researchers with some\ntraining in statistics how to think about and analyze data from language model\nevaluations. Conceptualizing evaluation questions as having been drawn from an\nunseen super-population, we present formulas for analyzing evaluation data,\nmeasuring differences between two models, and planning an evaluation\nexperiment. We make a number of specific recommendations for running language\nmodel evaluations and reporting experiment results in a way that minimizes\nstatistical noise and maximizes informativeness.",
        "timestamp": "2025-06-10T23:48:18.338Z",
        "rating": "novote",
        "publishedDate": "2024-11-01T14:57:16Z",
        "tags": [
          "stat.AP",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 3,
        "object_id": "paper:arxiv.2411.00640",
        "created_at": "2025-06-10T23:48:18+00:00",
        "updated_at": "2025-06-10T23:48:37+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.23501": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.23501",
        "url": "https://arxiv.org/pdf/2410.23501",
        "title": "All or None: Identifiable Linear Properties of Next-token Predictors in\n  Language Modeling",
        "authors": "Emanuele Marconato, S\u00e9bastien Lachapelle, Sebastian Weichwald, Luigi Gresele",
        "abstract": "We analyze identifiability as a possible explanation for the ubiquity of\nlinear properties across language models, such as the vector difference between\nthe representations of \"easy\" and \"easiest\" being parallel to that between\n\"lucky\" and \"luckiest\". For this, we ask whether finding a linear property in\none model implies that any model that induces the same distribution has that\nproperty, too. To answer that, we first prove an identifiability result to\ncharacterize distribution-equivalent next-token predictors, lifting a diversity\nrequirement of previous results. Second, based on a refinement of relational\nlinearity [Paccanaro and Hinton, 2001; Hernandez et al., 2024], we show how\nmany notions of linearity are amenable to our analysis. Finally, we show that\nunder suitable conditions, these linear properties either hold in all or none\ndistribution-equivalent next-token predictors.",
        "timestamp": "2025-06-10T23:46:40.077Z",
        "rating": "novote",
        "publishedDate": "2024-10-30T23:19:29Z",
        "tags": [
          "stat.ML",
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2,
        "object_id": "paper:arxiv.2410.23501",
        "created_at": "2025-06-10T23:46:40+00:00",
        "updated_at": "2025-06-10T23:47:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.08679": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08679",
        "url": "https://arxiv.org/pdf/2503.08679",
        "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
        "authors": "Iv\u00e1n Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy",
        "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful, i.e. CoT reasoning does not always reflect how models arrive\nat conclusions. So far, most of these studies have focused on unfaithfulness in\nunnatural contexts where an explicit bias has been introduced. In contrast, we\nshow that unfaithful CoT can occur on realistic prompts with no artificial\nbias. Our results reveal non-negligible rates of several forms of unfaithful\nreasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and\nChatGPT-4o (7.0%) all answer a notable proportion of question pairs\nunfaithfully. Specifically, we find that models rationalize their implicit\nbiases in answers to binary questions (\"implicit post-hoc rationalization\").\nFor example, when separately presented with the questions \"Is X bigger than Y?\"\nand \"Is Y bigger than X?\", models sometimes produce superficially coherent\narguments to justify answering Yes to both questions or No to both questions,\ndespite such responses being logically contradictory. We also investigate\nrestoration errors (Dziri et al., 2023), where models make and then silently\ncorrect errors in their reasoning, and unfaithful shortcuts, where models use\nclearly illogical reasoning to simplify solving problems in Putnam questions (a\nhard benchmark). Our findings raise challenges for AI safety work that relies\non monitoring CoT to detect undesired behavior.",
        "timestamp": "2025-06-10T23:57:09.917Z",
        "rating": "novote",
        "publishedDate": "2025-03-11T17:56:30Z",
        "tags": [
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 6,
        "object_id": "paper:arxiv.2503.08679",
        "created_at": "2025-06-10T23:57:10+00:00",
        "updated_at": "2025-06-10T23:57:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.04741": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.04741",
        "url": "https://arxiv.org/pdf/2505.04741",
        "title": "When Bad Data Leads to Good Models",
        "authors": "Kenneth Li, Yida Chen, Fernanda Vi\u00e9gas, Martin Wattenberg",
        "abstract": "In large language model (LLM) pretraining, data quality is believed to\ndetermine model quality. In this paper, we re-examine the notion of \"quality\"\nfrom the perspective of pre- and post-training co-design. Specifically, we\nexplore the possibility that pre-training on more toxic data can lead to better\ncontrol in post-training, ultimately decreasing a model's output toxicity.\nFirst, we use a toy experiment to study how data composition affects the\ngeometry of features in the representation space. Next, through controlled\nexperiments with Olmo-1B models trained on varying ratios of clean and toxic\ndata, we find that the concept of toxicity enjoys a less entangled linear\nrepresentation as the proportion of toxic data increases. Furthermore, we show\nthat although toxic data increases the generational toxicity of the base model,\nit also makes the toxicity easier to remove. Evaluations on Toxigen and Real\nToxicity Prompts demonstrate that models trained on toxic data achieve a better\ntrade-off between reducing generational toxicity and preserving general\ncapabilities when detoxifying techniques such as inference-time intervention\n(ITI) are applied. Our findings suggest that, with post-training taken into\naccount, bad data may lead to good models.",
        "timestamp": "2025-06-10T23:57:10.004Z",
        "rating": "novote",
        "publishedDate": "2025-05-07T19:17:49Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 5,
        "object_id": "paper:arxiv.2505.04741",
        "created_at": "2025-06-10T23:57:10+00:00",
        "updated_at": "2025-06-10T23:57:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.13775": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.13775",
        "url": "https://arxiv.org/pdf/2505.13775",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-11T00:00:45.778Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 8,
        "object_id": "paper:arxiv.2505.13775",
        "created_at": "2025-06-11T00:00:46+00:00",
        "updated_at": "2025-06-11T00:01:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.03714": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.03714",
        "url": "https://arxiv.org/pdf/2310.03714",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-11T00:00:36.952Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 7,
        "object_id": "paper:arxiv.2310.03714",
        "created_at": "2025-06-11T00:00:37+00:00",
        "updated_at": "2025-06-11T00:00:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.03001": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.03001",
        "interactions": []
      },
      "meta": {
        "issue_number": 10,
        "object_id": "interactions:arxiv.2410.03001",
        "created_at": "2025-06-11T00:03:29+00:00",
        "updated_at": "2025-06-11T00:03:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.03001": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.03001",
        "url": "https://arxiv.org/pdf/2410.03001",
        "title": "Can Transformers Learn $n$-gram Language Models?",
        "authors": "Anej Svete, Nadav Borenstein, Mike Zhou, Isabelle Augenstein, Ryan Cotterell",
        "abstract": "Much theoretical work has described the ability of transformers to represent\nformal languages. However, linking theoretical results to empirical performance\nis not straightforward due to the complex interplay between the architecture,\nthe learning algorithm, and training data. To test whether theoretical lower\nbounds imply \\emph{learnability} of formal languages, we turn to recent work\nrelating transformers to $n$-gram language models (LMs). We study transformers'\nability to learn random $n$-gram LMs of two kinds: ones with arbitrary\nnext-symbol probabilities and ones where those are defined with shared\nparameters. We find that classic estimation techniques for $n$-gram LMs such as\nadd-$\\lambda$ smoothing outperform transformers on the former, while\ntransformers perform better on the latter, outperforming methods specifically\ndesigned to learn $n$-gram LMs.",
        "timestamp": "2025-06-11T00:03:19.769Z",
        "rating": "novote",
        "publishedDate": "2024-10-03T21:21:02Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 9,
        "object_id": "paper:arxiv.2410.03001",
        "created_at": "2025-06-11T00:03:20+00:00",
        "updated_at": "2025-06-11T00:03:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.06264": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.06264",
        "url": "https://arxiv.org/abs/2412.06264",
        "title": "Flow Matching Guide and Code",
        "authors": "Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, Itai Gat",
        "abstract": "Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM.",
        "timestamp": "2025-06-11T13:52:37.558Z",
        "rating": "novote",
        "publishedDate": "2024/12/09",
        "tags": [
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 14,
        "object_id": "paper:arxiv.2412.06264",
        "created_at": "2025-06-11T13:52:37+00:00",
        "updated_at": "2025-06-11T13:53:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.02867": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.02867",
        "url": "https://arxiv.org/abs/2506.02867v1",
        "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning",
        "authors": "Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, Jing Shao",
        "abstract": "Large reasoning models (LRMs) have demonstrated impressive capabilities in complex problem-solving, yet their internal reasoning mechanisms remain poorly understood. In this paper, we investigate the reasoning trajectories of LRMs from an information-theoretic perspective. By tracking how mutual information (MI) between intermediate representations and the correct answer evolves during LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at specific generative steps exhibits a sudden and significant increase during LRM's reasoning process. We theoretically analyze such phenomenon and show that as MI increases, the probability of model's prediction error decreases. Furthermore, these MI peaks often correspond to tokens expressing reflection or transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the thinking tokens. We then demonstrate that these thinking tokens are crucial for LRM's reasoning performance, while other tokens has minimal impacts. Building on these analyses, we propose two simple yet effective methods to improve LRM's reasoning performance, by delicately leveraging these thinking tokens. Overall, our work provides novel insights into the reasoning mechanisms of LRMs and offers practical ways to improve their reasoning capabilities. The code is available at this https URL.",
        "timestamp": "2025-06-11T13:52:35.681Z",
        "rating": "novote",
        "publishedDate": "2025/06/03",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 13,
        "object_id": "paper:arxiv.2506.02867",
        "created_at": "2025-06-11T13:52:35+00:00",
        "updated_at": "2025-06-11T13:52:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.01939": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.01939",
        "url": "https://arxiv.org/abs/2506.01939",
        "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
        "authors": "Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, Junyang Lin",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.",
        "timestamp": "2025-06-11T13:52:34.922Z",
        "rating": "novote",
        "publishedDate": "2025/06/02",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 12,
        "object_id": "paper:arxiv.2506.01939",
        "created_at": "2025-06-11T13:52:35+00:00",
        "updated_at": "2025-06-11T13:52:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.18128": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.18128",
        "url": "https://arxiv.org/abs/2505.18128",
        "title": "Frankentext: Stitching random text fragments into long-form narratives",
        "authors": "Chau Minh Pham, Jenna Russell, Dzung Pham, Mohit Iyyer",
        "abstract": "We introduce Frankentexts, a new type of long-form narratives produced by LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from human writings. This task presents a challenging test of controllable generation, requiring models to satisfy a writing prompt, integrate disparate text fragments, and still produce a coherent narrative. To generate Frankentexts, we instruct the model to produce a draft by selecting and combining human-written passages, then iteratively revise the draft while maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts along three axes: writing quality, instruction adherence, and detectability. Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts are coherent and 100% relevant to the prompt. Notably, up to 59% of these outputs are misclassified as human-written by detectors like Pangram, revealing limitations in AI text detectors. Human annotators can sometimes identify Frankentexts through their abrupt tone shifts and inconsistent grammar between segments, especially in longer generations. Beyond presenting a challenging generation task, Frankentexts invite discussion on building effective detectors for this new grey zone of authorship, provide training data for mixed authorship detection, and serve as a sandbox for studying human-AI co-writing processes.",
        "timestamp": "2025-06-11T13:52:32.703Z",
        "rating": "novote",
        "publishedDate": "2025/05/23",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 11,
        "object_id": "paper:arxiv.2505.18128",
        "created_at": "2025-06-11T13:52:32+00:00",
        "updated_at": "2025-06-11T13:52:51+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.12821": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.12821",
        "url": "https://arxiv.org/pdf/2502.12821",
        "title": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in\n  Large Language Models",
        "authors": "Elena Stringli, Maria Lymperaiou, Giorgos Filandrianos, Athanasios Voulodimos, Giorgos Stamou",
        "abstract": "Inverse tasks can uncover potential reasoning gaps as Large Language Models\n(LLMs) scale up. In this work, we explore the redefinition task, in which we\nassign alternative values to well-known physical constants and units of\nmeasure, prompting LLMs to respond accordingly. Our findings show that not only\ndoes model performance degrade with scale, but its false confidence also rises.\nMoreover, while factors such as prompting strategies or response formatting are\ninfluential, they do not preclude LLMs from anchoring to memorized values.",
        "timestamp": "2025-06-11T14:53:10.606Z",
        "rating": "novote",
        "publishedDate": "2025-02-18T12:32:11Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 15,
        "object_id": "paper:arxiv.2502.12821",
        "created_at": "2025-06-11T14:53:10+00:00",
        "updated_at": "2025-06-11T14:53:29+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.07830": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.07830",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-11T17:44:34.634Z",
            "data": {
              "session_id": "session_1749663874214_sl3ztf9",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-06-11T17:44:23.108Z",
              "end_time": "2025-06-11T17:44:34.214Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-11T17:49:17.585Z",
            "data": {
              "session_id": "session_1749664157581_9tp4gxw",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-06-11T17:49:09.817Z",
              "end_time": "2025-06-11T17:49:17.581Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-16T21:05:46.244Z",
            "data": {
              "session_id": "session_1750107946228_u4xrade",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-06-16T21:05:29.068Z",
              "end_time": "2025-06-16T21:05:46.228Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-24T19:42:27.583Z",
            "data": {
              "session_id": "session_1750794147138_l03eij8",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-06-24T19:39:06.818Z",
              "end_time": "2025-06-24T19:42:27.138Z",
              "heartbeat_count": 40,
              "duration_seconds": 200,
              "idle_seconds": 0,
              "total_elapsed_seconds": 200
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T15:15:50.678Z",
            "data": {
              "session_id": "session_1751555750206_a7hcdap",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-07-03T15:15:38.078Z",
              "end_time": "2025-07-03T15:15:50.205Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T20:54:56.150Z",
            "data": {
              "session_id": "session_1751576095762_x6taswx",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-07-03T20:53:56.309Z",
              "end_time": "2025-07-03T20:54:55.762Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 4,
              "total_elapsed_seconds": 59
            }
          }
        ]
      },
      "meta": {
        "issue_number": 18,
        "object_id": "interactions:arxiv.2502.07830",
        "created_at": "2025-06-11T17:44:35+00:00",
        "updated_at": "2025-07-03T20:55:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.07830": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.07830",
        "url": "https://arxiv.org/pdf/2502.07830",
        "title": "Captured by Captions: On Memorization and its Mitigation in CLIP Models",
        "authors": "Wenhao Wang, Adam Dziedzic, Grace C. Kim, Michael Backes, Franziska Boenisch",
        "abstract": "Multi-modal models, such as CLIP, have demonstrated strong performance in\naligning visual and textual representations, excelling in tasks like image\nretrieval and zero-shot classification. Despite this success, the mechanisms by\nwhich these models utilize training data, particularly the role of\nmemorization, remain unclear. In uni-modal models, both supervised and\nself-supervised, memorization has been shown to be essential for\ngeneralization. However, it is not well understood how these findings would\napply to CLIP, which incorporates elements from both supervised learning via\ncaptions that provide a supervisory signal similar to labels, and from\nself-supervised learning via the contrastive objective. To bridge this gap in\nunderstanding, we propose a formal definition of memorization in CLIP (CLIPMem)\nand use it to quantify memorization in CLIP models. Our results indicate that\nCLIP's memorization behavior falls between the supervised and self-supervised\nparadigms, with \"mis-captioned\" samples exhibiting highest levels of\nmemorization. Additionally, we find that the text encoder contributes more to\nmemorization than the image encoder, suggesting that mitigation strategies\nshould focus on the text domain. Building on these insights, we propose\nmultiple strategies to reduce memorization while at the same time improving\nutility--something that had not been shown before for traditional learning\nparadigms where reducing memorization typically results in utility decrease.",
        "timestamp": "2025-06-11T17:44:23.430Z",
        "rating": "novote",
        "publishedDate": "2025-02-11T00:11:13Z",
        "tags": [
          "cs.CV",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 17,
        "object_id": "paper:arxiv.2502.07830",
        "created_at": "2025-06-11T17:44:23+00:00",
        "updated_at": "2025-06-11T17:44:44+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.04265": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.04265",
        "url": "https://arxiv.org/pdf/2410.04265",
        "title": "AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language\n  Models via Systematic Attribution of Machine Text against Web Text",
        "authors": "Ximing Lu, Melanie Sclar, Skyler Hallinan, Niloofar Mireshghallah, Jiacheng Liu, Seungju Han, Allyson Ettinger, Liwei Jiang, Khyathi Chandu, Nouha Dziri, Yejin Choi",
        "abstract": "Creativity has long been considered one of the most difficult aspect of human\nintelligence for AI to mimic. However, the rise of Large Language Models\n(LLMs), like ChatGPT, has raised questions about whether AI can match or even\nsurpass human creativity. We present CREATIVITY INDEX as the first step to\nquantify the linguistic creativity of a text by reconstructing it from existing\ntext snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that\nthe seemingly remarkable creativity of LLMs may be attributable in large part\nto the creativity of human-written texts on the web. To compute CREATIVITY\nINDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming\nalgorithm that can search verbatim and near-verbatim matches of text snippets\nfrom a given document against the web. Experiments reveal that the CREATIVITY\nINDEX of professional human authors is on average 66.2% higher than that of\nLLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of\n30.1%. In addition, we find that distinguished authors like Hemingway exhibit\nmeasurably higher CREATIVITY INDEX compared to other human writers. Finally, we\ndemonstrate that CREATIVITY INDEX can be used as a surprisingly effective\ncriterion for zero-shot machine text detection, surpassing the strongest\nexisting zero-shot system, DetectGPT, by a significant margin of 30.2%, and\neven outperforming the strongest supervised system, GhostBuster, in five out of\nsix domains.",
        "timestamp": "2025-06-11T17:44:07.863Z",
        "rating": "novote",
        "publishedDate": "2024-10-05T18:55:01Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 16,
        "object_id": "paper:arxiv.2410.04265",
        "created_at": "2025-06-11T17:44:08+00:00",
        "updated_at": "2025-06-11T17:44:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.07684": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.07684",
        "url": "https://arxiv.org/pdf/2412.07684",
        "title": "The Pitfalls of Memorization: When Memorization Hurts Generalization",
        "authors": "Reza Bayat, Mohammad Pezeshki, Elvis Dohmatob, David Lopez-Paz, Pascal Vincent",
        "abstract": "Neural networks often learn simple explanations that fit the majority of the\ndata while memorizing exceptions that deviate from these explanations.This\nbehavior leads to poor generalization when the learned explanations rely on\nspurious correlations. In this work, we formalize the interplay between\nmemorization and generalization, showing that spurious correlations would\nparticularly lead to poor generalization when are combined with memorization.\nMemorization can reduce training loss to zero, leaving no incentive to learn\nrobust, generalizable patterns. To address this, we propose memorization-aware\ntraining (MAT), which uses held-out predictions as a signal of memorization to\nshift a model's logits. MAT encourages learning robust patterns invariant\nacross distributions, improving generalization under distribution shifts.",
        "timestamp": "2025-06-11T21:24:39.795Z",
        "rating": "novote",
        "publishedDate": "2024-12-10T17:18:33Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 21,
        "object_id": "paper:arxiv.2412.07684",
        "created_at": "2025-06-11T21:24:40+00:00",
        "updated_at": "2025-06-11T21:24:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.01769": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.01769",
        "url": "https://arxiv.org/pdf/2410.01769?",
        "title": "Quantifying Generalization Complexity for Large Language Models",
        "authors": "Zhenting Qi, Hongyin Luo, Xuliang Huang, Zhuokai Zhao, Yibo Jiang, Xiangjun Fan, Himabindu Lakkaraju, James Glass",
        "abstract": "While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities.",
        "timestamp": "2025-06-11T21:24:39.623Z",
        "rating": "novote",
        "publishedDate": "2024-10-02T17:25:37Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 20,
        "object_id": "paper:arxiv.2410.01769",
        "created_at": "2025-06-11T21:24:39+00:00",
        "updated_at": "2025-06-11T21:25:03+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.14985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.14985",
        "url": "https://arxiv.org/pdf/2407.14985",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-11T21:24:11.679Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 19,
        "object_id": "paper:arxiv.2407.14985",
        "created_at": "2025-06-11T21:24:11+00:00",
        "updated_at": "2025-06-11T21:24:36+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.21530": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.21530",
        "url": "https://arxiv.org/abs/2407.21530",
        "title": "Data Contamination Report from the 2024 CONDA Shared Task",
        "authors": "Oscar Sainz, Iker Garc\u00eda-Ferrero, Alon Jacovi, Jon Ander Campos, Yanai Elazar, Eneko Agirre, Yoav Goldberg, Wei-Lin Chen, Jenny Chim, Leshem Choshen, Luca D'Amico-Wong, Melissa Dell, Run-Ze Fan, Shahriar Golchin, Yucheng Li, Pengfei Liu, Bhavish Pahwa, Ameya Prabhu, Suryansh Sharma, Emily Silcock, Kateryna Solonko, David Stap, Mihai Surdeanu, Yu-Min Tseng, Vishaal Udandarao, Zengzhi Wang, Ruijie Xu, Jinglin Yang",
        "abstract": "The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant aspects of data contamination in natural language processing, where data contamination is understood as situations where evaluation data is included in pre-training corpora used to train large scale models, compromising evaluation results. The workshop fostered a shared task to collect evidence on data contamination in current available datasets and models. The goal of the shared task and associated database is to assist the community in understanding the extent of the problem and to assist researchers in avoiding reporting evaluation results on known contaminated resources. The shared task provides a structured, centralized public database for the collection of contamination evidence, open to contributions from the community via GitHub pool requests. This first compilation paper is based on 566 reported entries over 91 contaminated sources from a total of 23 contributors. The details of the individual contamination events are available in the platform. The platform continues to be online, open to contributions from the community.",
        "timestamp": "2025-06-11T21:26:27.743Z",
        "rating": "novote",
        "publishedDate": "2024/07/31",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 22,
        "object_id": "paper:arxiv.2407.21530",
        "created_at": "2025-06-11T21:26:28+00:00",
        "updated_at": "2025-06-11T21:26:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.02810": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.02810",
        "url": "https://arxiv.org/abs/2504.02810",
        "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
        "authors": "Haowei Lin, Xiangyu Wang, Ruilin Yan, Baizhou Huang, Haotian Ye, Jianhua Zhu, Zihao Wang, James Zou, Jianzhu Ma, Yitao Liang",
        "abstract": "With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.",
        "timestamp": "2025-06-11T21:54:40.717Z",
        "rating": "novote",
        "publishedDate": "2025/04/03",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 23,
        "object_id": "paper:arxiv.2504.02810",
        "created_at": "2025-06-11T21:54:40+00:00",
        "updated_at": "2025-06-11T21:54:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.02126": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.02126",
        "url": "https://arxiv.org/pdf/2506.02126",
        "title": "Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains",
        "authors": "Juncheng Wu, Sheng Liu, Haoqin Tu, Hang Yu, Xiaoke Huang, James Zou, Cihang Xie, Yuyin Zhou",
        "abstract": "Recent advances in reasoning-enhanced Large Language Models such as\nOpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex\ntasks. However, the quality and transparency of their internal reasoning\nprocesses remain underexplored. This work moves beyond the final-answer\naccuracy and investigates step-by-step reasoning in the medical and\nmathematical domains by explicitly decomposing the thinking trajectories into\ntwo parts: knowledge and reasoning. Specifically, we introduce a fine-grained\nevaluation framework that judges: (1) the correctness of knowledge used\n(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured\nby Information Gain (InfoGain)). Using this framework, we study R1-distilled\nand base Qwen models trained with supervised fine-tuning (SFT) and/or\nreinforcement learning (RL) in the medical and math domains. Three intriguing\nfindings emerge: (1) The general reasoning abilities in R1-distilled models do\nnot transfer effectively to the medical domain through either SFT or RL. (2)\nSFT raises final-answer accuracy in both domains, but often at the cost of\nreasoning quality: InfoGain drops by 38.9% on average compared with untrained\nmodels; In the medical domain, however, SFT remains crucial because domain\nknowledge is indispensable. (3) RL enhances medical reasoning by pruning\ninaccurate or irrelevant knowledge from reasoning paths, thereby improving both\nreasoning accuracy and knowledge correctness.",
        "timestamp": "2025-06-12T17:19:47.018Z",
        "rating": "novote",
        "publishedDate": "2025-06-02T18:01:00Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 63,
        "object_id": "paper:arxiv.2506.02126",
        "created_at": "2025-06-12T17:19:47+00:00",
        "updated_at": "2025-06-12T17:20:22+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.03247": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.03247",
        "url": "https://arxiv.org/pdf/2408.03247",
        "title": "Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons",
        "authors": "Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, Daniel Dajun Zeng",
        "abstract": "In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.",
        "timestamp": "2025-06-12T17:19:47.017Z",
        "rating": "novote",
        "publishedDate": "2024-08-06T15:07:08Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 62,
        "object_id": "paper:arxiv.2408.03247",
        "created_at": "2025-06-12T17:19:47+00:00",
        "updated_at": "2025-06-12T17:20:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.09522": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09522",
        "url": "https://arxiv.org/pdf/2504.09522",
        "title": "How new data permeates LLM knowledge and how to dilute it",
        "authors": "Chen Sun, Renat Aksitov, Andrey Zhmoginov, Nolan Andrew Miller, Max Vladymyrov, Ulrich Rueckert, Been Kim, Mark Sandler",
        "abstract": "Large language models learn and continually learn through the accumulation of\ngradient-based updates, but how individual pieces of new information affect\nexisting knowledge, leading to both beneficial generalization and problematic\nhallucination, remains poorly understood. We demonstrate that when learning new\ninformation, LLMs exhibit a \"priming\" effect: learning a new fact can cause the\nmodel to inappropriately apply that knowledge in unrelated contexts. To\nsystematically study this phenomenon, we introduce \"Outlandish,\" a carefully\ncurated dataset of 1320 diverse text samples designed to probe how new\nknowledge permeates through an LLM's existing knowledge base. Using this\ndataset, we show that the degree of priming after learning new information can\nbe predicted by measuring the token probability of key words before learning.\nThis relationship holds robustly across different model architectures (PALM-2,\nGemma, Llama), sizes, and training stages. Finally, we develop two novel\ntechniques to modulate how new knowledge affects existing model behavior: (1) a\n``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update\npruning method. These approaches reduce undesirable priming effects by 50-95\\%\nwhile preserving the model's ability to learn new information. Our findings\nprovide both empirical insights into how LLMs learn and practical tools for\nimproving the specificity of knowledge insertion in language models. Further\nmaterials: https://sunchipsster1.github.io/projects/outlandish/",
        "timestamp": "2025-06-12T17:19:46.979Z",
        "rating": "novote",
        "publishedDate": "2025-04-13T11:25:04Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 61,
        "object_id": "paper:arxiv.2504.09522",
        "created_at": "2025-06-12T17:19:47+00:00",
        "updated_at": "2025-06-12T17:20:18+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.14685": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.14685",
        "url": "https://arxiv.org/pdf/2505.14685",
        "title": "Language Models use Lookbacks to Track Beliefs",
        "authors": "Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma, Christoph Riedl, Yonatan Belinkov, Tamar Rott Shaham, David Bau, Atticus Geiger",
        "abstract": "How do language models (LMs) represent characters' beliefs, especially when\nthose beliefs may differ from reality? This question lies at the heart of\nunderstanding the Theory of Mind (ToM) capabilities of LMs. We analyze\nLlama-3-70B-Instruct's ability to reason about characters' beliefs using causal\nmediation and abstraction. We construct a dataset that consists of simple\nstories where two characters each separately change the state of two objects,\npotentially unaware of each other's actions. Our investigation uncovered a\npervasive algorithmic pattern that we call a lookback mechanism, which enables\nthe LM to recall important information when it becomes necessary. The LM binds\neach character-object-state triple together by co-locating reference\ninformation about them, represented as their Ordering IDs (OIs) in low rank\nsubspaces of the state token's residual stream. When asked about a character's\nbeliefs regarding the state of an object, the binding lookback retrieves the\ncorresponding state OI and then an answer lookback retrieves the state token.\nWhen we introduce text specifying that one character is (not) visible to the\nother, we find that the LM first generates a visibility ID encoding the\nrelation between the observing and the observed character OIs. In a visibility\nlookback, this ID is used to retrieve information about the observed character\nand update the observing character's beliefs. Our work provides insights into\nthe LM's belief tracking mechanisms, taking a step toward reverse-engineering\nToM reasoning in LMs.",
        "timestamp": "2025-06-12T17:19:46.992Z",
        "rating": "novote",
        "publishedDate": "2025-05-20T17:59:45Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 60,
        "object_id": "paper:arxiv.2505.14685",
        "created_at": "2025-06-12T17:19:47+00:00",
        "updated_at": "2025-06-12T17:20:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.20879": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.20879",
        "url": "https://arxiv.org/pdf/2504.20879",
        "title": "The Leaderboard Illusion",
        "authors": "Shivalika Singh, Yiyang Nan, Alex Wang, Daniel D'Souza, Sayash Kapoor, Ahmet \u00dcst\u00fcn, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah A. Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker",
        "abstract": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field",
        "timestamp": "2025-06-12T17:19:46.991Z",
        "rating": "novote",
        "publishedDate": "2025-04-29T15:48:49Z",
        "tags": [
          "cs.AI",
          "cs.CL",
          "cs.LG",
          "stat.ME"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 59,
        "object_id": "paper:arxiv.2504.20879",
        "created_at": "2025-06-12T17:19:47+00:00",
        "updated_at": "2025-06-12T17:20:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.13121": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.13121",
        "url": "https://arxiv.org/pdf/2310.13121",
        "title": "Understanding Addition in Transformers",
        "authors": "Philip Quirke, Fazl Barez",
        "abstract": "Understanding the inner workings of machine learning models like Transformers\nis vital for their safe and ethical use. This paper provides a comprehensive\nanalysis of a one-layer Transformer model trained to perform n-digit integer\naddition. Our findings suggest that the model dissects the task into parallel\nstreams dedicated to individual digits, employing varied algorithms tailored to\ndifferent positions within the digits. Furthermore, we identify a rare scenario\ncharacterized by high loss, which we explain. By thoroughly elucidating the\nmodel's algorithm, we provide new insights into its functioning. These findings\nare validated through rigorous testing and mathematical modeling, thereby\ncontributing to the broader fields of model understanding and interpretability.\nOur approach opens the door for analyzing more complex tasks and multi-layer\nTransformer models.",
        "timestamp": "2025-06-12T17:19:43.181Z",
        "rating": "novote",
        "publishedDate": "2023-10-19T19:34:42Z",
        "tags": [
          "cs.LG",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 58,
        "object_id": "paper:arxiv.2310.13121",
        "created_at": "2025-06-12T17:19:43+00:00",
        "updated_at": "2025-06-12T17:20:20+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.00985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.00985",
        "url": "https://arxiv.org/pdf/2505.00985",
        "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
        "authors": "Yash Goel, Ayan Sengupta, Tanmoy Chakraborty",
        "abstract": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development.",
        "timestamp": "2025-06-12T17:19:43.183Z",
        "rating": "novote",
        "publishedDate": "2025-05-02T04:13:27Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 57,
        "object_id": "paper:arxiv.2505.00985",
        "created_at": "2025-06-12T17:19:43+00:00",
        "updated_at": "2025-06-12T17:20:18+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.17148": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.17148",
        "url": "https://arxiv.org/abs/2501.17148",
        "title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders",
        "authors": "Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, Christopher Potts",
        "abstract": "Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering, we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.",
        "timestamp": "2025-06-12T17:19:43.193Z",
        "rating": "novote",
        "publishedDate": "2025/01/28",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 56,
        "object_id": "paper:arxiv.2501.17148",
        "created_at": "2025-06-12T17:19:43+00:00",
        "updated_at": "2025-06-12T17:20:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2312.01552": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2312.01552",
        "url": "https://arxiv.org/pdf/2312.01552",
        "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context\n  Learning",
        "authors": "Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, Yejin Choi",
        "abstract": "The alignment tuning process of large language models (LLMs) typically\ninvolves instruction learning through supervised fine-tuning (SFT) and\npreference tuning via reinforcement learning from human feedback (RLHF). A\nrecent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for\nSFT can achieve significant alignment performance as well, suggesting that the\neffect of alignment tuning might be \"superficial.\" This raises questions about\nhow exactly the alignment tuning transforms a base LLM.\n  We analyze the effect of alignment tuning by examining the token distribution\nshift between base LLMs and their aligned counterpart. Our findings reveal that\nbase LLMs and their alignment-tuned versions perform nearly identically in\ndecoding on the majority of token positions. Most distribution shifts occur\nwith stylistic tokens. These direct evidence strongly supports the Superficial\nAlignment Hypothesis suggested by LIMA.\n  Based on these findings, we rethink the alignment of LLMs by posing the\nresearch question: how effectively can we align base LLMs without SFT or RLHF?\nTo address this, we introduce a simple, tuning-free alignment method, URIAL.\nURIAL achieves effective alignment purely through in-context learning (ICL)\nwith base LLMs, requiring as few as three constant stylistic examples and a\nsystem prompt. We conduct a fine-grained and interpretable evaluation on a\ndiverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that\nbase LLMs with URIAL can match or even surpass the performance of LLMs aligned\nwith SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based\nalignment methods can be significantly reduced through strategic prompting and\nICL. Our findings on the superficial nature of alignment tuning and results\nwith URIAL suggest that deeper analysis and theoretical understanding of\nalignment is crucial to future LLM research.",
        "timestamp": "2025-06-12T17:19:42.594Z",
        "rating": "novote",
        "publishedDate": "2023-12-04T00:46:11Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 55,
        "object_id": "paper:arxiv.2312.01552",
        "created_at": "2025-06-12T17:19:43+00:00",
        "updated_at": "2025-06-12T17:20:23+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.08019": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.08019",
        "url": "https://arxiv.org/pdf/2411.08019",
        "title": "Language Models as Causal Effect Generators",
        "authors": "Lucius E. J. Bynum, Kyunghyun Cho",
        "abstract": "We present a framework for large language model (LLM) based data generation\nwith controllable causal structure. In particular, we define a procedure for\nturning any language model and any directed acyclic graph (DAG) into a\nsequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM\nis a causal model with user-defined structure and LLM-defined structural\nequations. We characterize how an SD-SCM allows sampling from observational,\ninterventional, and counterfactual distributions according to the desired\ncausal structure. We then leverage this procedure to propose a new type of\nbenchmark for causal inference methods, generating individual-level\ncounterfactual data without needing to manually specify functional\nrelationships between variables. We create an example benchmark consisting of\nthousands of datasets, and test a suite of popular estimation methods on these\ndatasets for average, conditional average, and individual treatment effect\nestimation, both with and without hidden confounding. Apart from generating\ndata, the same procedure also allows us to test for the presence of a causal\neffect that might be encoded in an LLM. This procedure can underpin auditing\nLLMs for misinformation, discrimination, or otherwise undesirable behavior. We\nbelieve SD-SCMs can serve as a useful tool in any application that would\nbenefit from sequential data with controllable causal structure.",
        "timestamp": "2025-06-12T17:19:42.507Z",
        "rating": "novote",
        "publishedDate": "2024-11-12T18:50:35Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG",
          "stat.AP",
          "stat.ME",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 54,
        "object_id": "paper:arxiv.2411.08019",
        "created_at": "2025-06-12T17:19:43+00:00",
        "updated_at": "2025-06-12T17:20:18+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.18114": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.18114",
        "url": "https://arxiv.org/pdf/2504.18114",
        "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection",
        "authors": "Atharva Kulkarni, Yuan Zhang, Joel Ruben Antony Moniz, Xiou Ge, Bo-Hsiang Tseng, Dhivya Piraviperumal, Swabha Swayamdipta, Hong Yu",
        "abstract": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them.",
        "timestamp": "2025-06-12T17:19:41.879Z",
        "rating": "novote",
        "publishedDate": "2025-04-25T06:37:29Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 53,
        "object_id": "paper:arxiv.2504.18114",
        "created_at": "2025-06-12T17:19:43+00:00",
        "updated_at": "2025-06-12T17:20:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.03867": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.03867",
        "url": "https://arxiv.org/pdf/2403.03867",
        "title": "On the Origins of Linear Representations in Large Language Models",
        "authors": "Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, Victor Veitch",
        "abstract": "Recent works have argued that high-level semantic concepts are encoded\n\"linearly\" in the representation space of large language models. In this work,\nwe study the origins of such linear representations. To that end, we introduce\na simple latent variable model to abstract and formalize the concept dynamics\nof the next token prediction. We use this formalism to show that the next token\nprediction objective (softmax with cross-entropy) and the implicit bias of\ngradient descent together promote the linear representation of concepts.\nExperiments show that linear representations emerge when learning from data\nmatching the latent variable model, confirming that this simple structure\nalready suffices to yield linear representations. We additionally confirm some\npredictions of the theory using the LLaMA-2 large language model, giving\nevidence that the simplified model yields generalizable insights.",
        "timestamp": "2025-06-12T17:19:41.258Z",
        "rating": "novote",
        "publishedDate": "2024-03-06T17:17:36Z",
        "tags": [
          "cs.CL",
          "cs.LG",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 52,
        "object_id": "paper:arxiv.2403.03867",
        "created_at": "2025-06-12T17:19:41+00:00",
        "updated_at": "2025-06-12T17:20:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.04289": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.04289",
        "url": "https://arxiv.org/pdf/2406.04289",
        "title": "What Languages are Easy to Language-Model? A Perspective from Learning\n  Probabilistic Regular Languages",
        "authors": "Nadav Borenstein, Anej Svete, Robin Chan, Josef Valvoda, Franz Nowak, Isabelle Augenstein, Eleanor Chodroff, Ryan Cotterell",
        "abstract": "What can large language models learn? By definition, language models (LM) are\ndistributions over strings. Therefore, an intuitive way of addressing the above\nquestion is to formalize it as a matter of learnability of classes of\ndistributions over strings. While prior work in this direction focused on\nassessing the theoretical limits, in contrast, we seek to understand the\nempirical learnability. Unlike prior empirical work, we evaluate neural LMs on\ntheir home turf-learning probabilistic languages-rather than as classifiers of\nformal languages. In particular, we investigate the learnability of regular LMs\n(RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs\nas a function of various complexity parameters of the RLM and the hidden state\nsize of the neural LM. We find that the RLM rank, which corresponds to the size\nof linear space spanned by the logits of its conditional distributions, and the\nexpected length of sampled strings are strong and significant predictors of\nlearnability for both RNNs and Transformers. Several other predictors also\nreach significance, but with differing patterns between RNNs and Transformers.",
        "timestamp": "2025-06-12T17:19:41.006Z",
        "rating": "novote",
        "publishedDate": "2024-06-06T17:34:24Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 51,
        "object_id": "paper:arxiv.2406.04289",
        "created_at": "2025-06-12T17:19:41+00:00",
        "updated_at": "2025-06-12T17:20:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2210.10749": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.10749",
        "url": "https://arxiv.org/pdf/2210.10749",
        "title": "Transformers Learn Shortcuts to Automata",
        "authors": "Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang",
        "abstract": "Algorithmic reasoning requires capabilities which are most naturally\nunderstood through recurrent models of computation, like the Turing machine.\nHowever, Transformer models, while lacking recurrence, are able to perform such\nreasoning using far fewer layers than the number of reasoning steps. This\nraises the question: what solutions are learned by these shallow and\nnon-recurrent models? We find that a low-depth Transformer can represent the\ncomputations of any finite-state automaton (thus, any bounded-memory\nalgorithm), by hierarchically reparameterizing its recurrent dynamics. Our\ntheoretical results characterize shortcut solutions, whereby a Transformer with\n$o(T)$ layers can exactly replicate the computation of an automaton on an input\nsequence of length $T$. We find that polynomial-sized $O(\\log T)$-depth\nsolutions always exist; furthermore, $O(1)$-depth simulators are surprisingly\ncommon, and can be understood using tools from Krohn-Rhodes theory and circuit\ncomplexity. Empirically, we perform synthetic experiments by training\nTransformers to simulate a wide variety of automata, and show that shortcut\nsolutions can be learned via standard training. We further investigate the\nbrittleness of these solutions and propose potential mitigations.",
        "timestamp": "2025-06-12T17:19:41.006Z",
        "rating": "novote",
        "publishedDate": "2022-10-19T17:45:48Z",
        "tags": [
          "cs.LG",
          "cs.FL",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 50,
        "object_id": "paper:arxiv.2210.10749",
        "created_at": "2025-06-12T17:19:41+00:00",
        "updated_at": "2025-06-12T17:20:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.03703": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.03703",
        "url": "https://arxiv.org/pdf/2503.03703",
        "title": "SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus\n  Searches",
        "authors": "Hiroyuki Deguchi, Go Kamoda, Yusuke Matsushita, Chihiro Taguchi, Kohei Suenaga, Masaki Waga, Sho Yokoi",
        "abstract": "Researchers and practitioners in natural language processing and\ncomputational linguistics frequently observe and analyze the real language\nusage in large-scale corpora. For that purpose, they often employ off-the-shelf\npattern-matching tools, such as grep, and keyword-in-context concordancers,\nwhich is widely used in corpus linguistics for gathering examples. Nonetheless,\nthese existing techniques rely on surface-level string matching, and thus they\nsuffer from the major limitation of not being able to handle orthographic\nvariations and paraphrasing -- notable and common phenomena in any natural\nlanguage. In addition, existing continuous approaches such as dense vector\nsearch tend to be overly coarse, often retrieving texts that are unrelated but\nshare similar topics. Given these challenges, we propose a novel algorithm that\nachieves \\emph{soft} (or semantic) yet efficient pattern matching by relaxing a\nsurface-level matching with word embeddings. Our algorithm is highly scalable\nwith respect to the size of the corpus text utilizing inverted indexes. We have\nprepared an efficient implementation, and we provide an accessible web tool.\nOur experiments demonstrate that the proposed method (i) can execute searches\non billion-scale corpora in less than a second, which is comparable in speed to\nsurface-level string matching and dense vector search; (ii) can extract harmful\ninstances that semantically match queries from a large set of English and\nJapanese Wikipedia articles; and (iii) can be effectively applied to\ncorpus-linguistic analyses of Latin, a language with highly diverse\ninflections.",
        "timestamp": "2025-06-12T17:19:40.474Z",
        "rating": "novote",
        "publishedDate": "2025-03-05T17:53:11Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 49,
        "object_id": "paper:arxiv.2503.03703",
        "created_at": "2025-06-12T17:19:41+00:00",
        "updated_at": "2025-06-12T17:20:14+00:00",
        "version": 1
      }
    },
    "paper:openreview.HvSytvg3Jh": {
      "data": {
        "sourceId": "openreview",
        "paperId": "HvSytvg3Jh",
        "url": "https://openreview.net/pdf?id=HvSytvg3Jh",
        "title": "HvSytvg3Jh",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-12T17:19:39.787Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 48,
        "object_id": "paper:openreview.HvSytvg3Jh",
        "created_at": "2025-06-12T17:19:40+00:00",
        "updated_at": "2025-06-12T17:20:11+00:00",
        "version": 1
      }
    },
    "paper:openreview.HD6bWcj87Y": {
      "data": {
        "sourceId": "openreview",
        "paperId": "HD6bWcj87Y",
        "url": "https://openreview.net/pdf?id=HD6bWcj87Y",
        "title": "HD6bWcj87Y",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-12T17:19:38.488Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 47,
        "object_id": "paper:openreview.HD6bWcj87Y",
        "created_at": "2025-06-12T17:19:40+00:00",
        "updated_at": "2025-06-12T17:20:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.12578": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.12578",
        "url": "https://arxiv.org/pdf/2408.12578",
        "title": "A Percolation Model of Emergence: Analyzing Transformers Trained on a\n  Formal Language",
        "authors": "Ekdeep Singh Lubana, Kyogo Kawaguchi, Robert P. Dick, Hidenori Tanaka",
        "abstract": "Increase in data, size, or compute can lead to sudden learning of specific\ncapabilities by a neural network -- a phenomenon often called \"emergence''.\nBeyond scientific understanding, establishing the causal factors underlying\nsuch emergent capabilities is crucial to enable risk regulation frameworks for\nAI. In this work, we seek inspiration from study of emergent properties in\nother fields and propose a phenomenological definition for the concept in the\ncontext of neural networks. Our definition implicates the acquisition of\ngeneral structures underlying the data-generating process as a cause of sudden\nperformance growth for specific, narrower tasks. We empirically investigate\nthis definition by proposing an experimental system grounded in a\ncontext-sensitive formal language and find that Transformers trained to perform\ntasks on top of strings from this language indeed exhibit emergent\ncapabilities. Specifically, we show that once the language's underlying grammar\nand context-sensitivity inducing structures are learned by the model,\nperformance on narrower tasks suddenly begins to improve. We then analogize our\nnetwork's learning dynamics with the process of percolation on a bipartite\ngraph, establishing a formal phase transition model that predicts the shift in\nthe point of emergence observed in our experiments when changing the data\nstructure. Overall, our experimental and theoretical frameworks yield a step\ntowards better defining, characterizing, and predicting emergence in neural\nnetworks.",
        "timestamp": "2025-06-12T17:19:34.859Z",
        "rating": "novote",
        "publishedDate": "2024-08-22T17:44:22Z",
        "tags": [
          "cs.LG",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 46,
        "object_id": "paper:arxiv.2408.12578",
        "created_at": "2025-06-12T17:19:35+00:00",
        "updated_at": "2025-06-12T17:20:17+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2311.12786": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.12786",
        "url": "https://arxiv.org/pdf/2311.12786",
        "title": "Mechanistically analyzing the effects of fine-tuning on procedurally\n  defined tasks",
        "authors": "Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rockt\u00e4schel, David Scott Krueger",
        "abstract": "Fine-tuning large pre-trained models has become the de facto strategy for\ndeveloping both task-specific and general-purpose machine learning systems,\nincluding developing models that are safe to deploy. Despite its clear\nimportance, there has been minimal work that explains how fine-tuning alters\nthe underlying capabilities learned by a model during pretraining: does\nfine-tuning yield entirely novel capabilities or does it just modulate existing\nones? We address this question empirically in synthetic, controlled settings\nwhere we can use mechanistic interpretability tools (e.g., network pruning and\nprobing) to understand how the model's underlying capabilities are changing. We\nperform an extensive analysis of the effects of fine-tuning in these settings,\nand show that: (i) fine-tuning rarely alters the underlying model capabilities;\n(ii) a minimal transformation, which we call a 'wrapper', is typically learned\non top of the underlying model capabilities, creating the illusion that they\nhave been modified; and (iii) further fine-tuning on a task where such hidden\ncapabilities are relevant leads to sample-efficient 'revival' of the\ncapability, i.e., the model begins reusing these capability after only a few\ngradient steps. This indicates that practitioners can unintentionally remove a\nmodel's safety wrapper merely by fine-tuning it on a, e.g., superficially\nunrelated, downstream task. We additionally perform analysis on language models\ntrained on the TinyStories dataset to support our claims in a more realistic\nsetup.",
        "timestamp": "2025-06-12T17:19:34.136Z",
        "rating": "novote",
        "publishedDate": "2023-11-21T18:51:04Z",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 45,
        "object_id": "paper:arxiv.2311.12786",
        "created_at": "2025-06-12T17:19:34+00:00",
        "updated_at": "2025-06-12T17:20:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.18866": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18866",
        "url": "https://arxiv.org/pdf/2503.18866",
        "title": "Reasoning to Learn from Latent Thoughts",
        "authors": "Yangjun Ruan, Neil Band, Chris J. Maddison, Tatsunori Hashimoto",
        "abstract": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% $\\rightarrow$ 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.",
        "timestamp": "2025-06-12T17:19:34.135Z",
        "rating": "novote",
        "publishedDate": "2025-03-24T16:41:23Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 44,
        "object_id": "paper:arxiv.2503.18866",
        "created_at": "2025-06-12T17:19:34+00:00",
        "updated_at": "2025-06-12T17:19:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.10061": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.10061",
        "url": "https://arxiv.org/pdf/2503.10061",
        "title": "Compute Optimal Scaling of Skills: Knowledge vs Reasoning",
        "authors": "Nicholas Roberts, Niladri Chatterji, Sharan Narang, Mike Lewis, Dieuwke Hupkes",
        "abstract": "Scaling laws are a critical component of the LLM development pipeline, most\nfamously as a way to forecast training decisions such as 'compute-optimally'\ntrading-off parameter count and dataset size, alongside a more recent growing\nlist of other crucial decisions. In this work, we ask whether compute-optimal\nscaling behaviour can be skill-dependent. In particular, we examine knowledge\nand reasoning-based skills such as knowledge-based QA and code generation, and\nwe answer this question in the affirmative: scaling laws are skill-dependent.\nNext, to understand whether skill-dependent scaling is an artefact of the\npretraining datamix, we conduct an extensive ablation of different datamixes\nand find that, also when correcting for datamix differences, knowledge and code\nexhibit fundamental differences in scaling behaviour. We conclude with an\nanalysis of how our findings relate to standard compute-optimal scaling using a\nvalidation set, and find that a misspecified validation set can impact\ncompute-optimal parameter count by nearly 50%, depending on its skill\ncomposition.",
        "timestamp": "2025-06-12T17:19:33.076Z",
        "rating": "novote",
        "publishedDate": "2025-03-13T05:21:22Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 43,
        "object_id": "paper:arxiv.2503.10061",
        "created_at": "2025-06-12T17:19:34+00:00",
        "updated_at": "2025-06-12T17:20:20+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.12580": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.12580",
        "url": "https://arxiv.org/pdf/2411.12580",
        "title": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models",
        "authors": "Laura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwarak Talupuru, Acyr Locatelli, Robert Kirk, Tim Rockt\u00e4schel, Edward Grefenstette, Max Bartolo",
        "abstract": "The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning.",
        "timestamp": "2025-06-12T17:19:32.447Z",
        "rating": "novote",
        "publishedDate": "2024-11-19T15:47:12Z",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 42,
        "object_id": "paper:arxiv.2411.12580",
        "created_at": "2025-06-12T17:19:33+00:00",
        "updated_at": "2025-06-12T17:19:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2409.12183": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.12183",
        "url": "https://arxiv.org/pdf/2409.12183",
        "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic\n  reasoning",
        "authors": "Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett",
        "abstract": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications.",
        "timestamp": "2025-06-12T17:19:32.447Z",
        "rating": "novote",
        "publishedDate": "2024-09-18T17:55:00Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 41,
        "object_id": "paper:arxiv.2409.12183",
        "created_at": "2025-06-12T17:19:33+00:00",
        "updated_at": "2025-06-12T17:20:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2006.07710": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2006.07710",
        "url": "https://arxiv.org/pdf/2006.07710",
        "title": "The Pitfalls of Simplicity Bias in Neural Networks",
        "authors": "Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, Praneeth Netrapalli",
        "abstract": "Several works have proposed Simplicity Bias (SB)---the tendency of standard\ntraining procedures such as Stochastic Gradient Descent (SGD) to find simple\nmodels---to justify why neural networks generalize well [Arpit et al. 2017,\nNakkiran et al. 2019, Soudry et al. 2018]. However, the precise notion of\nsimplicity remains vague. Furthermore, previous settings that use SB to\ntheoretically justify why neural networks generalize well do not simultaneously\ncapture the non-robustness of neural networks---a widely observed phenomenon in\npractice [Goodfellow et al. 2014, Jo and Bengio 2017]. We attempt to reconcile\nSB and the superior standard generalization of neural networks with the\nnon-robustness observed in practice by designing datasets that (a) incorporate\na precise notion of simplicity, (b) comprise multiple predictive features with\nvarying levels of simplicity, and (c) capture the non-robustness of neural\nnetworks trained on real data. Through theory and empirics on these datasets,\nwe make four observations: (i) SB of SGD and variants can be extreme: neural\nnetworks can exclusively rely on the simplest feature and remain invariant to\nall predictive complex features. (ii) The extreme aspect of SB could explain\nwhy seemingly benign distribution shifts and small adversarial perturbations\nsignificantly degrade model performance. (iii) Contrary to conventional wisdom,\nSB can also hurt generalization on the same data distribution, as SB persists\neven when the simplest feature has less predictive power than the more complex\nfeatures. (iv) Common approaches to improve generalization and\nrobustness---ensembles and adversarial training---can fail in mitigating SB and\nits pitfalls. Given the role of SB in training neural networks, we hope that\nthe proposed datasets and methods serve as an effective testbed to evaluate\nnovel algorithmic approaches aimed at avoiding the pitfalls of SB.",
        "timestamp": "2025-06-12T17:19:32.437Z",
        "rating": "novote",
        "publishedDate": "2020-06-13T20:15:26Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 40,
        "object_id": "paper:arxiv.2006.07710",
        "created_at": "2025-06-12T17:19:33+00:00",
        "updated_at": "2025-06-12T17:20:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.14481": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14481",
        "url": "https://arxiv.org/pdf/2503.14481",
        "title": "Don't lie to your friends: Learning what you know from collaborative\n  self-play",
        "authors": "Jacob Eisenstein, Reza Aghajani, Adam Fisch, Dheeru Dua, Fantine Huot, Mirella Lapata, Vicky Zayats, Jonathan Berant",
        "abstract": "To be helpful assistants, AI agents must be aware of their own capabilities\nand limitations. This includes knowing when to answer from parametric knowledge\nversus using tools, when to trust tool outputs, and when to abstain or hedge.\nSuch capabilities are hard to teach through supervised fine-tuning because they\nrequire constructing examples that reflect the agent's specific capabilities.\nWe therefore propose a radically new approach to teaching agents what they\nknow: \\emph{collaborative self-play}. We construct multi-agent collaborations\nin which the group is rewarded for collectively arriving at correct answers.\nThe desired meta-knowledge emerges from the incentives built into the structure\nof the interaction. We focus on small societies of agents that have access to\nheterogeneous tools (corpus-specific retrieval), and therefore must collaborate\nto maximize their success while minimizing their effort. Experiments show that\ngroup-level rewards for multi-agent communities can induce policies that\n\\emph{transfer} to improve tool use and selective prediction in settings where\nindividual agents are deployed in isolation.",
        "timestamp": "2025-06-12T17:19:32.437Z",
        "rating": "novote",
        "publishedDate": "2025-03-18T17:53:20Z",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 39,
        "object_id": "paper:arxiv.2503.14481",
        "created_at": "2025-06-12T17:19:33+00:00",
        "updated_at": "2025-06-12T17:20:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.07776": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.07776",
        "url": "https://arxiv.org/pdf/2502.07776",
        "title": "Auditing Prompt Caching in Language Model APIs",
        "authors": "Chenchen Gu, Xiang Lisa Li, Rohith Kuditipudi, Percy Liang, Tatsunori Hashimoto",
        "abstract": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
        "timestamp": "2025-06-12T17:19:31.239Z",
        "rating": "novote",
        "publishedDate": "2025-02-11T18:58:04Z",
        "tags": [
          "cs.CL",
          "cs.CR",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 38,
        "object_id": "paper:arxiv.2502.07776",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:20:03+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.15845": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.15845",
        "url": "https://arxiv.org/pdf/2407.15845",
        "title": "Reconstructing Training Data From Real World Models Trained with\n  Transfer Learning",
        "authors": "Yakir Oz, Gilad Yehudai, Gal Vardi, Itai Antebi, Michal Irani, Niv Haim",
        "abstract": "Current methods for reconstructing training data from trained classifiers are\nrestricted to very small models, limited training set sizes, and low-resolution\nimages. Such restrictions hinder their applicability to real-world scenarios.\nIn this paper, we present a novel approach enabling data reconstruction in\nrealistic settings for models trained on high-resolution images. Our method\nadapts the reconstruction scheme of arXiv:2206.07758 to real-world scenarios --\nspecifically, targeting models trained via transfer learning over image\nembeddings of large pre-trained models like DINO-ViT and CLIP. Our work employs\ndata reconstruction in the embedding space rather than in the image space,\nshowcasing its applicability beyond visual data. Moreover, we introduce a novel\nclustering-based method to identify good reconstructions from thousands of\ncandidates. This significantly improves on previous works that relied on\nknowledge of the training set to identify good reconstructed images. Our\nfindings shed light on a potential privacy risk for data leakage from models\ntrained using transfer learning.",
        "timestamp": "2025-06-12T17:19:31.441Z",
        "rating": "novote",
        "publishedDate": "2024-07-22T17:59:10Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CR",
          "cs.CV"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 37,
        "object_id": "paper:arxiv.2407.15845",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:19:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.20292": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.20292",
        "url": "https://arxiv.org/pdf/2412.20292",
        "title": "An analytic theory of creativity in convolutional diffusion models",
        "authors": "Mason Kamb, Surya Ganguli",
        "abstract": "We obtain an analytic, interpretable and predictive theory of creativity in\nconvolutional diffusion models. Indeed, score-matching diffusion models can\ngenerate highly original images that lie far from their training data. However,\noptimal score-matching theory suggests that these models should only be able to\nproduce memorized training examples. To reconcile this theory-experiment gap,\nwe identify two simple inductive biases, locality and equivariance, that: (1)\ninduce a form of combinatorial creativity by preventing optimal score-matching;\n(2) result in fully analytic, completely mechanistically interpretable, local\nscore (LS) and equivariant local score (ELS) machines that, (3) after\ncalibrating a single time-dependent hyperparameter can quantitatively predict\nthe outputs of trained convolution only diffusion models (like ResNets and\nUNets) with high accuracy (median $r^2$ of $0.95, 0.94, 0.94, 0.96$ for our top\nmodel on CIFAR10, FashionMNIST, MNIST, and CelebA). Our model reveals a locally\nconsistent patch mosaic mechanism of creativity, in which diffusion models\ncreate exponentially many novel images by mixing and matching different local\ntraining set patches at different scales and image locations. Our theory also\npartially predicts the outputs of pre-trained self-attention enabled UNets\n(median $r^2 \\sim 0.77$ on CIFAR10), revealing an intriguing role for attention\nin carving out semantic coherence from local patch mosaics.",
        "timestamp": "2025-06-12T17:19:31.247Z",
        "rating": "novote",
        "publishedDate": "2024-12-28T22:33:29Z",
        "tags": [
          "cs.LG",
          "cond-mat.dis-nn",
          "cs.AI",
          "q-bio.NC",
          "stat.ML",
          "I.2.10"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 36,
        "object_id": "paper:arxiv.2412.20292",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:19:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.21278": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.21278",
        "url": "https://arxiv.org/pdf/2502.21278",
        "title": "Does Generation Require Memorization? Creative Diffusion Models using\n  Ambient Diffusion",
        "authors": "Kulin Shah, Alkis Kalavasis, Adam R. Klivans, Giannis Daras",
        "abstract": "There is strong empirical evidence that the state-of-the-art diffusion\nmodeling paradigm leads to models that memorize the training set, especially\nwhen the training set is small. Prior methods to mitigate the memorization\nproblem often lead to a decrease in image quality. Is it possible to obtain\nstrong and creative generative models, i.e., models that achieve high\ngeneration quality and low memorization? Despite the current pessimistic\nlandscape of results, we make significant progress in pushing the trade-off\nbetween fidelity and memorization. We first provide theoretical evidence that\nmemorization in diffusion models is only necessary for denoising problems at\nlow noise scales (usually used in generating high-frequency details). Using\nthis theoretical insight, we propose a simple, principled method to train the\ndiffusion models using noisy data at large noise scales. We show that our\nmethod significantly reduces memorization without decreasing the image quality,\nfor both text-conditional and unconditional models and for a variety of data\navailability settings.",
        "timestamp": "2025-06-12T17:19:31.324Z",
        "rating": "novote",
        "publishedDate": "2025-02-28T17:57:48Z",
        "tags": [
          "cs.LG",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 35,
        "object_id": "paper:arxiv.2502.21278",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:20:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2307.03056": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2307.03056",
        "url": "https://arxiv.org/pdf/2307.03056",
        "title": "Generalizing Backpropagation for Gradient-Based Interpretability",
        "authors": "Kevin Du, Lucas Torroba Hennigen, Niklas Stoehr, Alexander Warstadt, Ryan Cotterell",
        "abstract": "Many popular feature-attribution methods for interpreting deep neural\nnetworks rely on computing the gradients of a model's output with respect to\nits inputs. While these methods can indicate which input features may be\nimportant for the model's prediction, they reveal little about the inner\nworkings of the model itself. In this paper, we observe that the gradient\ncomputation of a model is a special case of a more general formulation using\nsemirings. This observation allows us to generalize the backpropagation\nalgorithm to efficiently compute other interpretable statistics about the\ngradient graph of a neural network, such as the highest-weighted path and\nentropy. We implement this generalized algorithm, evaluate it on synthetic\ndatasets to better understand the statistics it computes, and apply it to study\nBERT's behavior on the subject-verb number agreement task (SVA). With this\nmethod, we (a) validate that the amount of gradient flow through a component of\na model reflects its importance to a prediction and (b) for SVA, identify which\npathways of the self-attention mechanism are most important.",
        "timestamp": "2025-06-12T17:19:30.465Z",
        "rating": "novote",
        "publishedDate": "2023-07-06T15:19:53Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 34,
        "object_id": "paper:arxiv.2307.03056",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:19:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.20760": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.20760",
        "url": "https://arxiv.org/pdf/2412.20760",
        "title": "Attributing Culture-Conditioned Generations to Pretraining Corpora",
        "authors": "Huihan Li, Arnav Goel, Keyu He, Xiang Ren",
        "abstract": "In open-ended generative tasks like narrative writing or dialogue, large\nlanguage models often exhibit cultural biases, showing limited knowledge and\ngenerating templated outputs for less prevalent cultures. Recent works show\nthat these biases may stem from uneven cultural representation in pretraining\ncorpora. This work investigates how pretraining leads to biased\nculture-conditioned generations by analyzing how models associate entities with\ncultures based on pretraining data patterns. We propose the MEMOed framework\n(MEMOrization from pretraining document) to determine whether a generation for\na culture arises from memorization. Using MEMOed on culture-conditioned\ngenerations about food and clothing for 110 cultures, we find that\nhigh-frequency cultures in pretraining data yield more generations with\nmemorized symbols, while some low-frequency cultures produce none.\nAdditionally, the model favors generating entities with extraordinarily high\nfrequency regardless of the conditioned culture, reflecting biases toward\nfrequent pretraining terms irrespective of relevance. We hope that the MEMOed\nframework and our insights will inspire more works on attributing model\nperformance on pretraining data.",
        "timestamp": "2025-06-12T17:19:30.409Z",
        "rating": "novote",
        "publishedDate": "2024-12-30T07:09:25Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 33,
        "object_id": "paper:arxiv.2412.20760",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:20:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2010.12016": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2010.12016",
        "url": "https://arxiv.org/pdf/2010.12016",
        "title": "Towards falsifiable interpretability research",
        "authors": "Matthew L. Leavitt, Ari Morcos",
        "abstract": "Methods for understanding the decisions of and mechanisms underlying deep\nneural networks (DNNs) typically rely on building intuition by emphasizing\nsensory or semantic features of individual examples. For instance, methods aim\nto visualize the components of an input which are \"important\" to a network's\ndecision, or to measure the semantic properties of single neurons. Here, we\nargue that interpretability research suffers from an over-reliance on\nintuition-based approaches that risk-and in some cases have caused-illusory\nprogress and misleading conclusions. We identify a set of limitations that we\nargue impede meaningful progress in interpretability research, and examine two\npopular classes of interpretability methods-saliency and single-neuron-based\napproaches-that serve as case studies for how overreliance on intuition and\nlack of falsifiability can undermine interpretability research. To address\nthese concerns, we propose a strategy to address these impediments in the form\nof a framework for strongly falsifiable interpretability research. We encourage\nresearchers to use their intuitions as a starting point to develop and test\nclear, falsifiable hypotheses, and hope that our framework yields robust,\nevidence-based interpretability methods that generate meaningful advances in\nour understanding of DNNs.",
        "timestamp": "2025-06-12T17:19:31.042Z",
        "rating": "novote",
        "publishedDate": "2020-10-22T22:03:41Z",
        "tags": [
          "cs.CY",
          "cs.AI",
          "cs.CV",
          "cs.LG",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 32,
        "object_id": "paper:arxiv.2010.12016",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:19:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.01558": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.01558",
        "url": "https://arxiv.org/pdf/2501.01558",
        "title": "Predicting the Performance of Black-box LLMs through Self-Queries",
        "authors": "Dylan Sam, Marc Finzi, J. Zico Kolter",
        "abstract": "As large language models (LLMs) are increasingly relied on in AI systems,\npredicting when they make mistakes is crucial. While a great deal of work in\nthe field uses internal representations to interpret model behavior, these\nrepresentations are inaccessible when given solely black-box access through an\nAPI. In this paper, we extract features of LLMs in a black-box manner by using\nfollow-up prompts and taking the probabilities of different responses as\nrepresentations to train reliable predictors of model behavior. We demonstrate\nthat training a linear model on these low-dimensional representations produces\nreliable and generalizable predictors of model performance at the instance\nlevel (e.g., if a particular generation correctly answers a question).\nRemarkably, these can often outperform white-box linear predictors that operate\nover a model's hidden state or the full distribution over its vocabulary. In\naddition, we demonstrate that these extracted features can be used to evaluate\nmore nuanced aspects of a language model's state. For instance, they can be\nused to distinguish between a clean version of GPT-4o-mini and a version that\nhas been influenced via an adversarial system prompt that answers\nquestion-answering tasks incorrectly or introduces bugs into generated code.\nFurthermore, they can reliably distinguish between different model\narchitectures and sizes, enabling the detection of misrepresented models\nprovided through an API (e.g., identifying if GPT-3.5 is supplied instead of\nGPT-4o-mini).",
        "timestamp": "2025-06-12T17:19:30.047Z",
        "rating": "novote",
        "publishedDate": "2025-01-02T22:26:54Z",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 31,
        "object_id": "paper:arxiv.2501.01558",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:19:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.13852": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.13852",
        "url": "https://arxiv.org/pdf/2410.13852",
        "title": "Retrospective Learning from Interactions",
        "authors": "Zizhao Chen, Mustafa Omer Gul, Yiwei Chen, Gloria Geng, Anne Wu, Yoav Artzi",
        "abstract": "Multi-turn interactions between large language models (LLMs) and users\nnaturally include implicit feedback signals. If an LLM responds in an\nunexpected way to an instruction, the user is likely to signal it by rephrasing\nthe request, expressing frustration, or pivoting to an alternative task. Such\nsignals are task-independent and occupy a relatively constrained subspace of\nlanguage, allowing the LLM to identify them even if it fails on the actual\ntask. We introduce ReSpect, a method to learn from such signals in past\ninteractions via retrospection without additional annotations. We deploy\nReSpect in a new multimodal interaction scenario, where humans instruct a\nmultimodal LLM to solve an abstract reasoning task with a combinatorial\nsolution space. Through thousands of interactions with humans, we show how\nReSpect gradually improves task completion rate from 31% to 82%, all without\nany external annotation.",
        "timestamp": "2025-06-12T17:19:30.033Z",
        "rating": "novote",
        "publishedDate": "2024-10-17T17:59:03Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CV",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 30,
        "object_id": "paper:arxiv.2410.13852",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:19:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2304.15004": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2304.15004",
        "url": "https://arxiv.org/pdf/2304.15004",
        "title": "Are Emergent Abilities of Large Language Models a Mirage?",
        "authors": "Rylan Schaeffer, Brando Miranda, Sanmi Koyejo",
        "abstract": "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.",
        "timestamp": "2025-06-12T17:19:30.034Z",
        "rating": "novote",
        "publishedDate": "2023-04-28T17:52:11Z",
        "tags": [
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 29,
        "object_id": "paper:arxiv.2304.15004",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:19:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.14491": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.14491",
        "url": "https://arxiv.org/pdf/2310.14491",
        "title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning\n  Capabilities of Language Models",
        "authors": "Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, Mrinmaya Sachan",
        "abstract": "Recent work has shown that language models (LMs) have strong multi-step\n(i.e., procedural) reasoning capabilities. However, it is unclear whether LMs\nperform these tasks by cheating with answers memorized from pretraining corpus,\nor, via a multi-step reasoning mechanism. In this paper, we try to answer this\nquestion by exploring a mechanistic interpretation of LMs for multi-step\nreasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a\nreasoning tree resembling the correct reasoning process within it. We test this\nhypothesis by introducing a new probing approach (called MechanisticProbe) that\nrecovers the reasoning tree from the model's attention patterns. We use our\nprobe to analyze two LMs: GPT-2 on a synthetic task (k-th smallest element),\nand LLaMA on two simple language-based reasoning tasks (ProofWriter & AI2\nReasoning Challenge). We show that MechanisticProbe is able to detect the\ninformation of the reasoning tree from the model's attentions for most\nexamples, suggesting that the LM indeed is going through a process of\nmulti-step reasoning within its architecture in many cases.",
        "timestamp": "2025-06-12T17:19:30.047Z",
        "rating": "novote",
        "publishedDate": "2023-10-23T01:47:29Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 28,
        "object_id": "paper:arxiv.2310.14491",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:19:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.05017": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.05017",
        "url": "https://arxiv.org/pdf/2505.05017",
        "title": "Scalable Multi-Stage Influence Function for Large Language Models via\n  Eigenvalue-Corrected Kronecker-Factored Parameterization",
        "authors": "Yuntai Bao, Xuhong Zhang, Tianyu Du, Xinkui Zhao, Jiang Zong, Hao Peng, Jianwei Yin",
        "abstract": "Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to\ndownstream tasks. Since the majority of knowledge is acquired during\npre-training, attributing the predictions of fine-tuned LLMs to their\npre-training data may provide valuable insights. Influence functions have been\nproposed as a means to explain model predictions based on training data.\nHowever, existing approaches fail to compute ``multi-stage'' influence and lack\nscalability to billion-scale LLMs.\n  In this paper, we propose the multi-stage influence function to attribute the\ndownstream predictions of fine-tuned LLMs to pre-training data under the\nfull-parameter fine-tuning paradigm. To enhance the efficiency and practicality\nof our multi-stage influence function, we leverage Eigenvalue-corrected\nKronecker-Factored (EK-FAC) parameterization for efficient approximation.\nEmpirical results validate the superior scalability of EK-FAC approximation and\nthe effectiveness of our multi-stage influence function. Additionally, case\nstudies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,\nwith exemplars illustrating insights provided by multi-stage influence\nestimates. Our code is public at\nhttps://github.com/colored-dye/multi_stage_influence_function.",
        "timestamp": "2025-06-12T17:19:30.047Z",
        "rating": "novote",
        "publishedDate": "2025-05-08T07:43:44Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 27,
        "object_id": "paper:arxiv.2505.05017",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:19:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.02550": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.02550",
        "url": "https://arxiv.org/pdf/2406.02550",
        "title": "Learning to grok: Emergence of in-context learning and skill composition\n  in modular arithmetic tasks",
        "authors": "Tianyu He, Darshil Doshi, Aritra Das, Andrey Gromov",
        "abstract": "Large language models can solve tasks that were not present in the training\nset. This capability is believed to be due to in-context learning and skill\ncomposition. In this work, we study the emergence of in-context learning and\nskill composition in a collection of modular arithmetic tasks. Specifically, we\nconsider a finite collection of linear modular functions $z = a \\, x + b \\, y\n\\;\\mathrm{mod}\\; p$ labeled by the vector $(a, b) \\in \\mathbb{Z}_p^2$. We use\nsome of these tasks for pre-training and the rest for out-of-distribution\ntesting. We empirically show that a GPT-style transformer exhibits a transition\nfrom in-distribution to out-of-distribution generalization as the number of\npre-training tasks increases. We find that the smallest model capable of\nout-of-distribution generalization requires two transformer blocks, while for\ndeeper models, the out-of-distribution generalization phase is\n\\emph{transient}, necessitating early stopping. Finally, we perform an\ninterpretability study of the pre-trained models, revealing highly structured\nrepresentations in both attention heads and MLPs; and discuss the learned\nalgorithms. Notably, we find an algorithmic shift in deeper models, as we go\nfrom few to many in-context examples.",
        "timestamp": "2025-06-12T17:19:30.047Z",
        "rating": "novote",
        "publishedDate": "2024-06-04T17:59:36Z",
        "tags": [
          "cs.LG",
          "cond-mat.dis-nn",
          "hep-th",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 26,
        "object_id": "paper:arxiv.2406.02550",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:19:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.05790": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.05790",
        "url": "https://arxiv.org/pdf/2501.05790",
        "title": "Understanding Impact of Human Feedback via Influence Functions",
        "authors": "Taywon Min, Haeone Lee, Yongchan Kwon, Kimin Lee",
        "abstract": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn\nsuitable reward models from human feedback to align large language models\n(LLMs) with human intentions. However, human feedback can often be noisy,\ninconsistent, or biased, especially when evaluating complex responses. Such\nfeedback can lead to misaligned reward signals, potentially causing unintended\nside effects during the RLHF process. To address these challenges, we explore\nthe use of influence functions to measure the impact of human feedback on the\nperformance of reward models. We propose a compute-efficient approximation\nmethod that enables the application of influence functions to LLM-based reward\nmodels and large-scale preference datasets. In our experiments, we demonstrate\ntwo key applications of influence functions: (1) detecting common forms of\nlabeler bias in human feedback datasets and (2) guiding labelers to refine\ntheir strategies to align more closely with expert feedback. By quantifying the\nimpact of human feedback on reward models, we believe that influence functions\ncan enhance feedback interpretability and contribute to scalable oversight in\nRLHF, helping labelers provide more accurate and consistent feedback. Source\ncode is available at https://github.com/mintaywon/IF_RLHF",
        "timestamp": "2025-06-12T17:19:30.047Z",
        "rating": "novote",
        "publishedDate": "2025-01-10T08:50:38Z",
        "tags": [
          "cs.AI",
          "cs.HC",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 25,
        "object_id": "paper:arxiv.2501.05790",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:20:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.13981": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.13981",
        "url": "https://arxiv.org/pdf/2411.13981",
        "title": "On the Fairness, Diversity and Reliability of Text-to-Image Generative\n  Models",
        "authors": "Jordan Vice, Naveed Akhtar, Leonid Sigal, Richard Hartley, Ajmal Mian",
        "abstract": "The rapid proliferation of multimodal generative models has sparked critical\ndiscussions on their reliability, fairness and potential for misuse. While\ntext-to-image models excel at producing high-fidelity, user-guided content,\nthey often exhibit unpredictable behaviors and vulnerabilities that can be\nexploited to manipulate class or concept representations. To address this, we\npropose an evaluation framework to assess model reliability by analyzing\nresponses to global and local perturbations in the embedding space, enabling\nthe identification of inputs that trigger unreliable or biased behavior. Beyond\nsocial implications, fairness and diversity are fundamental to defining robust\nand trustworthy model behavior. Our approach offers deeper insights into these\nessential aspects by evaluating: (i) generative diversity, measuring the\nbreadth of visual representations for learned concepts, and (ii) generative\nfairness, which examines the impact that removing concepts from input prompts\nhas on control, under a low guidance setup. Beyond these evaluations, our\nmethod lays the groundwork for detecting unreliable, bias-injected models and\ntracing the provenance of embedded biases. Our code is publicly available at\nhttps://github.com/JJ-Vice/T2I_Fairness_Diversity_Reliability.\n  Keywords: Fairness, Reliability, AI Ethics, Bias, Text-to-Image Models",
        "timestamp": "2025-06-12T17:19:30.033Z",
        "rating": "novote",
        "publishedDate": "2024-11-21T09:46:55Z",
        "tags": [
          "cs.CV",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 24,
        "object_id": "paper:arxiv.2411.13981",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:20:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.21333": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.21333",
        "url": "https://www.google.com/url?q=https://arxiv.org/pdf/2410.21333v1&sa=D&source=docs&ust=1749753109568724&usg=AOvVaw0ncfFvr0rH_C-qLvKOiHfd",
        "title": "Redirecting",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-12T17:31:51.725Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 64,
        "object_id": "paper:arxiv.2410.21333",
        "created_at": "2025-06-12T17:31:52+00:00",
        "updated_at": "2025-06-12T17:32:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.01542": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.01542",
        "url": "https://arxiv.org/pdf/2504.01542",
        "title": "Register Always Matters: Analysis of LLM Pretraining Data Through the\n  Lens of Language Variation",
        "authors": "Amanda Myntti, Erik Henriksson, Veronika Laippala, Sampo Pyysalo",
        "abstract": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labeling systems, datasets\nare divided into categories, frequently reducing to a binary: those passing the\nfilters deemed as valuable examples, others discarded as useless or\ndetrimental. However, a more detailed understanding of the contribution of\ndifferent kinds of texts to model performance is still largely lacking. In this\narticle, we present the first study utilizing registers (also known as genres)\n- a widely used standard in corpus linguistics to model linguistic variation -\nto curate pretraining datasets and investigate the effect of register on the\nperformance of LLMs. We perform comparative studies by training models with\nregister classified data and evaluating them using standard benchmarks, and\nshow that the register of pretraining data substantially affects model\nperformance. We uncover surprising relationships between the pretraining\nmaterial and the resulting models: using the News register results in subpar\nperformance, and on the contrary, including the Opinion class, covering texts\nsuch as reviews and opinion blogs, is highly beneficial. While a model trained\non the entire unfiltered dataset outperforms those trained on datasets limited\nto a single register, combining well-performing registers like\nHow-to-Instructions, Informational Description, and Opinion leads to major\nimprovements. Furthermore, analysis of individual benchmark results reveals key\ndifferences in the strengths and drawbacks of specific register classes as\npretraining data. These findings show that register is an important explainer\nof model variation and can facilitate more deliberate future data selection\npractices.",
        "timestamp": "2025-06-13T00:13:02.909Z",
        "rating": "novote",
        "publishedDate": "2025-04-02T09:30:24Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 69,
        "object_id": "paper:arxiv.2504.01542",
        "created_at": "2025-06-13T00:13:03+00:00",
        "updated_at": "2025-06-13T00:13:23+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.13259": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.13259",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-13T00:12:48.287Z",
            "data": {
              "session_id": "session_1749773567705_jkfryvy",
              "source_id": "arxiv",
              "paper_id": "2502.13259",
              "start_time": "2025-06-13T00:12:39.595Z",
              "end_time": "2025-06-13T00:12:47.705Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:30:57.593Z",
            "data": {
              "session_id": "session_1750977057455_52putu9",
              "source_id": "arxiv",
              "paper_id": "2502.13259",
              "start_time": "2025-06-26T22:30:34.934Z",
              "end_time": "2025-06-26T22:30:57.455Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "issue_number": 68,
        "object_id": "interactions:arxiv.2502.13259",
        "created_at": "2025-06-13T00:12:48+00:00",
        "updated_at": "2025-06-26T22:31:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.13259": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.13259",
        "url": "https://arxiv.org/pdf/2502.13259",
        "title": "HumT DumT: Measuring and controlling human-like language in LLMs",
        "authors": "Myra Cheng, Sunny Yu, Dan Jurafsky",
        "abstract": "Should LLMs generate language that makes them seem human? Human-like language\nmight improve user experience, but might also lead to deception, overreliance,\nand stereotyping. Assessing these potential impacts requires a systematic way\nto measure human-like tone in LLM outputs. We introduce HumT and SocioT,\nmetrics for human-like tone and other dimensions of social perceptions in text\ndata based on relative probabilities from an LLM. By measuring HumT across\npreference and usage datasets, we find that users prefer less human-like\noutputs from LLMs in many contexts. HumT also offers insights into the\nperceptions and impacts of anthropomorphism: human-like LLM outputs are highly\ncorrelated with warmth, social closeness, femininity, and low status, which are\nclosely linked to the aforementioned harms. We introduce DumT, a method using\nHumT to systematically control and reduce the degree of human-like tone while\npreserving model performance. DumT offers a practical approach for mitigating\nrisks associated with anthropomorphic language generation.",
        "timestamp": "2025-06-13T00:12:40.100Z",
        "rating": "novote",
        "publishedDate": "2025-02-18T20:04:09Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 67,
        "object_id": "paper:arxiv.2502.13259",
        "created_at": "2025-06-13T00:12:40+00:00",
        "updated_at": "2025-06-13T00:12:58+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.21333": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.21333",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-16T17:40:27.058Z",
            "data": {
              "session_id": "session_1750095626319_8xlktr3",
              "source_id": "arxiv",
              "paper_id": "2410.21333",
              "start_time": "2025-06-16T17:39:20.072Z",
              "end_time": "2025-06-16T17:40:26.319Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 1,
              "total_elapsed_seconds": 66
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:32:46.230Z",
            "data": {
              "session_id": "session_1750977165959_w0dvg1h",
              "source_id": "arxiv",
              "paper_id": "2410.21333",
              "start_time": "2025-06-26T22:32:24.191Z",
              "end_time": "2025-06-26T22:32:45.959Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "issue_number": 66,
        "object_id": "interactions:arxiv.2410.21333",
        "created_at": "2025-06-12T17:34:01+00:00",
        "updated_at": "2025-06-26T22:33:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.01542": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.01542",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-13T00:19:53.302Z",
            "data": {
              "session_id": "session_1749773992868_voch0zz",
              "source_id": "arxiv",
              "paper_id": "2504.01542",
              "start_time": "2025-06-13T00:19:30.292Z",
              "end_time": "2025-06-13T00:19:52.868Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:30:17.171Z",
            "data": {
              "session_id": "session_1750977016646_x9w526w",
              "source_id": "arxiv",
              "paper_id": "2504.01542",
              "start_time": "2025-06-26T22:28:53.130Z",
              "end_time": "2025-06-26T22:30:16.646Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 4,
              "total_elapsed_seconds": 84
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:32:23.950Z",
            "data": {
              "session_id": "session_1750977143936_ftcej74",
              "source_id": "arxiv",
              "paper_id": "2504.01542",
              "start_time": "2025-06-26T22:32:18.661Z",
              "end_time": "2025-06-26T22:32:23.936Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T14:23:04.316Z",
            "data": {
              "session_id": "session_1762093383580_xbky5u6",
              "source_id": "arxiv",
              "paper_id": "2504.01542",
              "start_time": "2025-11-02T14:20:30.986Z",
              "end_time": "2025-11-02T14:23:03.580Z",
              "heartbeat_count": 30,
              "duration_seconds": 150,
              "idle_seconds": 3,
              "total_elapsed_seconds": 153
            }
          }
        ]
      },
      "meta": {
        "issue_number": 71,
        "object_id": "interactions:arxiv.2504.01542",
        "created_at": "2025-06-13T00:19:53+00:00",
        "updated_at": "2025-11-02T14:23:28+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2402.04614": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.04614",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-16T17:38:41.293Z",
            "data": {
              "session_id": "session_1750095520635_kt1mgsj",
              "source_id": "arxiv",
              "paper_id": "2402.04614",
              "start_time": "2025-06-16T17:38:25.864Z",
              "end_time": "2025-06-16T17:38:40.635Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 75,
        "object_id": "interactions:arxiv.2402.04614",
        "created_at": "2025-06-16T17:38:41+00:00",
        "updated_at": "2025-06-16T17:39:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.04614": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.04614",
        "url": "https://arxiv.org/pdf/2402.04614",
        "title": "Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations\n  from Large Language Models",
        "authors": "Chirag Agarwal, Sree Harsha Tanneru, Himabindu Lakkaraju",
        "abstract": "Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.",
        "timestamp": "2025-06-16T17:38:25.865Z",
        "rating": "novote",
        "publishedDate": "2024-02-07T06:32:50Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 73,
        "object_id": "paper:arxiv.2402.04614",
        "created_at": "2025-06-16T17:38:26+00:00",
        "updated_at": "2025-06-16T17:38:50+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.18128": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.18128",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-24T21:10:15.574Z",
            "data": {
              "session_id": "session_1750799415330_zk9u2q9",
              "source_id": "arxiv",
              "paper_id": "2505.18128",
              "start_time": "2025-06-24T21:10:07.409Z",
              "end_time": "2025-06-24T21:10:15.330Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:44:58.396Z",
            "data": {
              "session_id": "session_1751568298170_vvnc60o",
              "source_id": "arxiv",
              "paper_id": "2505.18128",
              "start_time": "2025-07-03T18:44:36.451Z",
              "end_time": "2025-07-03T18:44:58.170Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T20:06:58.754Z",
            "data": {
              "session_id": "session_1762200418113_pbbyhqi",
              "source_id": "arxiv",
              "paper_id": "2505.18128",
              "start_time": "2025-11-03T20:06:01.975Z",
              "end_time": "2025-11-03T20:06:58.113Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 1,
              "total_elapsed_seconds": 56
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T20:17:39.684Z",
            "data": {
              "session_id": "session_1762201059679_zp2487a",
              "source_id": "arxiv",
              "paper_id": "2505.18128",
              "start_time": "2025-11-03T20:17:30.358Z",
              "end_time": "2025-11-03T20:17:39.679Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 72,
        "object_id": "interactions:arxiv.2505.18128",
        "created_at": "2025-06-13T21:59:55+00:00",
        "updated_at": "2025-11-03T20:18:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.03826": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.03826",
        "url": "https://arxiv.org/pdf/2502.03826",
        "title": "FairT2I: Mitigating Social Bias in Text-to-Image Generation via Large\n  Language Model-Assisted Detection and Attribute Rebalancing",
        "authors": "Jinya Sakurai, Issei Sato",
        "abstract": "The proliferation of Text-to-Image (T2I) models has revolutionized content\ncreation, providing powerful tools for diverse applications ranging from\nartistic expression to educational material development and marketing. Despite\nthese technological advancements, significant ethical concerns arise from these\nmodels' reliance on large-scale datasets that often contain inherent societal\nbiases. These biases are further amplified when AI-generated content is\nincluded in training data, potentially reinforcing and perpetuating stereotypes\nin the generated outputs. In this paper, we introduce FairT2I, a novel\nframework that harnesses large language models to detect and mitigate social\nbiases in T2I generation. Our framework comprises two key components: (1) an\nLLM-based bias detection module that identifies potential social biases in\ngenerated images based on text prompts, and (2) an attribute rebalancing module\nthat fine-tunes sensitive attributes within the T2I model to mitigate\nidentified biases. Our extensive experiments across various T2I models and\ndatasets show that FairT2I can significantly reduce bias while maintaining\nhigh-quality image generation. We conducted both qualitative user studies and\nquantitative non-parametric analyses in the generated image feature space,\nbuilding upon the occupational dataset introduced in the Stable Bias study. Our\nresults show that FairT2I successfully mitigates social biases and enhances the\ndiversity of sensitive attributes in generated images. We further demonstrate,\nusing the P2 dataset, that our framework can detect subtle biases that are\nchallenging for human observers to perceive, extending beyond\noccupation-related prompts. On the basis of these findings, we introduce a new\nbenchmark dataset for evaluating bias in T2I models.",
        "timestamp": "2025-06-16T17:43:13.817Z",
        "rating": "novote",
        "publishedDate": "2025-02-06T07:22:57Z",
        "tags": [
          "cs.CV"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 76,
        "object_id": "paper:arxiv.2502.03826",
        "created_at": "2025-06-16T17:43:14+00:00",
        "updated_at": "2025-06-16T17:43:33+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.03826": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.03826",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-16T17:46:16.757Z",
            "data": {
              "session_id": "session_1750095975634_f5nvvjn",
              "source_id": "arxiv",
              "paper_id": "2502.03826",
              "start_time": "2025-06-16T17:43:13.815Z",
              "end_time": "2025-06-16T17:46:15.634Z",
              "heartbeat_count": 36,
              "duration_seconds": 180,
              "idle_seconds": 2,
              "total_elapsed_seconds": 182
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-16T17:47:32.855Z",
            "data": {
              "session_id": "session_1750096052138_m8bg6t0",
              "source_id": "arxiv",
              "paper_id": "2502.03826",
              "start_time": "2025-06-16T17:46:54.617Z",
              "end_time": "2025-06-16T17:47:32.138Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 3,
              "total_elapsed_seconds": 38
            }
          }
        ]
      },
      "meta": {
        "issue_number": 78,
        "object_id": "interactions:arxiv.2502.03826",
        "created_at": "2025-06-16T17:46:17+00:00",
        "updated_at": "2025-06-16T17:47:50+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.11618": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.11618",
        "interactions": []
      },
      "meta": {
        "issue_number": 83,
        "object_id": "interactions:arxiv.2506.11618",
        "created_at": "2025-06-16T18:14:20+00:00",
        "updated_at": "2025-06-16T18:14:22+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.11673": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.11673",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-16T18:15:33.719Z",
            "data": {
              "session_id": "session_1750097733709_4mo4se7",
              "source_id": "arxiv",
              "paper_id": "2506.11673",
              "start_time": "2025-06-16T18:15:09.732Z",
              "end_time": "2025-06-16T18:15:33.709Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "issue_number": 82,
        "object_id": "interactions:arxiv.2506.11673",
        "created_at": "2025-06-16T18:14:01+00:00",
        "updated_at": "2025-06-16T18:15:53+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.11673": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.11673",
        "url": "https://arxiv.org/pdf/2506.11673",
        "title": "Improving Causal Interventions in Amnesic Probing with Mean Projection\n  or LEACE",
        "authors": "Alicja Dobrzeniecka, Antske Fokkens, Pia Sommerauer",
        "abstract": "Amnesic probing is a technique used to examine the influence of specific\nlinguistic information on the behaviour of a model. This involves identifying\nand removing the relevant information and then assessing whether the model's\nperformance on the main task changes. If the removed information is relevant,\nthe model's performance should decline. The difficulty with this approach lies\nin removing only the target information while leaving other information\nunchanged. It has been shown that Iterative Nullspace Projection (INLP), a\nwidely used removal technique, introduces random modifications to\nrepresentations when eliminating target information. We demonstrate that Mean\nProjection (MP) and LEACE, two proposed alternatives, remove information in a\nmore targeted manner, thereby enhancing the potential for obtaining behavioural\nexplanations through Amnesic Probing.",
        "timestamp": "2025-06-16T18:13:52.394Z",
        "rating": "novote",
        "publishedDate": "2025-06-13T11:07:14Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 81,
        "object_id": "paper:arxiv.2506.11673",
        "created_at": "2025-06-16T18:13:52+00:00",
        "updated_at": "2025-06-16T18:14:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.11618": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.11618",
        "url": "https://arxiv.org/pdf/2506.11618",
        "title": "Convergent Linear Representations of Emergent Misalignment",
        "authors": "Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda",
        "abstract": "Fine-tuning large language models on narrow datasets can cause them to\ndevelop broadly misaligned behaviours: a phenomena known as emergent\nmisalignment. However, the mechanisms underlying this misalignment, and why it\ngeneralizes beyond the training domain, are poorly understood, demonstrating\ncritical gaps in our knowledge of model alignment. In this work, we train and\nstudy a minimal model organism which uses just 9 rank-1 adapters to emergently\nmisalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently\nmisaligned models converge to similar representations of misalignment. We\ndemonstrate this convergence by extracting a 'misalignment direction' from one\nfine-tuned model's activations, and using it to effectively ablate misaligned\nbehaviour from fine-tunes using higher dimensional LoRAs and different\ndatasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further\npresent a set of experiments for directly interpreting the fine-tuning\nadapters, showing that six contribute to general misalignment, while two\nspecialise for misalignment in just the fine-tuning domain. Emergent\nmisalignment is a particularly salient example of undesirable and unexpected\nmodel behaviour and by advancing our understanding of the mechanisms behind it,\nwe hope to move towards being able to better understand and mitigate\nmisalignment more generally.",
        "timestamp": "2025-06-16T18:13:49.017Z",
        "rating": "novote",
        "publishedDate": "2025-06-13T09:39:54Z",
        "tags": [
          "cs.LG",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 79,
        "object_id": "paper:arxiv.2506.11618",
        "created_at": "2025-06-16T18:13:49+00:00",
        "updated_at": "2025-06-16T18:14:08+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.12152": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.12152",
        "interactions": []
      },
      "meta": {
        "issue_number": 85,
        "object_id": "interactions:arxiv.2506.12152",
        "created_at": "2025-06-17T17:16:39+00:00",
        "updated_at": "2025-06-17T17:16:41+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.12152": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.12152",
        "url": "https://arxiv.org/abs/2506.12152",
        "title": "Because we have LLMs, we Can and Should Pursue Agentic Interpretability",
        "authors": "Been Kim, John Hewitt, Neel Nanda, Noah Fiedel, Oyvind Tafjord",
        "abstract": "The era of Large Language Models (LLMs) presents a new opportunity for interpretability--agentic interpretability: a multi-turn conversation with an LLM wherein the LLM proactively assists human understanding by developing and leveraging a mental model of the user, which in turn enables humans to develop better mental models of the LLM. Such conversation is a new capability that traditional `inspective' interpretability methods (opening the black-box) do not use. Having a language model that aims to teach and explain--beyond just knowing how to talk--is similar to a teacher whose goal is to teach well, understanding that their success will be measured by the student's comprehension. While agentic interpretability may trade off completeness for interactivity, making it less suitable for high-stakes safety situations with potentially deceptive models, it leverages a cooperative model to discover potentially superhuman concepts that can improve humans' mental model of machines. Agentic interpretability introduces challenges, particularly in evaluation, due to what we call `human-entangled-in-the-loop' nature (humans responses are integral part of the algorithm), making the design and evaluation difficult. We discuss possible solutions and proxy goals. As LLMs approach human parity in many tasks, agentic interpretability's promise is to help humans learn the potentially superhuman concepts of the LLMs, rather than see us fall increasingly far from understanding them.",
        "timestamp": "2025-06-17T17:16:08.446Z",
        "rating": "novote",
        "publishedDate": "2025/06/13",
        "tags": [
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 84,
        "object_id": "paper:arxiv.2506.12152",
        "created_at": "2025-06-17T17:16:08+00:00",
        "updated_at": "2025-06-17T17:16:32+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.12229": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.12229",
        "url": "https://arxiv.org/pdf/2506.12229",
        "title": "Infini-gram mini: Exact n-gram Search at the Internet Scale with\n  FM-Index",
        "authors": "Hao Xu, Jiacheng Liu, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi",
        "abstract": "Language models are trained mainly on massive text data from the Internet,\nand it becomes increasingly important to understand this data source.\nExact-match search engines enable searching in large text corpora -- counting\nstring appearances and retrieving the enclosing documents -- yet the high\nstorage overhead hinders their application on Internet-scale data. We present\nInfini-gram mini, an efficient and scalable system that can make petabyte-level\ntext corpora searchable. Based on the FM-index data structure (Ferragina and\nManzini, 2000), which simultaneously indexes and compresses text, our system\ncreates indexes with size only 44% of the corpus. Infini-gram mini greatly\nimproves upon the best existing implementation of FM-index in terms of indexing\nspeed (18$\\times$) and memory use during both indexing (3.2$\\times$ reduction)\nand querying (down to a negligible amount). We index 46TB of Internet text in\n50 days with a single 128-core CPU node (or 19 hours if using 75 such nodes).\nWe show one important use case of Infini-gram mini in a large-scale analysis of\nbenchmark contamination. We find several core LM evaluation benchmarks to be\nheavily contaminated in Internet crawls (up to 40% in SQuAD), which could lead\nto overestimating the capabilities of language models if trained on such data.\nWe host a benchmark contamination bulletin to share the contamination rate of\nmany core and community-contributed benchmarks. We also release a web interface\nand an API endpoint to serve general search queries on Infini-gram mini\nindexes.",
        "timestamp": "2025-06-17T18:01:55.586Z",
        "rating": "novote",
        "publishedDate": "2025-06-13T21:13:57Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 86,
        "object_id": "paper:arxiv.2506.12229",
        "created_at": "2025-06-17T18:01:55+00:00",
        "updated_at": "2025-06-17T18:02:20+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.12229": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.12229",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-17T18:32:11.143Z",
            "data": {
              "session_id": "session_1750185129326_msk8i4k",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-06-17T18:24:00.954Z",
              "end_time": "2025-06-17T18:32:09.326Z",
              "heartbeat_count": 97,
              "duration_seconds": 485,
              "idle_seconds": 3,
              "total_elapsed_seconds": 488
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-17T18:34:44.826Z",
            "data": {
              "session_id": "session_1750185284198_b09w501",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-06-17T18:32:27.999Z",
              "end_time": "2025-06-17T18:34:44.198Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 1,
              "total_elapsed_seconds": 136
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-17T22:15:29.023Z",
            "data": {
              "session_id": "session_1750198528364_hzfoa4m",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-06-17T22:02:36.842Z",
              "end_time": "2025-06-17T22:15:28.364Z",
              "heartbeat_count": 154,
              "duration_seconds": 770,
              "idle_seconds": 2,
              "total_elapsed_seconds": 772
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:29:06.528Z",
            "data": {
              "session_id": "session_1751491746159_vlu288u",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-07-02T21:27:44.197Z",
              "end_time": "2025-07-02T21:29:06.159Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 2,
              "total_elapsed_seconds": 82
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:33:36.733Z",
            "data": {
              "session_id": "session_1751492016279_5gbleve",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-07-02T21:29:19.774Z",
              "end_time": "2025-07-02T21:33:36.279Z",
              "heartbeat_count": 51,
              "duration_seconds": 255,
              "idle_seconds": 2,
              "total_elapsed_seconds": 257
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:35:08.754Z",
            "data": {
              "session_id": "session_1751492108331_qp36tin",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-07-02T21:34:31.147Z",
              "end_time": "2025-07-02T21:35:08.331Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:37:21.536Z",
            "data": {
              "session_id": "session_1751492241149_t8usv8f",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-07-02T21:35:16.668Z",
              "end_time": "2025-07-02T21:37:21.149Z",
              "heartbeat_count": 24,
              "duration_seconds": 120,
              "idle_seconds": 4,
              "total_elapsed_seconds": 124
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:47:14.143Z",
            "data": {
              "session_id": "session_1751492833952_rp3s252",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-07-02T21:47:00.114Z",
              "end_time": "2025-07-02T21:47:13.952Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:55:08.711Z",
            "data": {
              "session_id": "session_1751493308356_j2vg0u7",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-07-02T21:47:22.216Z",
              "end_time": "2025-07-02T21:55:08.356Z",
              "heartbeat_count": 93,
              "duration_seconds": 465,
              "idle_seconds": 1,
              "total_elapsed_seconds": 466
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T13:59:55.318Z",
            "data": {
              "session_id": "session_1762783195310_r74gx1g",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-10T13:59:32.517Z",
              "end_time": "2025-11-10T13:59:55.310Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T14:02:23.838Z",
            "data": {
              "session_id": "session_1762783342563_4s0o52x",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-10T14:00:04.114Z",
              "end_time": "2025-11-10T14:02:22.563Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 3,
              "total_elapsed_seconds": 138
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T14:34:43.593Z",
            "data": {
              "session_id": "session_1762785282158_v1c21td",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-10T14:34:34.178Z",
              "end_time": "2025-11-10T14:34:42.158Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T15:00:02.782Z",
            "data": {
              "session_id": "session_1762786801548_gsrmzfr",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-10T14:45:43.475Z",
              "end_time": "2025-11-10T15:00:01.548Z",
              "heartbeat_count": 171,
              "duration_seconds": 855,
              "idle_seconds": 3,
              "total_elapsed_seconds": 858
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T15:53:29.811Z",
            "data": {
              "session_id": "session_1762790009214_nu8uy4f",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-10T15:51:13.523Z",
              "end_time": "2025-11-10T15:53:29.214Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 1,
              "total_elapsed_seconds": 136
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T18:21:44.002Z",
            "data": {
              "session_id": "session_1762798903699_uixvmf3",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-10T18:21:11.563Z",
              "end_time": "2025-11-10T18:21:43.699Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-11T12:55:29.940Z",
            "data": {
              "session_id": "session_1762865729915_v6he57r",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-11T12:55:24.720Z",
              "end_time": "2025-11-11T12:55:29.915Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-11T15:07:11.345Z",
            "data": {
              "session_id": "session_1762873631336_xaf7q3r",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-11T15:06:38.587Z",
              "end_time": "2025-11-11T15:07:11.336Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 3,
              "total_elapsed_seconds": 33
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-11T16:04:23.243Z",
            "data": {
              "session_id": "session_1762877062613_6c31akt",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-11T15:32:10.143Z",
              "end_time": "2025-11-11T16:04:22.613Z",
              "heartbeat_count": 386,
              "duration_seconds": 1930,
              "idle_seconds": 2,
              "total_elapsed_seconds": 1932
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T09:53:32.991Z",
            "data": {
              "session_id": "session_1762941212365_svqtivc",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-12T09:51:47.165Z",
              "end_time": "2025-11-12T09:53:32.365Z",
              "heartbeat_count": 21,
              "duration_seconds": 105,
              "idle_seconds": 0,
              "total_elapsed_seconds": 105
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T09:55:28.404Z",
            "data": {
              "session_id": "session_1762941327717_angqlws",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-12T09:53:40.729Z",
              "end_time": "2025-11-12T09:55:27.717Z",
              "heartbeat_count": 21,
              "duration_seconds": 105,
              "idle_seconds": 2,
              "total_elapsed_seconds": 107
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T17:27:40.472Z",
            "data": {
              "session_id": "session_1762968460462_795fx7l",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-12T17:27:35.288Z",
              "end_time": "2025-11-12T17:27:40.462Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-16T16:17:03.891Z",
            "data": {
              "session_id": "session_1763309823879_ntilo1r",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-16T16:16:41.360Z",
              "end_time": "2025-11-16T16:17:03.879Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-16T18:43:32.333Z",
            "data": {
              "session_id": "session_1763318612322_wdyiwvj",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-11-16T18:43:26.694Z",
              "end_time": "2025-11-16T18:43:32.322Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 87,
        "object_id": "interactions:arxiv.2506.12229",
        "created_at": "2025-06-17T18:23:34+00:00",
        "updated_at": "2025-11-16T18:44:07+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.23501": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.23501",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-17T18:43:18.046Z",
            "data": {
              "session_id": "session_1750185798041_53upc5d",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-17T18:43:16.610Z",
              "end_time": "2025-06-17T18:43:18.041Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-23T18:14:33.885Z",
            "data": {
              "session_id": "session_1750702473880_yt9k5yw",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-23T18:14:24.531Z",
              "end_time": "2025-06-23T18:14:33.880Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-24T16:08:54.240Z",
            "data": {
              "session_id": "session_1750781333980_tu21zrx",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-24T16:08:52.995Z",
              "end_time": "2025-06-24T16:08:53.980Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-24T21:07:13.490Z",
            "data": {
              "session_id": "session_1750799233200_y9smqj4",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-24T21:06:44.607Z",
              "end_time": "2025-06-24T21:07:13.200Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T18:00:05.530Z",
            "data": {
              "session_id": "session_1750874405121_yhm5ndo",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-25T17:56:12.809Z",
              "end_time": "2025-06-25T18:00:05.121Z",
              "heartbeat_count": 46,
              "duration_seconds": 230,
              "idle_seconds": 2,
              "total_elapsed_seconds": 232
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T22:27:11.681Z",
            "data": {
              "session_id": "session_1750890430990_d8yzx7k",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-25T22:19:47.182Z",
              "end_time": "2025-06-25T22:27:10.990Z",
              "heartbeat_count": 88,
              "duration_seconds": 440,
              "idle_seconds": 4,
              "total_elapsed_seconds": 444
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T22:36:04.125Z",
            "data": {
              "session_id": "session_1750890963677_biwkk2h",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-25T22:35:58.004Z",
              "end_time": "2025-06-25T22:36:03.677Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:58:10.261Z",
            "data": {
              "session_id": "session_1751493490223_198cgrv",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T21:57:17.251Z",
              "end_time": "2025-07-02T21:58:10.223Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 3,
              "total_elapsed_seconds": 53
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:59:40.607Z",
            "data": {
              "session_id": "session_1751493580233_n3qdzlp",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T21:58:14.341Z",
              "end_time": "2025-07-02T21:59:40.233Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 1,
              "total_elapsed_seconds": 86
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T22:01:12.935Z",
            "data": {
              "session_id": "session_1751493672518_69m813a",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T22:00:36.295Z",
              "end_time": "2025-07-02T22:01:12.518Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 1,
              "total_elapsed_seconds": 36
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T22:29:09.665Z",
            "data": {
              "session_id": "session_1751495349280_r9v2bt2",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T22:27:18.440Z",
              "end_time": "2025-07-02T22:29:09.280Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 1,
              "total_elapsed_seconds": 111
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T22:33:14.358Z",
            "data": {
              "session_id": "session_1751495594344_afnjewt",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T22:33:08.211Z",
              "end_time": "2025-07-02T22:33:14.344Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T22:40:20.751Z",
            "data": {
              "session_id": "session_1751496020369_z8y5b7z",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T22:33:57.421Z",
              "end_time": "2025-07-02T22:40:20.369Z",
              "heartbeat_count": 76,
              "duration_seconds": 380,
              "idle_seconds": 3,
              "total_elapsed_seconds": 383
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T22:55:50.213Z",
            "data": {
              "session_id": "session_1751496949823_yvnydpq",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T22:50:19.343Z",
              "end_time": "2025-07-02T22:55:49.822Z",
              "heartbeat_count": 66,
              "duration_seconds": 330,
              "idle_seconds": 0,
              "total_elapsed_seconds": 330
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T23:03:39.110Z",
            "data": {
              "session_id": "session_1751497418674_g8987st",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T22:55:59.068Z",
              "end_time": "2025-07-02T23:03:38.674Z",
              "heartbeat_count": 91,
              "duration_seconds": 455,
              "idle_seconds": 5,
              "total_elapsed_seconds": 460
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T23:08:56.046Z",
            "data": {
              "session_id": "session_1751497735648_lc9mn3b",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T23:03:51.287Z",
              "end_time": "2025-07-02T23:08:55.648Z",
              "heartbeat_count": 60,
              "duration_seconds": 300,
              "idle_seconds": 4,
              "total_elapsed_seconds": 304
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T23:21:04.273Z",
            "data": {
              "session_id": "session_1751498463886_pow7k71",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T23:08:59.685Z",
              "end_time": "2025-07-02T23:21:03.886Z",
              "heartbeat_count": 144,
              "duration_seconds": 720,
              "idle_seconds": 4,
              "total_elapsed_seconds": 724
            }
          }
        ]
      },
      "meta": {
        "issue_number": 88,
        "object_id": "interactions:arxiv.2410.23501",
        "created_at": "2025-06-17T18:38:15+00:00",
        "updated_at": "2025-07-02T23:21:31+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.05017": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.05017",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-18T00:51:49.537Z",
            "data": {
              "session_id": "session_1750207908061_v27lxoz",
              "source_id": "arxiv",
              "paper_id": "2505.05017",
              "start_time": "2025-06-18T00:51:40.505Z",
              "end_time": "2025-06-18T00:51:48.061Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T23:07:47.552Z",
            "data": {
              "session_id": "session_1750979267332_yp1yqm1",
              "source_id": "arxiv",
              "paper_id": "2505.05017",
              "start_time": "2025-06-26T23:07:39.330Z",
              "end_time": "2025-06-26T23:07:47.332Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T10:35:27.944Z",
            "data": {
              "session_id": "session_1761993327676_62845p1",
              "source_id": "arxiv",
              "paper_id": "2505.05017",
              "start_time": "2025-11-01T10:35:14.122Z",
              "end_time": "2025-11-01T10:35:27.676Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "issue_number": 89,
        "object_id": "interactions:arxiv.2505.05017",
        "created_at": "2025-06-18T00:51:50+00:00",
        "updated_at": "2025-11-01T10:35:48+00:00",
        "version": 1
      }
    },
    "paper:openreview.tPNHOoZFl9": {
      "data": {
        "sourceId": "openreview",
        "paperId": "tPNHOoZFl9",
        "url": "https://openreview.net/forum?id=tPNHOoZFl9",
        "title": "Learning Dynamics of LLM Finetuning",
        "authors": "Yi Ren, Danica J. Sutherland",
        "abstract": "Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, \ngives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique ``squeezing effect'' to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance.",
        "timestamp": "2025-06-18T17:33:36.585Z",
        "rating": "novote",
        "publishedDate": "22 Jan 2025",
        "tags": [
          "Learning dynamics",
          "LLM",
          "finetuning",
          "DPO"
        ],
        "doi": "",
        "journalName": "ICLR 2025 Oral",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 91,
        "object_id": "paper:openreview.tPNHOoZFl9",
        "created_at": "2025-06-18T17:33:36+00:00",
        "updated_at": "2025-06-18T17:33:58+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.13886": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.13886",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:27:11.125Z",
            "data": {
              "session_id": "session_1751578030950_sm9rau5",
              "source_id": "arxiv",
              "paper_id": "2506.13886",
              "start_time": "2025-07-03T21:27:01.508Z",
              "end_time": "2025-07-03T21:27:10.949Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 93,
        "object_id": "interactions:arxiv.2506.13886",
        "created_at": "2025-06-18T20:08:11+00:00",
        "updated_at": "2025-07-03T21:27:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.13886": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.13886",
        "url": "https://arxiv.org/pdf/2506.13886",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-18T20:07:45.466Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 92,
        "object_id": "paper:arxiv.2506.13886",
        "created_at": "2025-06-18T20:07:45+00:00",
        "updated_at": "2025-06-18T20:08:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.08872": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.08872",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-18T20:09:48.467Z",
            "data": {
              "session_id": "session_1750277388432_wa51wiv",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-18T20:09:33.492Z",
              "end_time": "2025-06-18T20:09:48.432Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-22T21:11:02.332Z",
            "data": {
              "session_id": "session_1750626661672_5ylsih6",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-22T21:10:47.545Z",
              "end_time": "2025-06-22T21:11:01.672Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-23T17:46:59.550Z",
            "data": {
              "session_id": "session_1750700819038_t0mgd1y",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-23T17:45:41.603Z",
              "end_time": "2025-06-23T17:46:59.038Z",
              "heartbeat_count": 15,
              "duration_seconds": 75,
              "idle_seconds": 2,
              "total_elapsed_seconds": 77
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-23T17:59:04.553Z",
            "data": {
              "session_id": "session_1750701543912_bxdaxpr",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-23T17:58:29.780Z",
              "end_time": "2025-06-23T17:59:03.912Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-24T16:08:52.997Z",
            "data": {
              "session_id": "session_1750781332994_ym08pko",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-24T16:08:33.097Z",
              "end_time": "2025-06-24T16:08:52.994Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:24:20.126Z",
            "data": {
              "session_id": "session_1750976659635_b7ztxtx",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-26T22:18:48.664Z",
              "end_time": "2025-06-26T22:24:19.635Z",
              "heartbeat_count": 66,
              "duration_seconds": 330,
              "idle_seconds": 1,
              "total_elapsed_seconds": 331
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:25:29.148Z",
            "data": {
              "session_id": "session_1750976729144_oukloqi",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-26T22:25:21.866Z",
              "end_time": "2025-06-26T22:25:29.144Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 96,
        "object_id": "interactions:arxiv.2506.08872",
        "created_at": "2025-06-18T20:09:49+00:00",
        "updated_at": "2025-06-26T22:25:50+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.08872": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.08872",
        "url": "https://arxiv.org/pdf/2506.08872",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-18T20:09:29.973Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 95,
        "object_id": "paper:arxiv.2506.08872",
        "created_at": "2025-06-18T20:09:30+00:00",
        "updated_at": "2025-06-18T20:09:53+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.08324": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.08324",
        "url": "https://arxiv.org/html/2411.08324v1",
        "title": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-18T20:09:04.974Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 94,
        "object_id": "paper:arxiv.2411.08324",
        "created_at": "2025-06-18T20:09:05+00:00",
        "updated_at": "2025-06-18T20:09:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.06298": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.06298",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-22T19:58:05.405Z",
            "data": {
              "session_id": "session_1750622284879_mabvs41",
              "source_id": "arxiv",
              "paper_id": "2506.06298",
              "start_time": "2025-06-22T19:57:41.387Z",
              "end_time": "2025-06-22T19:58:04.879Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-22T20:01:40.848Z",
            "data": {
              "session_id": "session_1750622500408_yeai48u",
              "source_id": "arxiv",
              "paper_id": "2506.06298",
              "start_time": "2025-06-22T20:01:06.077Z",
              "end_time": "2025-06-22T20:01:40.408Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          }
        ]
      },
      "meta": {
        "issue_number": 99,
        "object_id": "interactions:arxiv.2506.06298",
        "created_at": "2025-06-22T19:58:05+00:00",
        "updated_at": "2025-06-22T20:01:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.06298": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.06298",
        "url": "https://arxiv.org/pdf/2506.06298",
        "title": "Pairwise Calibrated Rewards for Pluralistic Alignment",
        "authors": "Daniel Halpern, Evi Micha, Ariel D. Procaccia, Itai Shapira",
        "abstract": "Current alignment pipelines presume a single, universal notion of desirable\nbehavior. However, human preferences often diverge across users, contexts, and\ncultures. As a result, disagreement collapses into the majority signal and\nminority perspectives are discounted. To address this, we propose reflecting\ndiverse human preferences through a distribution over multiple reward\nfunctions, each inducing a distinct aligned policy. The distribution is learned\ndirectly from pairwise preference without annotator identifiers or predefined\ngroups. Instead, annotator disagreements are treated as informative soft\nlabels. Our central criterion is pairwise calibration: for every pair of\ncandidate responses, the proportion of reward functions preferring one response\nmatches the fraction of annotators with that preference. We prove that even a\nsmall outlier-free ensemble can accurately represent diverse preference\ndistributions. Empirically, we introduce and validate a practical training\nheuristic to learn such ensembles, and demonstrate its effectiveness through\nimproved calibration, implying a more faithful representation of pluralistic\nvalues.",
        "timestamp": "2025-06-22T19:57:41.869Z",
        "rating": "novote",
        "publishedDate": "2025-05-17T18:38:24Z",
        "tags": [
          "cs.LG",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 97,
        "object_id": "paper:arxiv.2506.06298",
        "created_at": "2025-06-22T19:57:42+00:00",
        "updated_at": "2025-06-22T19:58:03+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.14200": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.14200",
        "url": "https://arxiv.org/pdf/2506.14200",
        "title": "ELI-Why: Evaluating the Pedagogical Utility of Language Model\n  Explanations",
        "authors": "Brihi Joshi, Keyu He, Sahana Ramnath, Sadra Sabouri, Kaitlyn Zhou, Souti Chattopadhyay, Swabha Swayamdipta, Xiang Ren",
        "abstract": "Language models today are widely used in education, yet their ability to\ntailor responses for learners with varied informational needs and knowledge\nbackgrounds remains under-explored. To this end, we introduce ELI-Why, a\nbenchmark of 13.4K \"Why\" questions to evaluate the pedagogical capabilities of\nlanguage models. We then conduct two extensive human studies to assess the\nutility of language model-generated explanatory answers (explanations) on our\nbenchmark, tailored to three distinct educational grades: elementary,\nhigh-school and graduate school. In our first study, human raters assume the\nrole of an \"educator\" to assess model explanations' fit to different\neducational grades. We find that GPT-4-generated explanations match their\nintended educational background only 50% of the time, compared to 79% for lay\nhuman-curated explanations. In our second study, human raters assume the role\nof a learner to assess if an explanation fits their own informational needs.\nAcross all educational backgrounds, users deemed GPT-4-generated explanations\n20% less suited on average to their informational needs, when compared to\nexplanations curated by lay people. Additionally, automated evaluation metrics\nreveal that explanations generated across different language model families for\ndifferent informational needs remain indistinguishable in their grade-level,\nlimiting their pedagogical effectiveness.",
        "timestamp": "2025-06-23T18:25:55.019Z",
        "rating": "novote",
        "publishedDate": "2025-06-17T05:36:39Z",
        "tags": [
          "cs.CL",
          "cs.HC"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 100,
        "object_id": "paper:arxiv.2506.14200",
        "created_at": "2025-06-23T18:25:55+00:00",
        "updated_at": "2025-06-23T18:26:22+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.15758": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.15758",
        "url": "https://arxiv.org/pdf/2404.15758?",
        "title": "Let's Think Dot by Dot: Hidden Computation in Transformer Language\n  Models",
        "authors": "Jacob Pfau, William Merrill, Samuel R. Bowman",
        "abstract": "Chain-of-thought responses from language models improve performance across\nmost benchmarks. However, it remains unclear to what extent these performance\ngains can be attributed to human-like task decomposition or simply the greater\ncomputation that additional tokens allow. We show that transformers can use\nmeaningless filler tokens (e.g., '......') in place of a chain of thought to\nsolve two hard algorithmic tasks they could not solve when responding without\nintermediate tokens. However, we find empirically that learning to use filler\ntokens is difficult and requires specific, dense supervision to converge. We\nalso provide a theoretical characterization of the class of problems where\nfiller tokens are useful in terms of the quantifier depth of a first-order\nformula. For problems satisfying this characterization, chain-of-thought tokens\nneed not provide information about the intermediate computational steps\ninvolved in multi-token computations. In summary, our results show that\nadditional tokens can provide computational benefits independent of token\nchoice. The fact that intermediate tokens can act as filler tokens raises\nconcerns about large language models engaging in unauditable, hidden\ncomputations that are increasingly detached from the observed chain-of-thought\ntokens.",
        "timestamp": "2025-06-23T21:49:21.232Z",
        "rating": "novote",
        "publishedDate": "2024-04-24T09:30:00Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "I.2.6"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 101,
        "object_id": "paper:arxiv.2404.15758",
        "created_at": "2025-06-23T21:49:21+00:00",
        "updated_at": "2025-06-23T21:49:40+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.15758": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.15758",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-23T21:51:04.458Z",
            "data": {
              "session_id": "session_1750715463752_oh486es",
              "source_id": "arxiv",
              "paper_id": "2404.15758",
              "start_time": "2025-06-23T21:49:20.794Z",
              "end_time": "2025-06-23T21:51:03.752Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 3,
              "total_elapsed_seconds": 103
            }
          }
        ]
      },
      "meta": {
        "issue_number": 102,
        "object_id": "interactions:arxiv.2404.15758",
        "created_at": "2025-06-23T21:51:04+00:00",
        "updated_at": "2025-06-23T21:51:24+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2403.14653": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.14653",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T14:27:04.652Z",
            "data": {
              "session_id": "session_1750861623868_3zmswio",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-06-25T14:26:57.595Z",
              "end_time": "2025-06-25T14:27:03.868Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T17:52:11.022Z",
            "data": {
              "session_id": "session_1750873930624_tkgnau3",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-06-25T17:48:46.062Z",
              "end_time": "2025-06-25T17:52:10.624Z",
              "heartbeat_count": 40,
              "duration_seconds": 200,
              "idle_seconds": 5,
              "total_elapsed_seconds": 205
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T22:19:33.469Z",
            "data": {
              "session_id": "session_1750889973067_iyx5r60",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-06-25T22:16:46.384Z",
              "end_time": "2025-06-25T22:19:33.067Z",
              "heartbeat_count": 33,
              "duration_seconds": 165,
              "idle_seconds": 2,
              "total_elapsed_seconds": 167
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T23:00:20.237Z",
            "data": {
              "session_id": "session_1750892420038_2m7nnw2",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-06-25T23:00:18.959Z",
              "end_time": "2025-06-25T23:00:20.038Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:45:21.717Z",
            "data": {
              "session_id": "session_1751568321705_8oy5xqx",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-07-03T18:45:13.509Z",
              "end_time": "2025-07-03T18:45:21.705Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T19:17:59.401Z",
            "data": {
              "session_id": "session_1751570279142_or3am8b",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-07-03T19:17:47.693Z",
              "end_time": "2025-07-03T19:17:59.142Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:06:33.500Z",
            "data": {
              "session_id": "session_1751576793486_1dhgws4",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-07-03T21:05:36.038Z",
              "end_time": "2025-07-03T21:06:33.486Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 2,
              "total_elapsed_seconds": 57
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:15:32.679Z",
            "data": {
              "session_id": "session_1751577332673_dc5n751",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-07-03T21:15:21.364Z",
              "end_time": "2025-07-03T21:15:32.673Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 105,
        "object_id": "interactions:arxiv.2403.14653",
        "created_at": "2025-06-25T14:27:05+00:00",
        "updated_at": "2025-07-03T21:15:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.14653": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.14653",
        "url": "https://arxiv.org/abs/2403.14653",
        "title": "Between Copyright and Computer Science: The Law and Ethics of Generative AI",
        "authors": "Deven R. Desai, Mark Riedl",
        "abstract": "Copyright and computer science continue to intersect and clash, but they can coexist. The advent of new technologies such as digitization of visual and aural creations, sharing technologies, search engines, social media offerings, and more challenge copyright-based industries and reopen questions about the reach of copyright law. Breakthroughs in artificial intelligence research, especially Large Language Models that leverage copyrighted material as part of training models, are the latest examples of the ongoing tension between copyright and computer science. The exuberance, rush-to-market, and edge problem cases created by a few misguided companies now raises challenges to core legal doctrines and may shift Open Internet practices for the worse. That result does not have to be, and should not be, the outcome.\nThis Article shows that, contrary to some scholars' views, fair use law does not bless all ways that someone can gain access to copyrighted material even when the purpose is fair use. Nonetheless, the scientific need for more data to advance AI research means access to large book corpora and the Open Internet is vital for the future of that research. The copyright industry claims, however, that almost all uses of copyrighted material must be compensated, even for non-expressive uses. The Article's solution accepts that both sides need to change. It is one that forces the computer science world to discipline its behaviors and, in some cases, pay for copyrighted material. It also requires the copyright industry to abandon its belief that all uses must be compensated or restricted to uses sanctioned by the copyright industry. As part of this re-balancing, the Article addresses a problem that has grown out of this clash and under theorized.",
        "timestamp": "2025-06-25T14:26:55.279Z",
        "rating": "novote",
        "publishedDate": "2024/02/24",
        "tags": [
          "Computers and Society (cs.CY)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 103,
        "object_id": "paper:arxiv.2403.14653",
        "created_at": "2025-06-25T14:26:55+00:00",
        "updated_at": "2025-06-25T14:27:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2006.03010": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2006.03010",
        "url": "https://arxiv.org/abs/2006.03010",
        "title": "Syntactic Search by Example",
        "authors": "Micah Shlain, Hillel Taub-Tabib, Shoval Sadde, Yoav Goldberg",
        "abstract": "We present a system that allows a user to search a large linguistically annotated corpus using syntactic patterns over dependency graphs. In contrast to previous attempts to this effect, we introduce a light-weight query language that does not require the user to know the details of the underlying syntactic representations, and instead to query the corpus by providing an example sentence coupled with simple markup. Search is performed at an interactive speed due to an efficient linguistic graph-indexing and retrieval engine. This allows for rapid exploration, development and refinement of syntax-based queries. We demonstrate the system using queries over two corpora: the English wikipedia, and a collection of English pubmed abstracts. A demo of the wikipedia system is available at: this https URL",
        "timestamp": "2025-06-25T18:10:46.604Z",
        "rating": "novote",
        "publishedDate": "2020/06/04",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 106,
        "object_id": "paper:arxiv.2006.03010",
        "created_at": "2025-06-25T18:10:47+00:00",
        "updated_at": "2025-06-25T18:11:14+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.22592": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.22592",
        "url": "https://arxiv.org/abs/2410.22592",
        "title": "GRADE: Quantifying Sample Diversity in Text-to-Image Models",
        "authors": "Royi Rassin, Aviv Slobodkin, Shauli Ravfogel, Yanai Elazar, Yoav Goldberg",
        "abstract": "We introduce GRADE, an automatic method for quantifying sample diversity in text-to-image models. Our method leverages the world knowledge embedded in large language models and visual question-answering systems to identify relevant concept-specific axes of diversity (e.g., ``shape'' for the concept ``cookie''). It then estimates frequency distributions of concepts and their attributes and quantifies diversity using entropy. We use GRADE to measure the diversity of 12 models over a total of 720K images, revealing that all models display limited variation, with clear deterioration in stronger models. Further, we find that models often exhibit default behaviors, a phenomenon where a model consistently generates concepts with the same attributes (e.g., 98% of the cookies are round). Lastly, we show that a key reason for low diversity is underspecified captions in training data. Our work proposes an automatic, semantically-driven approach to measure sample diversity and highlights the stunning homogeneity in text-to-image models.",
        "timestamp": "2025-06-25T22:50:30.897Z",
        "rating": "novote",
        "publishedDate": "2024/10/29",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 107,
        "object_id": "paper:arxiv.2410.22592",
        "created_at": "2025-06-25T22:50:31+00:00",
        "updated_at": "2025-06-25T22:50:50+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.22592": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.22592",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T22:55:12.492Z",
            "data": {
              "session_id": "session_1750892112449_i8lpcj0",
              "source_id": "arxiv",
              "paper_id": "2410.22592",
              "start_time": "2025-06-25T22:55:03.802Z",
              "end_time": "2025-06-25T22:55:12.449Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T23:00:19.369Z",
            "data": {
              "session_id": "session_1750892418959_dtb978p",
              "source_id": "arxiv",
              "paper_id": "2410.22592",
              "start_time": "2025-06-25T22:55:39.567Z",
              "end_time": "2025-06-25T23:00:18.959Z",
              "heartbeat_count": 55,
              "duration_seconds": 275,
              "idle_seconds": 4,
              "total_elapsed_seconds": 279
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T00:23:21.896Z",
            "data": {
              "session_id": "session_1750897401877_fw0a2bl",
              "source_id": "arxiv",
              "paper_id": "2410.22592",
              "start_time": "2025-06-26T00:22:30.140Z",
              "end_time": "2025-06-26T00:23:21.877Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 2,
              "total_elapsed_seconds": 52
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T17:54:24.990Z",
            "data": {
              "session_id": "session_1750960464319_0c20b2q",
              "source_id": "arxiv",
              "paper_id": "2410.22592",
              "start_time": "2025-06-26T17:53:26.185Z",
              "end_time": "2025-06-26T17:54:24.319Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 3,
              "total_elapsed_seconds": 58
            }
          }
        ]
      },
      "meta": {
        "issue_number": 108,
        "object_id": "interactions:arxiv.2410.22592",
        "created_at": "2025-06-25T22:55:02+00:00",
        "updated_at": "2025-06-26T17:54:48+00:00",
        "version": 1
      }
    },
    "interactions:openreview.tPNHOoZFl9": {
      "data": {
        "sourceId": "openreview",
        "paperId": "tPNHOoZFl9",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:25:53.273Z",
            "data": {
              "session_id": "session_1750976753057_65m6l41",
              "source_id": "openreview",
              "paper_id": "tPNHOoZFl9",
              "start_time": "2025-06-26T22:25:46.440Z",
              "end_time": "2025-06-26T22:25:53.057Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 109,
        "object_id": "interactions:openreview.tPNHOoZFl9",
        "created_at": "2025-06-26T22:25:53+00:00",
        "updated_at": "2025-06-26T22:26:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.12120": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.12120",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T20:22:21.629Z",
            "data": {
              "session_id": "session_1752006141416_ntnndz7",
              "source_id": "arxiv",
              "paper_id": "2502.12120",
              "start_time": "2025-07-08T20:22:01.156Z",
              "end_time": "2025-07-08T20:22:21.416Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T20:26:57.397Z",
            "data": {
              "session_id": "session_1752006417388_fir4wrf",
              "source_id": "arxiv",
              "paper_id": "2502.12120",
              "start_time": "2025-07-08T20:26:34.742Z",
              "end_time": "2025-07-08T20:26:57.388Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T20:29:08.178Z",
            "data": {
              "session_id": "session_1752006548173_a8kduha",
              "source_id": "arxiv",
              "paper_id": "2502.12120",
              "start_time": "2025-07-08T20:28:28.616Z",
              "end_time": "2025-07-08T20:29:08.173Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 5,
              "total_elapsed_seconds": 40
            }
          }
        ]
      },
      "meta": {
        "issue_number": 114,
        "object_id": "interactions:arxiv.2502.12120",
        "created_at": "2025-06-26T22:36:01+00:00",
        "updated_at": "2025-07-08T20:29:32+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.12120": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.12120",
        "url": "https://arxiv.org/abs/2502.12120",
        "title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws",
        "authors": "Prasanna Mayilvahanan, Thadd\u00e4us Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel",
        "abstract": "Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.",
        "timestamp": "2025-06-26T22:35:53.903Z",
        "rating": "novote",
        "publishedDate": "2025/02/17",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 113,
        "object_id": "paper:arxiv.2502.12120",
        "created_at": "2025-06-26T22:35:54+00:00",
        "updated_at": "2025-06-26T22:36:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.04741": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.04741",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:35:51.470Z",
            "data": {
              "session_id": "session_1750977351273_b5h40du",
              "source_id": "arxiv",
              "paper_id": "2505.04741",
              "start_time": "2025-06-26T22:35:34.061Z",
              "end_time": "2025-06-26T22:35:51.273Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "issue_number": 112,
        "object_id": "interactions:arxiv.2505.04741",
        "created_at": "2025-06-26T22:35:52+00:00",
        "updated_at": "2025-06-26T22:36:10+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.08679": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08679",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:35:30.896Z",
            "data": {
              "session_id": "session_1750977330627_sxgmmia",
              "source_id": "arxiv",
              "paper_id": "2503.08679",
              "start_time": "2025-06-26T22:35:16.533Z",
              "end_time": "2025-06-26T22:35:30.627Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T06:38:34.629Z",
            "data": {
              "session_id": "session_1760683114444_63gropz",
              "source_id": "arxiv",
              "paper_id": "2503.08679",
              "start_time": "2025-10-17T06:38:04.124Z",
              "end_time": "2025-10-17T06:38:34.444Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 0,
              "total_elapsed_seconds": 30
            }
          }
        ]
      },
      "meta": {
        "issue_number": 111,
        "object_id": "interactions:arxiv.2503.08679",
        "created_at": "2025-06-26T22:35:31+00:00",
        "updated_at": "2025-10-17T06:38:56+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2408.03247": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.03247",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:34:59.105Z",
            "data": {
              "session_id": "session_1750977299099_gsn6ml1",
              "source_id": "arxiv",
              "paper_id": "2408.03247",
              "start_time": "2025-06-26T22:34:43.568Z",
              "end_time": "2025-06-26T22:34:59.099Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "issue_number": 110,
        "object_id": "interactions:arxiv.2408.03247",
        "created_at": "2025-06-26T22:34:59+00:00",
        "updated_at": "2025-06-26T22:35:23+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2310.13121": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.13121",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:37:22.182Z",
            "data": {
              "session_id": "session_1750977441990_du0j0aq",
              "source_id": "arxiv",
              "paper_id": "2310.13121",
              "start_time": "2025-06-26T22:37:08.852Z",
              "end_time": "2025-06-26T22:37:21.990Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "issue_number": 118,
        "object_id": "interactions:arxiv.2310.13121",
        "created_at": "2025-06-26T22:37:22+00:00",
        "updated_at": "2025-06-26T22:37:46+00:00",
        "version": 1
      }
    },
    "interactions:openreview.HvSytvg3Jh": {
      "data": {
        "sourceId": "openreview",
        "paperId": "HvSytvg3Jh",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:37:08.365Z",
            "data": {
              "session_id": "session_1750977428111_e9cg9ns",
              "source_id": "openreview",
              "paper_id": "HvSytvg3Jh",
              "start_time": "2025-06-26T22:36:39.931Z",
              "end_time": "2025-06-26T22:37:08.111Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          }
        ]
      },
      "meta": {
        "issue_number": 117,
        "object_id": "interactions:openreview.HvSytvg3Jh",
        "created_at": "2025-06-26T22:37:08+00:00",
        "updated_at": "2025-06-26T22:37:30+00:00",
        "version": 1
      }
    },
    "interactions:openreview.HD6bWcj87Y": {
      "data": {
        "sourceId": "openreview",
        "paperId": "HD6bWcj87Y",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T10:44:58.234Z",
            "data": {
              "session_id": "session_1761993898221_6jr3fy8",
              "source_id": "openreview",
              "paper_id": "HD6bWcj87Y",
              "start_time": "2025-11-01T10:44:50.051Z",
              "end_time": "2025-11-01T10:44:58.221Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 116,
        "object_id": "interactions:openreview.HD6bWcj87Y",
        "created_at": "2025-06-26T22:36:40+00:00",
        "updated_at": "2025-11-01T10:45:20+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.00985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.00985",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:36:29.270Z",
            "data": {
              "session_id": "session_1750977388995_l7hc41c",
              "source_id": "arxiv",
              "paper_id": "2505.00985",
              "start_time": "2025-06-26T22:36:13.462Z",
              "end_time": "2025-06-26T22:36:28.995Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "issue_number": 115,
        "object_id": "interactions:arxiv.2505.00985",
        "created_at": "2025-06-26T22:36:29+00:00",
        "updated_at": "2025-06-26T22:36:51+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.09858": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09858",
        "url": "https://arxiv.org/abs/2504.09858",
        "title": "Reasoning Models Can Be Effective Without Thinking",
        "authors": "Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, Matei Zaharia",
        "abstract": "Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling.",
        "timestamp": "2025-06-26T22:38:53.205Z",
        "rating": "novote",
        "publishedDate": "2025/04/14",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 121,
        "object_id": "paper:arxiv.2504.09858",
        "created_at": "2025-06-26T22:38:53+00:00",
        "updated_at": "2025-06-26T22:39:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.09522": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09522",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:23:38.689Z",
            "data": {
              "session_id": "session_1760721818341_8qux928",
              "source_id": "arxiv",
              "paper_id": "2504.09522",
              "start_time": "2025-10-17T17:23:31.196Z",
              "end_time": "2025-10-17T17:23:38.341Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 120,
        "object_id": "interactions:arxiv.2504.09522",
        "created_at": "2025-06-26T22:38:40+00:00",
        "updated_at": "2025-10-17T17:24:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.18114": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.18114",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:38:19.948Z",
            "data": {
              "session_id": "session_1750977499736_co3bk0b",
              "source_id": "arxiv",
              "paper_id": "2504.18114",
              "start_time": "2025-06-26T22:37:25.727Z",
              "end_time": "2025-06-26T22:38:19.736Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 4,
              "total_elapsed_seconds": 54
            }
          }
        ]
      },
      "meta": {
        "issue_number": 119,
        "object_id": "interactions:arxiv.2504.18114",
        "created_at": "2025-06-26T22:38:20+00:00",
        "updated_at": "2025-06-26T22:38:41+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.21934": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21934",
        "interactions": []
      },
      "meta": {
        "issue_number": 123,
        "object_id": "interactions:arxiv.2503.21934",
        "created_at": "2025-06-26T22:39:47+00:00",
        "updated_at": "2025-06-26T22:39:49+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.21934": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21934",
        "url": "https://arxiv.org/abs/2503.21934",
        "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad",
        "authors": "Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovi\u0107, Nikola Jovanovi\u0107, Martin Vechev",
        "abstract": "Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce the first comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly: only Gemini-2.5-Pro achieves a non-trivial score of 25%, while all other models achieve less than 5%. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities.",
        "timestamp": "2025-06-26T22:39:32.573Z",
        "rating": "novote",
        "publishedDate": "2025/03/27",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 122,
        "object_id": "paper:arxiv.2503.21934",
        "created_at": "2025-06-26T22:39:32+00:00",
        "updated_at": "2025-06-26T22:40:02+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2501.17148": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.17148",
        "interactions": []
      },
      "meta": {
        "issue_number": 128,
        "object_id": "interactions:arxiv.2501.17148",
        "created_at": "2025-06-26T22:43:13+00:00",
        "updated_at": "2025-06-26T22:43:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2311.12786": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.12786",
        "interactions": []
      },
      "meta": {
        "issue_number": 127,
        "object_id": "interactions:arxiv.2311.12786",
        "created_at": "2025-06-26T22:42:55+00:00",
        "updated_at": "2025-06-26T22:42:57+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2408.12578": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.12578",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:42:44.191Z",
            "data": {
              "session_id": "session_1750977763894_pvti6ps",
              "source_id": "arxiv",
              "paper_id": "2408.12578",
              "start_time": "2025-06-26T22:42:32.922Z",
              "end_time": "2025-06-26T22:42:43.894Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 126,
        "object_id": "interactions:arxiv.2408.12578",
        "created_at": "2025-06-26T22:42:44+00:00",
        "updated_at": "2025-06-26T22:43:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.04289": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.04289",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:42:20.692Z",
            "data": {
              "session_id": "session_1750977740588_8yjc7zf",
              "source_id": "arxiv",
              "paper_id": "2406.04289",
              "start_time": "2025-06-26T22:42:14.188Z",
              "end_time": "2025-06-26T22:42:20.588Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 125,
        "object_id": "interactions:arxiv.2406.04289",
        "created_at": "2025-06-26T22:42:21+00:00",
        "updated_at": "2025-06-26T22:42:41+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2210.10749": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.10749",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:42:12.305Z",
            "data": {
              "session_id": "session_1750977732291_pvlhzfz",
              "source_id": "arxiv",
              "paper_id": "2210.10749",
              "start_time": "2025-06-26T22:41:52.814Z",
              "end_time": "2025-06-26T22:42:12.291Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 4,
              "total_elapsed_seconds": 19
            }
          }
        ]
      },
      "meta": {
        "issue_number": 124,
        "object_id": "interactions:arxiv.2210.10749",
        "created_at": "2025-06-26T22:42:12+00:00",
        "updated_at": "2025-06-26T22:42:36+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.11985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11985",
        "interactions": []
      },
      "meta": {
        "issue_number": 131,
        "object_id": "interactions:arxiv.2503.11985",
        "created_at": "2025-06-26T22:44:22+00:00",
        "updated_at": "2025-06-26T22:44:23+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.11985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11985",
        "url": "https://arxiv.org/abs/2503.11985",
        "title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language Models",
        "authors": "Charaka Vinayak Kumar, Ashok Urlana, Gopichand Kanumolu, Bala Mallikarjunarao Garlapati, Pruthwik Mishra",
        "abstract": "Advancements in Large Language Models (LLMs) have increased the performance of different natural language understanding as well as generation tasks. Although LLMs have breached the state-of-the-art performance in various tasks, they often reflect different forms of bias present in the training data. In the light of this perceived limitation, we provide a unified evaluation of benchmarks using a set of representative small and medium-sized LLMs that cover different forms of biases starting from physical characteristics to socio-economic categories. Moreover, we propose five prompting approaches to carry out the bias detection task across different aspects of bias. Further, we formulate three research questions to gain valuable insight in detecting biases in LLMs using different approaches and evaluation metrics across benchmarks. The results indicate that each of the selected LLMs suffer from one or the other form of bias with the Phi-3.5B model being the least biased. Finally, we conclude the paper with the identification of key challenges and possible future directions.",
        "timestamp": "2025-06-26T22:44:15.683Z",
        "rating": "novote",
        "publishedDate": "2025/03/15",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 130,
        "object_id": "paper:arxiv.2503.11985",
        "created_at": "2025-06-26T22:44:15+00:00",
        "updated_at": "2025-06-26T22:44:33+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.10061": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.10061",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:43:25.994Z",
            "data": {
              "session_id": "session_1750977805562_4m204um",
              "source_id": "arxiv",
              "paper_id": "2503.10061",
              "start_time": "2025-06-26T22:43:19.462Z",
              "end_time": "2025-06-26T22:43:25.562Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 129,
        "object_id": "interactions:arxiv.2503.10061",
        "created_at": "2025-06-26T22:43:26+00:00",
        "updated_at": "2025-06-26T22:43:45+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2409.12183": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.12183",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:46:29.873Z",
            "data": {
              "session_id": "session_1750977989869_2q6i5sq",
              "source_id": "arxiv",
              "paper_id": "2409.12183",
              "start_time": "2025-06-26T22:46:21.162Z",
              "end_time": "2025-06-26T22:46:29.869Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 135,
        "object_id": "interactions:arxiv.2409.12183",
        "created_at": "2025-06-26T22:45:44+00:00",
        "updated_at": "2025-06-26T22:46:47+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2411.12580": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.12580",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T09:34:35.939Z",
            "data": {
              "session_id": "session_1761989675660_g9vbl2l",
              "source_id": "arxiv",
              "paper_id": "2411.12580",
              "start_time": "2025-11-01T09:34:29.825Z",
              "end_time": "2025-11-01T09:34:35.660Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T09:37:17.142Z",
            "data": {
              "session_id": "session_1761989836510_ddwr7y4",
              "source_id": "arxiv",
              "paper_id": "2411.12580",
              "start_time": "2025-11-01T09:34:35.660Z",
              "end_time": "2025-11-01T09:37:16.510Z",
              "heartbeat_count": 32,
              "duration_seconds": 160,
              "idle_seconds": 1,
              "total_elapsed_seconds": 161
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T09:38:15.832Z",
            "data": {
              "session_id": "session_1761989895818_80zkrfl",
              "source_id": "arxiv",
              "paper_id": "2411.12580",
              "start_time": "2025-11-01T09:38:05.652Z",
              "end_time": "2025-11-01T09:38:15.818Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "issue_number": 134,
        "object_id": "interactions:arxiv.2411.12580",
        "created_at": "2025-06-26T22:45:36+00:00",
        "updated_at": "2025-11-01T09:38:35+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.14481": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14481",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:45:25.329Z",
            "data": {
              "session_id": "session_1750977925232_jdhzshm",
              "source_id": "arxiv",
              "paper_id": "2503.14481",
              "start_time": "2025-06-26T22:45:17.481Z",
              "end_time": "2025-06-26T22:45:25.232Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 133,
        "object_id": "interactions:arxiv.2503.14481",
        "created_at": "2025-06-26T22:45:25+00:00",
        "updated_at": "2025-06-26T22:45:46+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2006.07710": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2006.07710",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:45:10.662Z",
            "data": {
              "session_id": "session_1750977910655_qta4tik",
              "source_id": "arxiv",
              "paper_id": "2006.07710",
              "start_time": "2025-06-26T22:44:40.578Z",
              "end_time": "2025-06-26T22:45:10.655Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 0,
              "total_elapsed_seconds": 30
            }
          }
        ]
      },
      "meta": {
        "issue_number": 132,
        "object_id": "interactions:arxiv.2006.07710",
        "created_at": "2025-06-26T22:45:11+00:00",
        "updated_at": "2025-06-26T22:45:31+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.01822": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.01822",
        "interactions": []
      },
      "meta": {
        "issue_number": 139,
        "object_id": "interactions:arxiv.2503.01822",
        "created_at": "2025-06-26T22:47:04+00:00",
        "updated_at": "2025-06-26T22:47:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.01822": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.01822",
        "url": "https://arxiv.org/abs/2503.01822",
        "title": "Projecting Assumptions: The Duality Between Sparse Autoencoders and Concept Geometry",
        "authors": "Sai Sumedh R. Hindupur, Ekdeep Singh Lubana, Thomas Fel, Demba Ba",
        "abstract": "Sparse Autoencoders (SAEs) are widely used to interpret neural networks by identifying meaningful concepts from their representations. However, do SAEs truly uncover all concepts a model relies on, or are they inherently biased toward certain kinds of concepts? We introduce a unified framework that recasts SAEs as solutions to a bilevel optimization problem, revealing a fundamental challenge: each SAE imposes structural assumptions about how concepts are encoded in model representations, which in turn shapes what it can and cannot detect. This means different SAEs are not interchangeable -- switching architectures can expose entirely new concepts or obscure existing ones. To systematically probe this effect, we evaluate SAEs across a spectrum of settings: from controlled toy models that isolate key variables, to semi-synthetic experiments on real model activations and finally to large-scale, naturalistic datasets. Across this progression, we examine two fundamental properties that real-world concepts often exhibit: heterogeneity in intrinsic dimensionality (some concepts are inherently low-dimensional, others are not) and nonlinear separability. We show that SAEs fail to recover concepts when these properties are ignored, and we design a new SAE that explicitly incorporates both, enabling the discovery of previously hidden concepts and reinforcing our theoretical insights. Our findings challenge the idea of a universal SAE and underscores the need for architecture-specific choices in model interpretability. Overall, we argue an SAE does not just reveal concepts -- it determines what can be seen at all.",
        "timestamp": "2025-06-26T22:46:59.177Z",
        "rating": "novote",
        "publishedDate": "2025/03/03",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 138,
        "object_id": "paper:arxiv.2503.01822",
        "created_at": "2025-06-26T22:46:59+00:00",
        "updated_at": "2025-06-26T22:47:17+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.03862": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.03862",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:46:52.455Z",
            "data": {
              "session_id": "session_1750978012437_szcq18d",
              "source_id": "arxiv",
              "paper_id": "2503.03862",
              "start_time": "2025-06-26T22:46:37.271Z",
              "end_time": "2025-06-26T22:46:52.437Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 137,
        "object_id": "interactions:arxiv.2503.03862",
        "created_at": "2025-06-26T22:46:52+00:00",
        "updated_at": "2025-06-26T22:47:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.03862": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.03862",
        "url": "https://arxiv.org/abs/2503.03862",
        "title": "Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions",
        "authors": "Emmy Liu, Amanda Bertsch, Lintang Sutawika, Lindia Tjuatja, Patrick Fernandes, Lara Marinov, Michael Chen, Shreya Singhal, Carolin Lawrence, Aditi Raghunathan, Kiril Gashteovski, Graham Neubig",
        "abstract": "Improvements in language model capabilities are often attributed to increasing model size or training data, but in some cases smaller models trained on curated data or with different architectural decisions can outperform larger ones trained on more tokens. What accounts for this? To quantify the impact of these design choices, we meta-analyze 92 open-source pretrained models across a wide array of scales, including state-of-the-art open-weights models as well as less performant models and those with less conventional design decisions. We find that by incorporating features besides model size and number of training tokens, we can achieve a relative 3-28% increase in ability to predict downstream performance compared with using scale alone. Analysis of model design decisions reveal insights into data composition, such as the trade-off between language and code tasks at 15-25\\% code, as well as the better performance of some architectural decisions such as choosing rotary over learned embeddings. Broadly, our framework lays a foundation for more systematic investigation of how model development choices shape final capabilities.",
        "timestamp": "2025-06-26T22:46:34.099Z",
        "rating": "novote",
        "publishedDate": "2025/03/05",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 136,
        "object_id": "paper:arxiv.2503.03862",
        "created_at": "2025-06-26T22:46:34+00:00",
        "updated_at": "2025-06-26T22:46:56+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2412.20292": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.20292",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:58:27.854Z",
            "data": {
              "session_id": "session_1750978707460_3d5tc43",
              "source_id": "arxiv",
              "paper_id": "2412.20292",
              "start_time": "2025-06-26T22:56:12.824Z",
              "end_time": "2025-06-26T22:58:27.460Z",
              "heartbeat_count": 26,
              "duration_seconds": 130,
              "idle_seconds": 5,
              "total_elapsed_seconds": 135
            }
          }
        ]
      },
      "meta": {
        "issue_number": 141,
        "object_id": "interactions:arxiv.2412.20292",
        "created_at": "2025-06-26T22:51:46+00:00",
        "updated_at": "2025-06-26T22:58:44+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2407.15845": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.15845",
        "interactions": []
      },
      "meta": {
        "issue_number": 140,
        "object_id": "interactions:arxiv.2407.15845",
        "created_at": "2025-06-26T22:50:50+00:00",
        "updated_at": "2025-06-26T22:50:51+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2411.13981": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.13981",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T23:07:01.907Z",
            "data": {
              "session_id": "session_1750979221895_ohycwt5",
              "source_id": "arxiv",
              "paper_id": "2411.13981",
              "start_time": "2025-06-26T23:06:52.446Z",
              "end_time": "2025-06-26T23:07:01.895Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 144,
        "object_id": "interactions:arxiv.2411.13981",
        "created_at": "2025-06-26T23:07:02+00:00",
        "updated_at": "2025-06-26T23:07:21+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.02550": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.02550",
        "interactions": []
      },
      "meta": {
        "issue_number": 143,
        "object_id": "interactions:arxiv.2406.02550",
        "created_at": "2025-06-26T23:03:33+00:00",
        "updated_at": "2025-06-26T23:03:34+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2412.20760": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.20760",
        "interactions": []
      },
      "meta": {
        "issue_number": 142,
        "object_id": "interactions:arxiv.2412.20760",
        "created_at": "2025-06-26T23:00:54+00:00",
        "updated_at": "2025-06-26T23:00:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.09070": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.09070",
        "url": "https://arxiv.org/pdf/2406.09070",
        "title": "FairCoT: Enhancing Fairness in Text-to-Image Generation via Chain of\n  Thought Reasoning with Multimodal Large Language Models",
        "authors": "Zahraa Al Sahili, Ioannis Patras, Matthew Purver",
        "abstract": "In the domain of text-to-image generative models, biases inherent in training\ndatasets often propagate into generated content, posing significant ethical\nchallenges, particularly in socially sensitive contexts. We introduce FairCoT,\na novel framework that enhances fairness in text to image models through Chain\nof Thought (CoT) reasoning within multimodal generative large language models.\nFairCoT employs iterative CoT refinement to systematically mitigate biases, and\ndynamically adjusts textual prompts in real time, ensuring diverse and\nequitable representation in generated images. By integrating iterative\nreasoning processes, FairCoT addresses the limitations of zero shot CoT in\nsensitive scenarios, balancing creativity with ethical responsibility.\nExperimental evaluations across popular text-to-image systems including DALLE\nand various Stable Diffusion variants, demonstrate that FairCoT significantly\nenhances fairness and diversity without sacrificing image quality or semantic\nfidelity. By combining robust reasoning, lightweight deployment, and\nextensibility to multiple models, FairCoT represents a promising step toward\nmore socially responsible and transparent AI driven content generation.",
        "timestamp": "2025-07-01T17:39:36.486Z",
        "rating": "novote",
        "publishedDate": "2024-06-13T12:55:10Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CV"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 145,
        "object_id": "paper:arxiv.2406.09070",
        "created_at": "2025-07-01T17:39:36+00:00",
        "updated_at": "2025-07-01T17:40:02+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.09070": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.09070",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-01T17:41:06.306Z",
            "data": {
              "session_id": "session_1751391665868_tbqw9us",
              "source_id": "arxiv",
              "paper_id": "2406.09070",
              "start_time": "2025-07-01T17:39:36.217Z",
              "end_time": "2025-07-01T17:41:05.868Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 5,
              "total_elapsed_seconds": 90
            }
          }
        ]
      },
      "meta": {
        "issue_number": 146,
        "object_id": "interactions:arxiv.2406.09070",
        "created_at": "2025-07-01T17:41:06+00:00",
        "updated_at": "2025-07-01T17:41:34+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.03239": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.03239",
        "url": "https://arxiv.org/abs/2402.03239",
        "title": "Bluesky and the AT Protocol: Usable Decentralized Social Media",
        "authors": "Martin Kleppmann, Paul Frazee, Jake Gold, Jay Graber, Daniel Holmgren, Devin Ivy, Jeromy Johnson, Bryan Newbold, Jaz Volpert",
        "abstract": "Bluesky is a new social network built upon the AT Protocol, a decentralized foundation for public social media. It was launched in private beta in February 2023, and has grown to over 10 million registered users by October 2024. In this paper we introduce the architecture of Bluesky and the AT Protocol, and explain how the technical design of Bluesky is informed by our goals: to enable decentralization by having multiple interoperable providers for every part of the system; to make it easy for users to switch providers; to give users agency over the content they see; and to provide a simple user experience that does not burden users with complexity arising from the system's decentralized nature. The system's openness allows anybody to contribute to content moderation and community management, and we invite the research community to use Bluesky as a dataset and testing ground for new approaches in social media moderation.",
        "timestamp": "2025-07-01T17:50:32.101Z",
        "rating": "novote",
        "publishedDate": "2024/02/05",
        "tags": [
          "Distributed",
          "Parallel",
          "and Cluster Computing (cs.DC)",
          "Social and Information Networks (cs.SI)"
        ],
        "doi": "10.1145/3694809.3700740",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 147,
        "object_id": "paper:arxiv.2402.03239",
        "created_at": "2025-07-01T17:50:32+00:00",
        "updated_at": "2025-07-01T17:50:58+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.14685": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.14685",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:24:14.578Z",
            "data": {
              "session_id": "session_1751567054109_rip3ilw",
              "source_id": "arxiv",
              "paper_id": "2505.14685",
              "start_time": "2025-07-03T18:14:42.649Z",
              "end_time": "2025-07-03T18:24:14.109Z",
              "heartbeat_count": 114,
              "duration_seconds": 570,
              "idle_seconds": 1,
              "total_elapsed_seconds": 571
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:29:25.628Z",
            "data": {
              "session_id": "session_1751567365152_t8qmic4",
              "source_id": "arxiv",
              "paper_id": "2505.14685",
              "start_time": "2025-07-03T18:27:31.236Z",
              "end_time": "2025-07-03T18:29:25.152Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 4,
              "total_elapsed_seconds": 114
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:41:57.872Z",
            "data": {
              "session_id": "session_1751568117452_pfu748b",
              "source_id": "arxiv",
              "paper_id": "2505.14685",
              "start_time": "2025-07-03T18:40:53.435Z",
              "end_time": "2025-07-03T18:41:57.452Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 4,
              "total_elapsed_seconds": 64
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:43:40.015Z",
            "data": {
              "session_id": "session_1751568220004_957h28n",
              "source_id": "arxiv",
              "paper_id": "2505.14685",
              "start_time": "2025-07-03T18:43:20.478Z",
              "end_time": "2025-07-03T18:43:40.004Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "issue_number": 149,
        "object_id": "interactions:arxiv.2505.14685",
        "created_at": "2025-07-02T18:27:01+00:00",
        "updated_at": "2025-07-03T18:44:11+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2204.10628": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2204.10628",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:45:34.591Z",
            "data": {
              "session_id": "session_1751492734046_o6j6z7r",
              "source_id": "arxiv",
              "paper_id": "2204.10628",
              "start_time": "2025-07-02T21:44:48.461Z",
              "end_time": "2025-07-02T21:45:34.046Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 1,
              "total_elapsed_seconds": 46
            }
          }
        ]
      },
      "meta": {
        "issue_number": 151,
        "object_id": "interactions:arxiv.2204.10628",
        "created_at": "2025-07-02T21:37:41+00:00",
        "updated_at": "2025-07-02T21:45:53+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2204.10628": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2204.10628",
        "url": "https://arxiv.org/pdf/2204.10628",
        "title": "Autoregressive Search Engines: Generating Substrings as Document\n  Identifiers",
        "authors": "Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, Fabio Petroni",
        "abstract": "Knowledge-intensive language tasks require NLP systems to both provide the\ncorrect answer and retrieve supporting evidence for it in a given corpus.\nAutoregressive language models are emerging as the de-facto standard for\ngenerating answers, with newer and more powerful systems emerging at an\nastonishing pace. In this paper we argue that all this (and future) progress\ncan be directly applied to the retrieval problem with minimal intervention to\nthe models' architecture. Previous work has explored ways to partition the\nsearch space into hierarchical structures and retrieve documents by\nautoregressively generating their unique identifier. In this work we propose an\nalternative that doesn't force any structure in the search space: using all\nngrams in a passage as its possible identifiers. This setup allows us to use an\nautoregressive model to generate and score distinctive ngrams, that are then\nmapped to full passages through an efficient data structure. Empirically, we\nshow this not only outperforms prior autoregressive approaches but also leads\nto an average improvement of at least 10 points over more established retrieval\nsolutions for passage-level retrieval on the KILT benchmark, establishing new\nstate-of-the-art downstream performance on some datasets, while using a\nconsiderably lighter memory footprint than competing systems. Code and\npre-trained models at https://github.com/facebookresearch/SEAL.",
        "timestamp": "2025-07-02T21:37:33.933Z",
        "rating": "novote",
        "publishedDate": "2022-04-22T10:45:01Z",
        "tags": [
          "cs.CL",
          "cs.IR"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 150,
        "object_id": "paper:arxiv.2204.10628",
        "created_at": "2025-07-02T21:37:34+00:00",
        "updated_at": "2025-07-02T21:37:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.18777": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.18777",
        "url": "https://arxiv.org/abs/2506.18777",
        "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training",
        "authors": "Jonathan Cook, Silvia Sapora, Arash Ahmadian, Akbir Khan, Tim Rocktaschel, Jakob Foerster, Laura Ruis",
        "abstract": "Training large language models (LLMs) on source code significantly enhances their general-purpose reasoning abilities, but the mechanisms underlying this generalisation are poorly understood. In this paper, we propose Programming by Backprop (PBB) as a potential driver of this effect - teaching a model to evaluate a program for inputs by training on its source code alone, without ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of programs representing simple maths problems and algorithms: one with source code and I/O examples (w/ IO), the other with source code only (w/o IO). We find evidence that LLMs have some ability to evaluate w/o IO programs for inputs in a range of experimental settings, and make several observations. Firstly, PBB works significantly better when programs are provided as code rather than semantically equivalent language descriptions. Secondly, LLMs can produce outputs for w/o IO programs directly, by implicitly evaluating the program within the forward pass, and more reliably when stepping through the program in-context via chain-of-thought. We further show that PBB leads to more robust evaluation of programs across inputs than training on I/O pairs drawn from a distribution that mirrors naturally occurring data. Our findings suggest a mechanism for enhanced reasoning through code training: it allows LLMs to internalise reusable algorithmic abstractions. Significant scope remains for future work to enable LLMs to more effectively learn from symbolic procedures, and progress in this direction opens other avenues like model alignment by training on formal constitutional principles.",
        "timestamp": "2025-07-03T18:46:46.748Z",
        "rating": "novote",
        "publishedDate": "2025/06/23",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 152,
        "object_id": "paper:arxiv.2506.18777",
        "created_at": "2025-07-03T18:46:47+00:00",
        "updated_at": "2025-07-03T18:47:03+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.18777": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.18777",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:49:05.331Z",
            "data": {
              "session_id": "session_1751568544752_xx16te1",
              "source_id": "arxiv",
              "paper_id": "2506.18777",
              "start_time": "2025-07-03T18:46:49.814Z",
              "end_time": "2025-07-03T18:49:04.752Z",
              "heartbeat_count": 26,
              "duration_seconds": 130,
              "idle_seconds": 5,
              "total_elapsed_seconds": 135
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T19:59:41.376Z",
            "data": {
              "session_id": "session_1751572781110_kmbr7yt",
              "source_id": "arxiv",
              "paper_id": "2506.18777",
              "start_time": "2025-07-03T19:58:59.388Z",
              "end_time": "2025-07-03T19:59:41.110Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 2,
              "total_elapsed_seconds": 42
            }
          }
        ]
      },
      "meta": {
        "issue_number": 153,
        "object_id": "interactions:arxiv.2506.18777",
        "created_at": "2025-07-03T18:49:05+00:00",
        "updated_at": "2025-07-03T20:00:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.02273": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.02273",
        "url": "https://arxiv.org/pdf/2407.02273#page=28.10",
        "title": "Language Model Alignment in Multilingual Trolley Problems",
        "authors": "Zhijing Jin, Max Kleiman-Weiner, Giorgio Piatti, Sydney Levine, Jiarui Liu, Fernando Gonzalez, Francesco Ortu, Andr\u00e1s Strausz, Mrinmaya Sachan, Rada Mihalcea, Yejin Choi, Bernhard Sch\u00f6lkopf",
        "abstract": "We evaluate the moral alignment of LLMs with human preferences in\nmultilingual trolley problems. Building on the Moral Machine experiment, which\ncaptures over 40 million human judgments across 200+ countries, we develop a\ncross-lingual corpus of moral dilemma vignettes in over 100 languages called\nMultiTP. This dataset enables the assessment of LLMs' decision-making processes\nin diverse linguistic contexts. Our analysis explores the alignment of 19\ndifferent LLMs with human judgments, capturing preferences across six moral\ndimensions: species, gender, fitness, status, age, and the number of lives\ninvolved. By correlating these preferences with the demographic distribution of\nlanguage speakers and examining the consistency of LLM responses to various\nprompt paraphrasings, our findings provide insights into cross-lingual and\nethical biases of LLMs and their intersection. We discover significant variance\nin alignment across languages, challenging the assumption of uniform moral\nreasoning in AI systems and highlighting the importance of incorporating\ndiverse perspectives in AI ethics. The results underscore the need for further\nresearch on the integration of multilingual dimensions in responsible AI\nresearch to ensure fair and equitable AI interactions worldwide. Our code and\ndata are at https://github.com/causalNLP/moralmachine",
        "timestamp": "2025-07-03T19:17:43.652Z",
        "rating": "novote",
        "publishedDate": "2024-07-02T14:02:53Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 154,
        "object_id": "paper:arxiv.2407.02273",
        "created_at": "2025-07-03T19:17:43+00:00",
        "updated_at": "2025-07-03T19:18:03+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.04265": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.04265",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:01:40.549Z",
            "data": {
              "session_id": "session_1751576500303_ppz0pc6",
              "source_id": "arxiv",
              "paper_id": "2410.04265",
              "start_time": "2025-07-03T21:01:03.944Z",
              "end_time": "2025-07-03T21:01:40.303Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 1,
              "total_elapsed_seconds": 36
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T17:07:18.260Z",
            "data": {
              "session_id": "session_1760980037944_346g4h5",
              "source_id": "arxiv",
              "paper_id": "2410.04265",
              "start_time": "2025-10-20T17:07:06.020Z",
              "end_time": "2025-10-20T17:07:17.944Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "issue_number": 155,
        "object_id": "interactions:arxiv.2410.04265",
        "created_at": "2025-07-03T21:01:41+00:00",
        "updated_at": "2025-10-20T17:07:43+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.13775": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.13775",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:05:32.473Z",
            "data": {
              "session_id": "session_1751576732080_h2yt7go",
              "source_id": "arxiv",
              "paper_id": "2505.13775",
              "start_time": "2025-07-03T21:01:50.649Z",
              "end_time": "2025-07-03T21:05:32.080Z",
              "heartbeat_count": 44,
              "duration_seconds": 220,
              "idle_seconds": 1,
              "total_elapsed_seconds": 221
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:09:45.415Z",
            "data": {
              "session_id": "session_1751576985397_r0nkqjf",
              "source_id": "arxiv",
              "paper_id": "2505.13775",
              "start_time": "2025-07-03T21:09:40.012Z",
              "end_time": "2025-07-03T21:09:45.397Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 156,
        "object_id": "interactions:arxiv.2505.13775",
        "created_at": "2025-07-03T21:05:32+00:00",
        "updated_at": "2025-07-03T21:09:51+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.02867": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.02867",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:18:44.560Z",
            "data": {
              "session_id": "session_1751577524118_0klevah",
              "source_id": "arxiv",
              "paper_id": "2506.02867",
              "start_time": "2025-07-03T21:16:21.804Z",
              "end_time": "2025-07-03T21:18:44.118Z",
              "heartbeat_count": 28,
              "duration_seconds": 140,
              "idle_seconds": 2,
              "total_elapsed_seconds": 142
            }
          }
        ]
      },
      "meta": {
        "issue_number": 158,
        "object_id": "interactions:arxiv.2506.02867",
        "created_at": "2025-07-03T21:16:22+00:00",
        "updated_at": "2025-07-03T21:19:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2312.01552": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2312.01552",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:15:20.310Z",
            "data": {
              "session_id": "session_1751577319914_tn13kta",
              "source_id": "arxiv",
              "paper_id": "2312.01552",
              "start_time": "2025-07-03T21:10:14.183Z",
              "end_time": "2025-07-03T21:15:19.914Z",
              "heartbeat_count": 61,
              "duration_seconds": 305,
              "idle_seconds": 1,
              "total_elapsed_seconds": 306
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:16:00.442Z",
            "data": {
              "session_id": "session_1751577360431_th8v8ay",
              "source_id": "arxiv",
              "paper_id": "2312.01552",
              "start_time": "2025-07-03T21:15:54.049Z",
              "end_time": "2025-07-03T21:16:00.431Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 157,
        "object_id": "interactions:arxiv.2312.01552",
        "created_at": "2025-07-03T21:15:21+00:00",
        "updated_at": "2025-07-03T21:16:19+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.13765": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.13765",
        "url": "https://arxiv.org/pdf/2407.13765",
        "title": "Latent Causal Probing: A Formal Perspective on Probing with Causal\n  Models of Data",
        "authors": "Charles Jin, Martin Rinard",
        "abstract": "As language models (LMs) deliver increasing performance on a range of NLP\ntasks, probing classifiers have become an indispensable technique in the effort\nto better understand their inner workings. A typical setup involves (1)\ndefining an auxiliary task consisting of a dataset of text annotated with\nlabels, then (2) supervising small classifiers to predict the labels from the\nrepresentations of a pretrained LM as it processed the dataset. A high probing\naccuracy is interpreted as evidence that the LM has learned to perform the\nauxiliary task as an unsupervised byproduct of its original pretraining\nobjective. Despite the widespread usage of probes, however, the robust design\nand analysis of probing experiments remains a challenge. We develop a formal\nperspective on probing using structural causal models (SCM). Specifically,\ngiven an SCM which explains the distribution of tokens observed during\ntraining, we frame the central hypothesis as whether the LM has learned to\nrepresent the latent variables of the SCM. Empirically, we extend a recent\nstudy of LMs in the context of a synthetic grid-world navigation task, where\nhaving an exact model of the underlying causal structure allows us to draw\nstrong inferences from the result of probing experiments. Our techniques\nprovide robust empirical evidence for the ability of LMs to induce the latent\nconcepts underlying text.",
        "timestamp": "2025-07-06T18:48:36.277Z",
        "rating": "novote",
        "publishedDate": "2024-07-18T17:59:27Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 166,
        "object_id": "paper:arxiv.2407.13765",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:49:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.19200": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.19200",
        "url": "https://arxiv.org/pdf/2407.19200",
        "title": "On Behalf of the Stakeholders: Trends in NLP Model Interpretability in\n  the Era of LLMs",
        "authors": "Nitay Calderon, Roi Reichart",
        "abstract": "Recent advancements in NLP systems, particularly with the introduction of\nLLMs, have led to widespread adoption of these systems by a broad spectrum of\nusers across various domains, impacting decision-making, the job market,\nsociety, and scientific research. This surge in usage has led to an explosion\nin NLP model interpretability and analysis research, accompanied by numerous\ntechnical surveys. Yet, these surveys often overlook the needs and perspectives\nof explanation stakeholders. In this paper, we address three fundamental\nquestions: Why do we need interpretability, what are we interpreting, and how?\nBy exploring these questions, we examine existing interpretability paradigms,\ntheir properties, and their relevance to different stakeholders. We further\nexplore the practical implications of these paradigms by analyzing trends from\nthe past decade across multiple research fields. To this end, we retrieved\nthousands of papers and employed an LLM to characterize them. Our analysis\nreveals significant disparities between NLP developers and non-developer users,\nas well as between research fields, underscoring the diverse needs of\nstakeholders. For example, explanations of internal model components are rarely\nused outside the NLP field. We hope this paper informs the future design,\ndevelopment, and application of methods that align with the objectives and\nrequirements of various stakeholders.",
        "timestamp": "2025-07-06T18:48:36.240Z",
        "rating": "novote",
        "publishedDate": "2024-07-27T08:00:27Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 165,
        "object_id": "paper:arxiv.2407.19200",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:48:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.10827": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.10827",
        "url": "https://arxiv.org/pdf/2407.10827",
        "title": "LLM Circuit Analyses Are Consistent Across Training and Scale",
        "authors": "Curt Tigges, Michael Hanna, Qinan Yu, Stella Biderman",
        "abstract": "Most currently deployed large language models (LLMs) undergo continuous\ntraining or additional finetuning. By contrast, most research into LLMs'\ninternal mechanisms focuses on models at one snapshot in time (the end of\npre-training), raising the question of whether their results generalize to\nreal-world settings. Existing studies of mechanisms over time focus on\nencoder-only or toy models, which differ significantly from most deployed\nmodels. In this study, we track how model mechanisms, operationalized as\ncircuits, emerge and evolve across 300 billion tokens of training in\ndecoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.\nWe find that task abilities and the functional components that support them\nemerge consistently at similar token counts across scale. Moreover, although\nsuch components may be implemented by different attention heads over time, the\noverarching algorithm that they implement remains. Surprisingly, both these\nalgorithms and the types of components involved therein can replicate across\nmodel scale. These results suggest that circuit analyses conducted on small\nmodels at the end of pre-training can provide insights that still apply after\nadditional pre-training and over model scale.",
        "timestamp": "2025-07-06T18:48:36.240Z",
        "rating": "novote",
        "publishedDate": "2024-07-15T15:38:51Z",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 164,
        "object_id": "paper:arxiv.2407.10827",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:48:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.15510": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.15510",
        "url": "https://arxiv.org/pdf/2408.15510",
        "title": "How Reliable are Causal Probing Interventions?",
        "authors": "Marc Canby, Adam Davies, Chirag Rastogi, Julia Hockenmaier",
        "abstract": "Causal probing aims to analyze foundation models by examining how intervening\non their representation of various latent properties impacts their outputs.\nRecent works have cast doubt on the theoretical basis of several leading causal\nprobing methods, but it has been unclear how to systematically evaluate the\neffectiveness of these methods in practice. To address this, we define two key\ncausal probing desiderata: completeness (how thoroughly the representation of\nthe target property has been transformed) and selectivity (how little\nnon-targeted properties have been impacted). We find that there is an inherent\ntradeoff between the two, which we define as reliability, their harmonic mean.\nWe introduce an empirical analysis framework to measure and evaluate these\nquantities, allowing us to make the first direct comparisons between different\nfamilies of leading causal probing methods (e.g., linear vs. nonlinear, or\nconcept removal vs. counterfactual interventions). We find that: (1) no method\nis reliable across all layers; (2) more reliable methods have a greater impact\non LLM behavior; (3) nonlinear interventions are more reliable in early and\nintermediate layers, and linear interventions are more reliable in later\nlayers; and (4) concept removal methods are far less reliable than\ncounterfactual interventions, suggesting that they may not be an effective\napproach to causal probing.",
        "timestamp": "2025-07-06T18:48:36.239Z",
        "rating": "novote",
        "publishedDate": "2024-08-28T03:45:49Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 163,
        "object_id": "paper:arxiv.2408.15510",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:48:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.01687": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.01687",
        "url": "https://arxiv.org/pdf/2407.01687",
        "title": "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought:\n  Probability, Memorization, and Noisy Reasoning",
        "authors": "Akshara Prabhakar, Thomas L. Griffiths, R. Thomas McCoy",
        "abstract": "Chain-of-Thought (CoT) prompting has been shown to enhance the multi-step\nreasoning capabilities of Large Language Models (LLMs). However, debates\npersist about whether LLMs exhibit abstract generalization or rely on shallow\nheuristics when given CoT prompts. To understand the factors influencing CoT\nreasoning we provide a detailed case study of the symbolic reasoning task of\ndecoding shift ciphers, where letters are shifted forward some number of steps\nin the alphabet. We analyze the pattern of results produced by three LLMs --\nGPT-4, Claude 3, and Llama 3.1 -- performing this task using CoT prompting. By\nfocusing on a single relatively simple task, we are able to identify three\nfactors that systematically affect CoT performance: the probability of the\ntask's expected output (probability), what the model has implicitly learned\nduring pre-training (memorization), and the number of intermediate operations\ninvolved in reasoning (noisy reasoning). We show that these factors can\ndrastically influence task accuracy across all three LLMs; e.g., when tested\nwith GPT-4, varying the output's probability of occurrence shifts accuracy from\n26% to 70%. Overall, we conclude that CoT prompting performance reflects both\nmemorization and a probabilistic version of genuine reasoning. Code and data at\nthis https://github.com/aksh555/deciphering_cot",
        "timestamp": "2025-07-06T18:48:36.277Z",
        "rating": "novote",
        "publishedDate": "2024-07-01T18:01:07Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 162,
        "object_id": "paper:arxiv.2407.01687",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:48:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.04690": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.04690",
        "url": "https://arxiv.org/pdf/2407.04690",
        "title": "Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for\n  Interpreting Neural Networks",
        "authors": "Aaron Mueller",
        "abstract": "Interpretability research takes counterfactual theories of causality for\ngranted. Most causal methods rely on counterfactual interventions to inputs or\nthe activations of particular model components, followed by observations of the\nchange in models' output logits or behaviors. While this yields more faithful\nevidence than correlational methods, counterfactuals nonetheless have key\nproblems that bias our findings in specific and predictable ways. Specifically,\n(i) counterfactual theories do not effectively capture multiple independently\nsufficient causes of the same effect, which leads us to miss certain causes\nentirely; and (ii) counterfactual dependencies in neural networks are generally\nnot transitive, which complicates methods for extracting and interpreting\ncausal graphs from neural networks. We discuss the implications of these\nchallenges for interpretability researchers and propose concrete suggestions\nfor future work.",
        "timestamp": "2025-07-06T18:48:36.240Z",
        "rating": "novote",
        "publishedDate": "2024-07-05T17:53:03Z",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 161,
        "object_id": "paper:arxiv.2407.04690",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:48:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.16997": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.16997",
        "url": "https://arxiv.org/pdf/2407.16997",
        "title": "Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal\n  Intervention Perspective",
        "authors": "Yujian Liu, Yang Zhang, Tommi Jaakkola, Shiyu Chang",
        "abstract": "This paper investigates Who's Harry Potter (WHP), a pioneering yet\ninsufficiently understood method for LLM unlearning. We explore it in two\nsteps. First, we introduce a new task of LLM targeted unlearning, where given\nan unlearning target (e.g., a person) and some unlearning documents, we aim to\nunlearn only the information about the target, rather than everything in the\nunlearning documents. We further argue that a successful unlearning should\nsatisfy criteria such as not outputting gibberish, not fabricating facts about\nthe unlearning target, and not releasing factual information under jailbreak\nattacks. Second, we construct a causal intervention framework for targeted\nunlearning, where the knowledge of the unlearning target is modeled as a\nconfounder between LLM input and output, and the unlearning process as a\ndeconfounding process. This framework justifies and extends WHP, deriving a\nsimple unlearning algorithm that includes WHP as a special case. Experiments on\nexisting and new datasets show that our approach, without explicitly optimizing\nfor the aforementioned criteria, achieves competitive performance in all of\nthem. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/causal_unlearn.git.",
        "timestamp": "2025-07-06T18:48:36.239Z",
        "rating": "novote",
        "publishedDate": "2024-07-24T04:39:24Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 160,
        "object_id": "paper:arxiv.2407.16997",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:49:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.15017": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.15017",
        "url": "https://arxiv.org/pdf/2407.15017",
        "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
        "authors": "Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang",
        "abstract": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research.",
        "timestamp": "2025-07-06T18:48:36.240Z",
        "rating": "novote",
        "publishedDate": "2024-07-22T06:15:59Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CV",
          "cs.HC",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 159,
        "object_id": "paper:arxiv.2407.15017",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:48:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2110.11334": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2110.11334",
        "url": "https://arxiv.org/abs/2110.11334",
        "title": "Generalized Out-of-Distribution Detection: A Survey",
        "authors": "Jingkang Yang, Kaiyang Zhou, Yixuan Li, Ziwei Liu",
        "abstract": "Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this survey, we first present a unified framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. We then review each of these five areas by summarizing their recent technical developments, with a special focus on OOD detection methodologies. We conclude this survey with open challenges and potential research directions.",
        "timestamp": "2025-07-08T16:44:15.579Z",
        "rating": "novote",
        "publishedDate": "2021/10/21",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 167,
        "object_id": "paper:arxiv.2110.11334",
        "created_at": "2025-07-08T16:44:15+00:00",
        "updated_at": "2025-07-08T16:44:39+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2108.13624": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2108.13624",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T16:50:57.491Z",
            "data": {
              "session_id": "session_1751993457102_oagru5s",
              "source_id": "arxiv",
              "paper_id": "2108.13624",
              "start_time": "2025-07-08T16:50:48.947Z",
              "end_time": "2025-07-08T16:50:57.102Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T16:52:45.797Z",
            "data": {
              "session_id": "session_1751993565279_k4j1iss",
              "source_id": "arxiv",
              "paper_id": "2108.13624",
              "start_time": "2025-07-08T16:51:04.647Z",
              "end_time": "2025-07-08T16:52:45.279Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 1,
              "total_elapsed_seconds": 101
            }
          }
        ]
      },
      "meta": {
        "issue_number": 171,
        "object_id": "interactions:arxiv.2108.13624",
        "created_at": "2025-07-08T16:50:57+00:00",
        "updated_at": "2025-07-08T16:53:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2108.13624": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2108.13624",
        "url": "https://arxiv.org/pdf/2108.13624",
        "title": "Towards Out-Of-Distribution Generalization: A Survey",
        "authors": "Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, Peng Cui",
        "abstract": "Traditional machine learning paradigms are based on the assumption that both\ntraining and test data follow the same statistical pattern, which is\nmathematically referred to as Independent and Identically Distributed\n($i.i.d.$). However, in real-world applications, this $i.i.d.$ assumption often\nfails to hold due to unforeseen distributional shifts, leading to considerable\ndegradation in model performance upon deployment. This observed discrepancy\nindicates the significance of investigating the Out-of-Distribution (OOD)\ngeneralization problem. OOD generalization is an emerging topic of machine\nlearning research that focuses on complex scenarios wherein the distributions\nof the test data differ from those of the training data. This paper represents\nthe first comprehensive, systematic review of OOD generalization, encompassing\na spectrum of aspects from problem definition, methodological development, and\nevaluation procedures, to the implications and future directions of the field.\nOur discussion begins with a precise, formal characterization of the OOD\ngeneralization problem. Following that, we categorize existing methodologies\ninto three segments: unsupervised representation learning, supervised model\nlearning, and optimization, according to their positions within the overarching\nlearning process. We provide an in-depth discussion on representative\nmethodologies for each category, further elucidating the theoretical links\nbetween them. Subsequently, we outline the prevailing benchmark datasets\nemployed in OOD generalization studies. To conclude, we overview the existing\nbody of work in this domain and suggest potential avenues for future research\non OOD generalization. A summary of the OOD generalization methodologies\nsurveyed in this paper can be accessed at\nhttp://out-of-distribution-generalization.com.",
        "timestamp": "2025-07-08T16:50:49.165Z",
        "rating": "novote",
        "publishedDate": "2021-08-31T05:28:42Z",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 170,
        "object_id": "paper:arxiv.2108.13624",
        "created_at": "2025-07-08T16:50:49+00:00",
        "updated_at": "2025-07-08T16:51:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2308.00755": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2308.00755",
        "url": "https://arxiv.org/abs/2308.00755",
        "title": "The Bias Amplification Paradox in Text-to-Image Generation",
        "authors": "Preethi Seshadri, Sameer Singh, Yanai Elazar",
        "abstract": "Bias amplification is a phenomenon in which models exacerbate biases or stereotypes present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION) considerably. However, we discover that amplification can be largely attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while our prompts do not, which leads to a distribution shift and consequently inflates bias measures. Once we account for distributional differences between texts used for training and generation when evaluating amplification, we observe that amplification decreases drastically. Our findings illustrate the challenges of comparing biases in models and their training data, and highlight confounding factors that impact analyses.",
        "timestamp": "2025-07-08T18:16:54.469Z",
        "rating": "novote",
        "publishedDate": "2023/08/01",
        "tags": [
          "Machine Learning (cs.LG)",
          "Computation and Language (cs.CL)",
          "Computer Vision and Pattern Recognition (cs.CV)",
          "Computers and Society (cs.CY)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 175,
        "object_id": "paper:arxiv.2308.00755",
        "created_at": "2025-07-08T18:16:54+00:00",
        "updated_at": "2025-07-08T18:17:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.00211": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.00211",
        "url": "https://arxiv.org/abs/2407.00211",
        "title": "Detection and Measurement of Syntactic Templates in Generated Text",
        "authors": "Chantal Shaib, Yanai Elazar, Junyi Jessy Li, Byron C. Wallace",
        "abstract": "Recent work on evaluating the diversity of text generated by LLMs has focused on word-level features. Here we offer an analysis of syntactic features to characterize general repetition in models, beyond frequent n-grams. Specifically, we define syntactic templates and show that models tend to produce templated text in downstream tasks at a higher rate than what is found in human-reference texts. We find that most (76%) templates in model-generated text can be found in pre-training data (compared to only 35% of human-authored text), and are not overwritten during fine-tuning processes such as RLHF. This connection to the pre-training data allows us to analyze syntactic templates in models where we do not have the pre-training data. We also find that templates as features are able to differentiate between models, tasks, and domains, and are useful for qualitatively evaluating common model constructions. Finally, we demonstrate the use of templates as a useful tool for analyzing style memorization of training data in LLMs.",
        "timestamp": "2025-07-08T18:16:34.475Z",
        "rating": "novote",
        "publishedDate": "2024/06/28",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 174,
        "object_id": "paper:arxiv.2407.00211",
        "created_at": "2025-07-08T18:16:34+00:00",
        "updated_at": "2025-07-08T18:16:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.15002": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.15002",
        "url": "https://arxiv.org/abs/2410.15002",
        "title": "How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold",
        "authors": "Sahil Verma, Royi Rassin, Arnav Das, Gantavya Bhatt, Preethi Seshadri, Chirag Shah, Jeff Bilmes, Hannaneh Hajishirzi, Yanai Elazar",
        "abstract": "Text-to-image models are trained using large datasets collected by scraping image-text pairs from the internet. These datasets often include private, copyrighted, and licensed material. Training models on such datasets enables them to generate images with such content, which might violate copyright laws and individual privacy. This phenomenon is termed imitation -- generation of images with content that has recognizable similarity to its training images. In this work we study the relationship between a concept's frequency in the training dataset and the ability of a model to imitate it. We seek to determine the point at which a model was trained on enough instances to imitate a concept -- the imitation threshold. We posit this question as a new problem: Finding the Imitation Threshold (FIT) and propose an efficient approach that estimates the imitation threshold without incurring the colossal cost of training multiple models from scratch. We experiment with two domains -- human faces and art styles -- for which we create four datasets, and evaluate three text-to-image models which were trained on two pretraining datasets. Our results reveal that the imitation threshold of these models is in the range of 200-600 images, depending on the domain and the model. The imitation threshold can provide an empirical basis for copyright violation claims and acts as a guiding principle for text-to-image model developers that aim to comply with copyright and privacy laws. We release the code and data at \\url{this https URL} and the project's website is hosted at \\url{this https URL}.",
        "timestamp": "2025-07-08T18:16:21.735Z",
        "rating": "novote",
        "publishedDate": "2024/10/19",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 172,
        "object_id": "paper:arxiv.2410.15002",
        "created_at": "2025-07-08T18:16:22+00:00",
        "updated_at": "2025-07-08T18:16:40+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2407.14985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.14985",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:33:06.844Z",
            "data": {
              "session_id": "session_1751999586838_5a8grh7",
              "source_id": "arxiv",
              "paper_id": "2407.14985",
              "start_time": "2025-07-08T18:32:35.228Z",
              "end_time": "2025-07-08T18:33:06.838Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:39:48.946Z",
            "data": {
              "session_id": "session_1751999988491_r8wehx7",
              "source_id": "arxiv",
              "paper_id": "2407.14985",
              "start_time": "2025-07-08T18:33:16.036Z",
              "end_time": "2025-07-08T18:39:48.491Z",
              "heartbeat_count": 78,
              "duration_seconds": 390,
              "idle_seconds": 2,
              "total_elapsed_seconds": 392
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T13:57:30.304Z",
            "data": {
              "session_id": "session_1763733449615_2nwx9il",
              "source_id": "arxiv",
              "paper_id": "2407.14985",
              "start_time": "2025-11-21T13:56:05.700Z",
              "end_time": "2025-11-21T13:57:29.615Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 4,
              "total_elapsed_seconds": 84
            }
          }
        ]
      },
      "meta": {
        "issue_number": 180,
        "object_id": "interactions:arxiv.2407.14985",
        "created_at": "2025-07-08T18:21:30+00:00",
        "updated_at": "2025-11-21T13:57:50+00:00",
        "version": 1
      }
    },
    "paper:openreview.EDoD3DgivF": {
      "data": {
        "sourceId": "openreview",
        "paperId": "EDoD3DgivF",
        "url": "https://openreview.net/pdf?id=EDoD3DgivF",
        "title": "EDoD3DgivF",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-07-08T18:20:47.575Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 179,
        "object_id": "paper:openreview.EDoD3DgivF",
        "created_at": "2025-07-08T18:20:47+00:00",
        "updated_at": "2025-07-08T18:21:10+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2308.00755": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2308.00755",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:23:18.907Z",
            "data": {
              "session_id": "session_1751998998492_9aa31mx",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-07-08T18:22:32.588Z",
              "end_time": "2025-07-08T18:23:18.492Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 1,
              "total_elapsed_seconds": 46
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:30:10.972Z",
            "data": {
              "session_id": "session_1751999410527_upyri17",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-07-08T18:27:44.761Z",
              "end_time": "2025-07-08T18:30:10.527Z",
              "heartbeat_count": 29,
              "duration_seconds": 145,
              "idle_seconds": 1,
              "total_elapsed_seconds": 146
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T18:52:38.262Z",
            "data": {
              "session_id": "session_1752087157906_gbz74f7",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-07-09T18:51:27.241Z",
              "end_time": "2025-07-09T18:52:37.906Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 1,
              "total_elapsed_seconds": 71
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T18:54:08.834Z",
            "data": {
              "session_id": "session_1752087248416_vapqamc",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-07-09T18:52:40.502Z",
              "end_time": "2025-07-09T18:54:08.416Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 3,
              "total_elapsed_seconds": 88
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T19:00:08.760Z",
            "data": {
              "session_id": "session_1752087608342_qhu55iy",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-07-09T18:58:39.290Z",
              "end_time": "2025-07-09T19:00:08.342Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 4,
              "total_elapsed_seconds": 89
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T19:46:49.346Z",
            "data": {
              "session_id": "session_1752090409142_joelq51",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-07-09T19:46:34.115Z",
              "end_time": "2025-07-09T19:46:49.141Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T13:45:41.589Z",
            "data": {
              "session_id": "session_1763646341525_rzl4nrg",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-11-20T13:45:29.702Z",
              "end_time": "2025-11-20T13:45:41.525Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-22T11:01:38.084Z",
            "data": {
              "session_id": "session_1763809296742_exa3a3a",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-11-22T11:01:31.233Z",
              "end_time": "2025-11-22T11:01:36.742Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T08:20:04.195Z",
            "data": {
              "session_id": "session_1764145204191_7ayj31p",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-11-26T08:19:55.885Z",
              "end_time": "2025-11-26T08:20:04.191Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T10:35:15.832Z",
            "data": {
              "session_id": "session_1764153315825_46tp14c",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-11-26T10:35:01.259Z",
              "end_time": "2025-11-26T10:35:15.825Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T10:39:43.386Z",
            "data": {
              "session_id": "session_1764153583071_y50evwp",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-11-26T10:39:38.011Z",
              "end_time": "2025-11-26T10:39:43.071Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 178,
        "object_id": "interactions:arxiv.2308.00755",
        "created_at": "2025-07-08T18:20:25+00:00",
        "updated_at": "2025-11-26T10:40:07+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2407.00211": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.00211",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:20:19.077Z",
            "data": {
              "session_id": "session_1751998818716_lhn5cid",
              "source_id": "arxiv",
              "paper_id": "2407.00211",
              "start_time": "2025-07-08T18:19:03.947Z",
              "end_time": "2025-07-08T18:20:18.716Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 5,
              "total_elapsed_seconds": 75
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:32:33.807Z",
            "data": {
              "session_id": "session_1751999553784_lf3r1aa",
              "source_id": "arxiv",
              "paper_id": "2407.00211",
              "start_time": "2025-07-08T18:31:42.413Z",
              "end_time": "2025-07-08T18:32:33.784Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 1,
              "total_elapsed_seconds": 51
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T16:35:44.776Z",
            "data": {
              "session_id": "session_1752078944299_mxk0yrb",
              "source_id": "arxiv",
              "paper_id": "2407.00211",
              "start_time": "2025-07-09T16:34:19.367Z",
              "end_time": "2025-07-09T16:35:44.299Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 5,
              "total_elapsed_seconds": 85
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T18:58:38.579Z",
            "data": {
              "session_id": "session_1752087518558_01p6rzq",
              "source_id": "arxiv",
              "paper_id": "2407.00211",
              "start_time": "2025-07-09T18:58:06.184Z",
              "end_time": "2025-07-09T18:58:38.558Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T20:21:19.974Z",
            "data": {
              "session_id": "session_1762806079944_nb912d0",
              "source_id": "arxiv",
              "paper_id": "2407.00211",
              "start_time": "2025-11-10T20:20:24.335Z",
              "end_time": "2025-11-10T20:21:19.944Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 1,
              "total_elapsed_seconds": 56
            }
          }
        ]
      },
      "meta": {
        "issue_number": 177,
        "object_id": "interactions:arxiv.2407.00211",
        "created_at": "2025-07-08T18:18:39+00:00",
        "updated_at": "2025-11-10T20:21:44+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.15002": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.15002",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:20:38.011Z",
            "data": {
              "session_id": "session_1751998837994_2cek6pb",
              "source_id": "arxiv",
              "paper_id": "2410.15002",
              "start_time": "2025-07-08T18:20:24.531Z",
              "end_time": "2025-07-08T18:20:37.994Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:22:14.051Z",
            "data": {
              "session_id": "session_1751998934037_iqon5fm",
              "source_id": "arxiv",
              "paper_id": "2410.15002",
              "start_time": "2025-07-08T18:21:31.747Z",
              "end_time": "2025-07-08T18:22:14.037Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 2,
              "total_elapsed_seconds": 42
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T16:37:04.661Z",
            "data": {
              "session_id": "session_1752079024623_pv080w4",
              "source_id": "arxiv",
              "paper_id": "2410.15002",
              "start_time": "2025-07-09T16:36:50.912Z",
              "end_time": "2025-07-09T16:37:04.623Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T18:49:58.325Z",
            "data": {
              "session_id": "session_1752086998311_ptn0997",
              "source_id": "arxiv",
              "paper_id": "2410.15002",
              "start_time": "2025-07-09T18:49:37.037Z",
              "end_time": "2025-07-09T18:49:58.311Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T18:55:32.273Z",
            "data": {
              "session_id": "session_1752087332265_rvbsg4b",
              "source_id": "arxiv",
              "paper_id": "2410.15002",
              "start_time": "2025-07-09T18:55:26.278Z",
              "end_time": "2025-07-09T18:55:32.265Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T19:47:46.099Z",
            "data": {
              "session_id": "session_1752090466086_da0ab2e",
              "source_id": "arxiv",
              "paper_id": "2410.15002",
              "start_time": "2025-07-09T19:47:18.155Z",
              "end_time": "2025-07-09T19:47:46.086Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          }
        ]
      },
      "meta": {
        "issue_number": 176,
        "object_id": "interactions:arxiv.2410.15002",
        "created_at": "2025-07-08T18:18:11+00:00",
        "updated_at": "2025-07-09T19:48:07+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.21828": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.21828",
        "url": "https://arxiv.org/html/2505.21828v1",
        "title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety\n  Facts",
        "authors": "Chen Yueh-Han, Guy Davidson, Brenden M. Lake",
        "abstract": "Do LLMs robustly generalize critical safety facts to novel situations?\nLacking this ability is dangerous when users ask naive questions. For instance,\n\"I'm considering packing melon balls for my 10-month-old's lunch. What other\nfoods would be good to include?\" Before offering food options, the LLM should\nwarn that melon balls pose a choking hazard to toddlers, as documented by the\nCDC. Failing to provide such warnings could result in serious injuries or even\ndeath. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic\nGEneralization evaluation, the first benchmark that tests whether LLMs properly\napply well established safety facts to naive user queries. SAGE-Eval comprises\n104 facts manually sourced from reputable organizations, systematically\naugmented to create 10,428 test scenarios across 7 common domains (e.g.,\nOutdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,\npasses only 58% of all the safety facts tested. We also observe that model\ncapabilities and training compute weakly correlate with performance on\nSAGE-Eval, implying that scaling up is not the golden solution. Our findings\nsuggest frontier LLMs still lack robust generalization ability. We recommend\ndevelopers use SAGE-Eval in pre-deployment evaluations to assess model\nreliability in addressing salient risks. We publicly release SAGE-Eval at\nhttps://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available\nat https://github.com/YuehHanChen/SAGE-Eval/tree/main.",
        "timestamp": "2025-07-08T20:02:26.847Z",
        "rating": "novote",
        "publishedDate": "2025-05-27T23:29:32Z",
        "tags": [
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 181,
        "object_id": "paper:arxiv.2505.21828",
        "created_at": "2025-07-08T20:02:27+00:00",
        "updated_at": "2025-07-08T20:02:47+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.02126": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.02126",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T20:11:28.273Z",
            "data": {
              "session_id": "session_1752005488264_75ql3wv",
              "source_id": "arxiv",
              "paper_id": "2506.02126",
              "start_time": "2025-07-08T20:11:11.086Z",
              "end_time": "2025-07-08T20:11:28.264Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T20:21:55.169Z",
            "data": {
              "session_id": "session_1752006114630_4zlrxep",
              "source_id": "arxiv",
              "paper_id": "2506.02126",
              "start_time": "2025-07-08T20:12:21.323Z",
              "end_time": "2025-07-08T20:21:54.630Z",
              "heartbeat_count": 114,
              "duration_seconds": 570,
              "idle_seconds": 3,
              "total_elapsed_seconds": 573
            }
          }
        ]
      },
      "meta": {
        "issue_number": 183,
        "object_id": "interactions:arxiv.2506.02126",
        "created_at": "2025-07-08T20:11:02+00:00",
        "updated_at": "2025-07-08T20:22:17+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.21828": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.21828",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T17:53:03.650Z",
            "data": {
              "session_id": "session_1752083583062_en16ww9",
              "source_id": "arxiv",
              "paper_id": "2505.21828",
              "start_time": "2025-07-09T17:50:45.176Z",
              "end_time": "2025-07-09T17:53:03.062Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 3,
              "total_elapsed_seconds": 138
            }
          }
        ]
      },
      "meta": {
        "issue_number": 182,
        "object_id": "interactions:arxiv.2505.21828",
        "created_at": "2025-07-08T20:03:09+00:00",
        "updated_at": "2025-07-09T17:53:20+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.17241": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.17241",
        "interactions": []
      },
      "meta": {
        "issue_number": 185,
        "object_id": "interactions:arxiv.2406.17241",
        "created_at": "2025-07-09T00:40:21+00:00",
        "updated_at": "2025-07-09T00:40:22+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.17241": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.17241",
        "url": "https://arxiv.org/abs/2406.17241",
        "title": "Understanding Language Model Circuits through Knowledge Editing",
        "authors": "Huaizhi Ge, Frank Rudzicz, Zining Zhu",
        "abstract": "Recent advances in language model interpretability have identified circuits, critical subnetworks that replicate model behaviors, yet how knowledge is structured within these crucial subnetworks remains opaque. To gain an understanding toward the knowledge in the circuits, we conduct systematic knowledge editing experiments on the circuits of the GPT-2 language model. Our analysis reveals intriguing patterns in how circuits respond to editing attempts, the extent of knowledge distribution across network components, and the architectural composition of knowledge-bearing circuits. These findings offer insights into the complex relationship between model circuits and knowledge representation, deepening the understanding of how information is organized within language models. Our findings offer novel insights into the ``meanings'' of the circuits, and introduce directions for further interpretability and safety research of language models.",
        "timestamp": "2025-07-09T00:40:06.639Z",
        "rating": "novote",
        "publishedDate": "2024/06/25",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 184,
        "object_id": "paper:arxiv.2406.17241",
        "created_at": "2025-07-09T00:40:07+00:00",
        "updated_at": "2025-07-09T00:40:28+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.15075": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.15075",
        "interactions": []
      },
      "meta": {
        "issue_number": 199,
        "object_id": "interactions:arxiv.2505.15075",
        "created_at": "2025-07-09T00:42:00+00:00",
        "updated_at": "2025-07-09T00:42:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.03074": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.03074",
        "url": "https://arxiv.org/html/2506.03074v1",
        "title": "GL-LowPopArt: A Nearly Instance-Wise Minimax-Optimal Estimator for\n  Generalized Low-Rank Trace Regression",
        "authors": "Junghyun Lee, Kyoungseok Jang, Kwang-Sung Jun, Milan Vojnovi\u0107, Se-Young Yun",
        "abstract": "We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized\nlow-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it\nemploys a two-stage approach: nuclear norm regularization followed by matrix\nCatoni estimation. We establish state-of-the-art estimation error bounds,\nsurpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and\nreveal a novel experimental design objective, $\\mathrm{GL}(\\pi)$. The key\ntechnical challenge is controlling bias from the nonlinear inverse link\nfunction, which we address by our two-stage approach. We prove a *local*\nminimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise\noptimality up to the condition number of the ground-truth Hessian. Applications\ninclude generalized linear matrix completion, where `GL-LowPopArt` achieves a\nstate-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a\nnovel setting inspired by general preference learning (Zhang et al., 2024). Our\nanalysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,\npotentially interesting problem-dependent quantity, along with improved Borda\nregret bound than vectorization (Wu et al., 2024).",
        "timestamp": "2025-07-09T00:41:59.895Z",
        "rating": "novote",
        "publishedDate": "2025-06-03T16:52:24Z",
        "tags": [
          "stat.ML",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 198,
        "object_id": "paper:arxiv.2506.03074",
        "created_at": "2025-07-09T00:42:00+00:00",
        "updated_at": "2025-07-09T00:42:24+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.03708": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.03708",
        "url": "https://arxiv.org/html/2502.03708v1",
        "title": "Toward universal steering and monitoring of AI models",
        "authors": "Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adser\u00e0, Mikhail Belkin",
        "abstract": "Modern AI models contain much of human knowledge, yet understanding of their\ninternal representation of this knowledge remains elusive. Characterizing the\nstructure and properties of this representation will lead to improvements in\nmodel capabilities and development of effective safeguards. Building on recent\nadvances in feature learning, we develop an effective, scalable approach for\nextracting linear representations of general concepts in large-scale AI models\n(language models, vision-language models, and reasoning models). We show how\nthese representations enable model steering, through which we expose\nvulnerabilities, mitigate misaligned behaviors, and improve model capabilities.\nAdditionally, we demonstrate that concept representations are remarkably\ntransferable across human languages and combinable to enable multi-concept\nsteering. Through quantitative analysis across hundreds of concepts, we find\nthat newer, larger models are more steerable and steering can improve model\ncapabilities beyond standard prompting. We show how concept representations are\neffective for monitoring misaligned content (hallucinations, toxic content). We\ndemonstrate that predictive models built using concept representations are more\naccurate for monitoring misaligned content than using models that judge outputs\ndirectly. Together, our results illustrate the power of using internal\nrepresentations to map the knowledge in AI models, advance AI safety, and\nimprove model capabilities.",
        "timestamp": "2025-07-09T00:41:53.374Z",
        "rating": "novote",
        "publishedDate": "2025-02-06T01:41:48Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 197,
        "object_id": "paper:arxiv.2502.03708",
        "created_at": "2025-07-09T00:41:53+00:00",
        "updated_at": "2025-07-09T00:42:14+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.00418": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.00418",
        "interactions": []
      },
      "meta": {
        "issue_number": 196,
        "object_id": "interactions:arxiv.2506.00418",
        "created_at": "2025-07-09T00:41:52+00:00",
        "updated_at": "2025-07-09T00:41:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.08142": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.08142",
        "url": "https://arxiv.org/html/2403.08142v2",
        "title": "FieldNet: Efficient Real-Time Shadow Removal for Enhanced Vision in\n  Field Robotics",
        "authors": "Alzayat Saleh, Alex Olsen, Jake Wood, Bronson Philippa, Mostafa Rahimi Azghadi",
        "abstract": "Shadows significantly hinder computer vision tasks in outdoor environments,\nparticularly in field robotics, where varying lighting conditions complicate\nobject detection and localisation. We present FieldNet, a novel deep learning\nframework for real-time shadow removal, optimised for resource-constrained\nhardware. FieldNet introduces a probabilistic enhancement module and a novel\nloss function to address challenges of inconsistent shadow boundary supervision\nand artefact generation, achieving enhanced accuracy and simplicity without\nrequiring shadow masks during inference. Trained on a dataset of 10,000 natural\nimages augmented with synthetic shadows, FieldNet outperforms state-of-the-art\nmethods on benchmark datasets (ISTD, ISTD+, SRD), with up to $9$x speed\nimprovements (66 FPS on Nvidia 2080Ti) and superior shadow removal quality\n(PSNR: 38.67, SSIM: 0.991). Real-world case studies in precision agriculture\nrobotics demonstrate the practical impact of FieldNet in enhancing weed\ndetection accuracy. These advancements establish FieldNet as a robust,\nefficient solution for real-time vision tasks in field robotics and beyond.",
        "timestamp": "2025-07-09T00:41:49.971Z",
        "rating": "novote",
        "publishedDate": "2024-03-13T00:04:07Z",
        "tags": [
          "cs.CV"
        ],
        "doi": "10.1016/j.eswa.2025.127442",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 195,
        "object_id": "paper:arxiv.2403.08142",
        "created_at": "2025-07-09T00:41:50+00:00",
        "updated_at": "2025-07-09T00:42:30+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2212.08983": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2212.08983",
        "url": "https://arxiv.org/html/2212.08983v2",
        "title": "Adaptive deep learning framework for robust unsupervised underwater\n  image enhancement",
        "authors": "Alzayat Saleh, Marcus Sheaves, Dean Jerry, Mostafa Rahimi Azghadi",
        "abstract": "One of the main challenges in deep learning-based underwater image\nenhancement is the limited availability of high-quality training data.\nUnderwater images are difficult to capture and are often of poor quality due to\nthe distortion and loss of colour and contrast in water. This makes it\ndifficult to train supervised deep learning models on large and diverse\ndatasets, which can limit the model's performance. In this paper, we explore an\nalternative approach to supervised underwater image enhancement. Specifically,\nwe propose a novel unsupervised underwater image enhancement framework that\nemploys a conditional variational autoencoder (cVAE) to train a deep learning\nmodel with probabilistic adaptive instance normalization (PAdaIN) and\nstatistically guided multi-colour space stretch that produces realistic\nunderwater images. The resulting framework is composed of a U-Net as a feature\nextractor and a PAdaIN to encode the uncertainty, which we call UDnet. To\nimprove the visual quality of the images generated by UDnet, we use a\nstatistically guided multi-colour space stretch module that ensures visual\nconsistency with the input image and provides an alternative to training using\na ground truth image. The proposed model does not need manual human annotation\nand can learn with a limited amount of data and achieves state-of-the-art\nresults on underwater images. We evaluated our proposed framework on eight\npublicly-available datasets. The results show that our proposed framework\nyields competitive performance compared to other state-of-the-art approaches in\nquantitative as well as qualitative metrics. Code available at\nhttps://github.com/alzayats/UDnet .",
        "timestamp": "2025-07-09T00:41:47.883Z",
        "rating": "novote",
        "publishedDate": "2022-12-18T01:07:20Z",
        "tags": [
          "cs.CV"
        ],
        "doi": "10.1016/j.eswa.2024.126314",
        "journalName": "Expert Systems with Applications, 126314 (2025)",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 194,
        "object_id": "paper:arxiv.2212.08983",
        "created_at": "2025-07-09T00:41:48+00:00",
        "updated_at": "2025-07-09T00:42:07+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.13461": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.13461",
        "url": "https://arxiv.org/html/2501.13461v1",
        "title": "Knowledge-Informed Multi-Agent Trajectory Prediction at Signalized\n  Intersections for Infrastructure-to-Everything",
        "authors": "Huilin Yin, Yangwenhui Xu, Jiaxiang Li, Hao Zhang, Gerhard Rigoll",
        "abstract": "Multi-agent trajectory prediction at signalized intersections is crucial for\ndeveloping efficient intelligent transportation systems and safe autonomous\ndriving systems. Due to the complexity of intersection scenarios and the\nlimitations of single-vehicle perception, the performance of vehicle-centric\nprediction methods has reached a plateau. In this paper, we introduce an\nInfrastructure-to-Everything (I2X) collaborative prediction scheme. In this\nscheme, roadside units (RSUs) independently forecast the future trajectories of\nall vehicles and transmit these predictions unidirectionally to subscribing\nvehicles. Building on this scheme, we propose I2XTraj, a dedicated\ninfrastructure-based trajectory prediction model. I2XTraj leverages real-time\ntraffic signal states, prior maneuver strategy knowledge, and multi-agent\ninteractions to generate accurate, joint multi-modal trajectory prediction.\nFirst, a continuous signal-informed mechanism is proposed to adaptively process\nreal-time traffic signals to guide trajectory proposal generation under varied\nintersection configurations. Second, a driving strategy awareness mechanism\nestimates the joint distribution of maneuver strategies by integrating spatial\npriors of intersection areas with dynamic vehicle states, enabling coverage of\nthe full set of feasible maneuvers. Third, a spatial-temporal-mode attention\nnetwork models multi-agent interactions to refine and adjust joint trajectory\noutputs.Finally, I2XTraj is evaluated on two real-world datasets of signalized\nintersections, the V2X-Seq and the SinD drone dataset. In both\nsingle-infrastructure and online collaborative scenarios, our model outperforms\nstate-of-the-art methods by over 30\\% on V2X-Seq and 15\\% on SinD,\ndemonstrating strong generalizability and robustness.",
        "timestamp": "2025-07-09T00:41:46.396Z",
        "rating": "novote",
        "publishedDate": "2025-01-23T08:23:45Z",
        "tags": [
          "cs.RO",
          "cs.CV",
          "cs.MA"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 193,
        "object_id": "paper:arxiv.2501.13461",
        "created_at": "2025-07-09T00:41:46+00:00",
        "updated_at": "2025-07-09T00:42:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.01324": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.01324",
        "url": "https://arxiv.org/html/2506.01324v1",
        "title": "Near-Optimal Clustering in Mixture of Markov Chains",
        "authors": "Junghyun Lee, Yassir Jedra, Alexandre Prouti\u00e8re, Se-Young Yun",
        "abstract": "We study the problem of clustering $T$ trajectories of length $H$, each\ngenerated by one of $K$ unknown ergodic Markov chains over a finite state space\nof size $S$. The goal is to accurately group trajectories according to their\nunderlying generative model. We begin by deriving an instance-dependent,\nhigh-probability lower bound on the clustering error rate, governed by the\nweighted KL divergence between the transition kernels of the chains. We then\npresent a novel two-stage clustering algorithm. In Stage~I, we apply spectral\nclustering using a new injective Euclidean embedding for ergodic Markov chains\n-- a contribution of independent interest that enables sharp concentration\nresults. Stage~II refines the initial clusters via a single step of\nlikelihood-based reassignment. Our method achieves a near-optimal clustering\nerror with high probability, under the conditions $H =\n\\tilde{\\Omega}(\\gamma_{\\mathrm{ps}}^{-1} (S^2 \\vee \\pi_{\\min}^{-1}))$ and $TH =\n\\tilde{\\Omega}(\\gamma_{\\mathrm{ps}}^{-1} S^2 )$, where $\\pi_{\\min}$ is the\nminimum stationary probability of a state across the $K$ chains and\n$\\gamma_{\\mathrm{ps}}$ is the minimum pseudo-spectral gap. These requirements\nprovide significant improvements, if not at least comparable, to the\nstate-of-the-art guarantee (Kausik et al., 2023), and moreover, our algorithm\noffers a key practical advantage: unlike existing approach, it requires no\nprior knowledge of model-specific quantities (e.g., separation between kernels\nor visitation probabilities). We conclude by discussing the inherent gap\nbetween our upper and lower bounds, providing insights into the unique\nstructure of this clustering problem.",
        "timestamp": "2025-07-09T00:41:46.101Z",
        "rating": "novote",
        "publishedDate": "2025-06-02T05:10:40Z",
        "tags": [
          "stat.ML",
          "cs.IT",
          "cs.LG",
          "math.IT",
          "math.PR"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 192,
        "object_id": "paper:arxiv.2506.01324",
        "created_at": "2025-07-09T00:41:46+00:00",
        "updated_at": "2025-07-09T00:42:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.13940": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.13940",
        "url": "https://arxiv.org/html/2408.13940",
        "title": "Derailer-Rerailer: Adaptive Verification for Efficient and Reliable\n  Language Model Reasoning",
        "authors": "Guangya Wan, Yuqi Wu, Hao Wang, Shengming Zhao, Jie Chen, Sheng Li",
        "abstract": "Large Language Models (LLMs) have shown impressive reasoning capabilities,\nyet existing prompting methods face a critical trade-off: simple approaches\noften struggle with complex tasks and reasoning stability, while more\nsophisticated methods require multiple inferences and substantial computational\nresources, limiting their practical deployment. To address this challenge, we\npropose Derailer-Rerailer, a novel framework that adaptively balances reasoning\naccuracy and computational efficiency. At its core, our framework employs a\nlightweight Derailer mechanism to assess reasoning stability and selectively\ntriggers an advanced Rerailer verification process only when necessary, thereby\noptimizing computational resource usage. Extensive evaluation across both open\nand closed-source models on more than 20 categories of mathematical, symbolic,\nand commonsense reasoning tasks demonstrates our framework's effectiveness:\nDerailer-Rerailer achieves significant accuracy improvements (8-11\\% across\nvarious reasoning tasks) while maintaining 2-3 times better efficiency than\nexisting verification methods, with particularly strong performance in\nmathematical and symbolic reasoning, offering a practical solution for\nenhancing LLM reasoning reliability while significantly reducing computational\noverhead.",
        "timestamp": "2025-07-09T00:41:44.534Z",
        "rating": "novote",
        "publishedDate": "2024-08-25T21:20:17Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 191,
        "object_id": "paper:arxiv.2408.13940",
        "created_at": "2025-07-09T00:41:44+00:00",
        "updated_at": "2025-07-09T00:42:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.08667": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.08667",
        "url": "https://arxiv.org/html/2501.08667v1",
        "title": "TimeFlow: Longitudinal Brain Image Registration and Aging Progression\n  Analysis",
        "authors": "Bailiang Jian, Jiazhen Pan, Yitong Li, Fabian Bongratz, Ruochen Li, Daniel Rueckert, Benedikt Wiestler, Christian Wachinger",
        "abstract": "Predicting future brain states is crucial for understanding healthy aging and\nneurodegenerative diseases. Longitudinal brain MRI registration, a cornerstone\nfor such analyses, has long been limited by its inability to forecast future\ndevelopments, reliance on extensive dense longitudinal data, and the need to\nbalance registration accuracy with temporal smoothness. In this work, we\npresent \\emph{TimeFlow}, a novel framework for longitudinal brain MRI\nregistration that overcomes all these challenges. TimeFlow leverages a U-Net\narchitecture with temporal conditioning inspired by diffusion models, enabling\naccurate registration using only two images as input and facilitating\nprospective analyses through future image prediction. Unlike traditional\nmethods, TimeFlow eliminates the demand for explicit smoothness regularizers\nand dense sequential data while maintaining temporal consistency and\ncontinuity. Experimental results highlight its superior performance in both\nfuture timepoint prediction and registration accuracy compared to\nstate-of-the-art methods. Additionally, TimeFlow supports novel biological\nbrain aging analyses, effectively differentiating neurodegenerative conditions\nfrom healthy aging, all without requiring segmentation, thus avoiding\nnon-trivial annotation and inconsistent segmentation flaws. This framework\npaves the way for accurate, data-efficient, and annotation-free prospective\nanalyses of brain aging and chronic diseases.",
        "timestamp": "2025-07-09T00:41:41.436Z",
        "rating": "novote",
        "publishedDate": "2025-01-15T09:02:04Z",
        "tags": [
          "eess.IV",
          "cs.CV"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 190,
        "object_id": "paper:arxiv.2501.08667",
        "created_at": "2025-07-09T00:41:41+00:00",
        "updated_at": "2025-07-09T00:42:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.22998": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.22998",
        "url": "https://arxiv.org/html/2505.22998v1",
        "title": "LLM Agents for Bargaining with Utility-based Feedback",
        "authors": "Jihwan Oh, Murad Aghazada, Se-Young Yun, Taehyeon Kim",
        "abstract": "Bargaining, a critical aspect of real-world interactions, presents challenges\nfor large language models (LLMs) due to limitations in strategic depth and\nadaptation to complex human factors. Existing benchmarks often fail to capture\nthis real-world complexity. To address this and enhance LLM capabilities in\nrealistic bargaining, we introduce a comprehensive framework centered on\nutility-based feedback. Our contributions are threefold: (1) BargainArena, a\nnovel benchmark dataset with six intricate scenarios (e.g., deceptive\npractices, monopolies) to facilitate diverse strategy modeling; (2)\nhuman-aligned, economically-grounded evaluation metrics inspired by utility\ntheory, incorporating agent utility and negotiation power, which implicitly\nreflect and promote opponent-aware reasoning (OAR); and (3) a structured\nfeedback mechanism enabling LLMs to iteratively refine their bargaining\nstrategies. This mechanism can positively collaborate with in-context learning\n(ICL) prompts, including those explicitly designed to foster OAR. Experimental\nresults show that LLMs often exhibit negotiation strategies misaligned with\nhuman preferences, and that our structured feedback mechanism significantly\nimproves their performance, yielding deeper strategic and opponent-aware\nreasoning.",
        "timestamp": "2025-07-09T00:41:39.610Z",
        "rating": "novote",
        "publishedDate": "2025-05-29T02:07:27Z",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 189,
        "object_id": "paper:arxiv.2505.22998",
        "created_at": "2025-07-09T00:41:40+00:00",
        "updated_at": "2025-07-09T00:42:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.19918": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.19918",
        "url": "https://arxiv.org/html/2502.19918v2",
        "title": "Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning\n  in Large Language Models",
        "authors": "Yuan Sui, Yufei He, Tri Cao, Simeng Han, Yulin Chen, Bryan Hooi",
        "abstract": "Large Language Models (LLMs) increasingly rely on prolonged reasoning chains\nto solve complex tasks. However, this trial-and-error approach often leads to\nhigh computational overhead and error propagation, where early mistakes can\nderail subsequent steps. To address these issues, we introduce Meta-Reasoner, a\nframework that dynamically optimizes inference-time reasoning by enabling LLMs\nto \"think about how to think.\" Drawing inspiration from human meta-cognition\nand dual-process theory, Meta-Reasoner operates as a strategic advisor,\ndecoupling high-level guidance from step-by-step generation. It employs\ncontextual multi-armed bandits to iteratively evaluate reasoning progress and\nselect optimal strategies (e.g., backtrack, clarify ambiguity, restart from\nscratch, or propose alternative approaches), and reallocates computational\nresources toward the most promising paths. Our evaluations on mathematical\nreasoning and puzzles highlight the potential of dynamic reasoning chains to\novercome inherent challenges in the LLM reasoning process and also show promise\nin broader applications, offering a scalable and adaptable solution for\nreasoning-intensive tasks.",
        "timestamp": "2025-07-09T00:41:32.959Z",
        "rating": "novote",
        "publishedDate": "2025-02-27T09:40:13Z",
        "tags": [
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 188,
        "object_id": "paper:arxiv.2502.19918",
        "created_at": "2025-07-09T00:41:33+00:00",
        "updated_at": "2025-07-09T00:41:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.15075": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.15075",
        "url": "https://arxiv.org/html/2505.15075v1",
        "title": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in\n  Multimodal LLMs",
        "authors": "Hao Wang, Pinzhi Huang, Jihan Yang, Saining Xie, Daisuke Kawahara",
        "abstract": "The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models.",
        "timestamp": "2025-07-09T00:41:30.652Z",
        "rating": "novote",
        "publishedDate": "2025-05-21T03:43:37Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CV",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 187,
        "object_id": "paper:arxiv.2505.15075",
        "created_at": "2025-07-09T00:41:31+00:00",
        "updated_at": "2025-07-09T00:41:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.00418": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.00418",
        "url": "https://arxiv.org/html/2506.00418v1",
        "title": "Dual Debiasing for Noisy In-Context Learning for Text Generation",
        "authors": "Siqi Liang, Sumyeong Ahn, Paramveer S. Dhillon, Jiayu Zhou",
        "abstract": "In context learning (ICL) relies heavily on high quality demonstrations drawn\nfrom large annotated corpora. Existing approaches detect noisy annotations by\nranking local perplexities, presuming that noisy samples yield higher\nperplexities than their clean counterparts. However, this assumption breaks\ndown when the noise ratio is high and many demonstrations are flawed. We\nreexamine the perplexity based paradigm for text generation under noisy\nannotations, highlighting two sources of bias in perplexity: the annotation\nitself and the domain specific knowledge inherent in large language models\n(LLMs). To overcome these biases, we introduce a dual debiasing framework that\nuses synthesized neighbors to explicitly correct perplexity estimates, yielding\na robust Sample Cleanliness Score. This metric uncovers absolute sample\ncleanliness regardless of the overall corpus noise level. Extensive experiments\ndemonstrate our method's superior noise detection capabilities and show that\nits final ICL performance is comparable to that of a fully clean demonstration\ncorpus. Moreover, our approach remains robust even when noise ratios are\nextremely high.",
        "timestamp": "2025-07-09T00:41:30.539Z",
        "rating": "novote",
        "publishedDate": "2025-05-31T06:44:48Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "I.2.7"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 186,
        "object_id": "paper:arxiv.2506.00418",
        "created_at": "2025-07-09T00:41:31+00:00",
        "updated_at": "2025-07-09T00:41:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.16048": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.16048",
        "url": "https://arxiv.org/abs/2402.16048",
        "title": "How Likely Do LLMs with CoT Mimic Human Reasoning?",
        "authors": "Guangsheng Bao, Hongbo Zhang, Cunxiang Wang, Linyi Yang, Yue Zhang",
        "abstract": "Chain-of-thought emerges as a promising technique for eliciting reasoning capabilities from Large Language Models (LLMs). However, it does not always improve task performance or accurately represent reasoning processes, leaving unresolved questions about its usage. In this paper, we diagnose the underlying mechanism by comparing the reasoning process of LLMs with humans, using causal analysis to understand the relationships between the problem instruction, reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often deviate from the ideal causal chain, resulting in spurious correlations and potential consistency errors (inconsistent reasoning and answers). We also examine various factors influencing the causal structure, finding that in-context learning with examples strengthens it, while post-training techniques like supervised fine-tuning and reinforcement learning on human feedback weaken it. To our surprise, the causal structure cannot be strengthened by enlarging the model size only, urging research on new techniques. We hope that this preliminary study will shed light on understanding and improving the reasoning process in LLM.",
        "timestamp": "2025-07-09T15:56:09.189Z",
        "rating": "novote",
        "publishedDate": "2024/02/25",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 200,
        "object_id": "paper:arxiv.2402.16048",
        "created_at": "2025-07-09T15:56:09+00:00",
        "updated_at": "2025-07-09T15:56:32+00:00",
        "version": 1
      }
    },
    "interactions:openreview.EDoD3DgivF": {
      "data": {
        "sourceId": "openreview",
        "paperId": "EDoD3DgivF",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-22T11:03:21.830Z",
            "data": {
              "session_id": "session_1763809401679_89vb1o2",
              "source_id": "openreview",
              "paper_id": "EDoD3DgivF",
              "start_time": "2025-11-22T11:03:09.432Z",
              "end_time": "2025-11-22T11:03:21.679Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-22T12:04:22.388Z",
            "data": {
              "session_id": "session_1763813062161_5p86zpa",
              "source_id": "openreview",
              "paper_id": "EDoD3DgivF",
              "start_time": "2025-11-22T12:04:12.339Z",
              "end_time": "2025-11-22T12:04:22.161Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-22T12:06:56.289Z",
            "data": {
              "session_id": "session_1763813216219_997u0wb",
              "source_id": "openreview",
              "paper_id": "EDoD3DgivF",
              "start_time": "2025-11-22T12:06:49.250Z",
              "end_time": "2025-11-22T12:06:56.219Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T09:05:44.932Z",
            "data": {
              "session_id": "session_1763888744924_aqajomo",
              "source_id": "openreview",
              "paper_id": "EDoD3DgivF",
              "start_time": "2025-11-23T09:05:37.382Z",
              "end_time": "2025-11-23T09:05:44.924Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 276,
        "object_id": "interactions:openreview.EDoD3DgivF",
        "created_at": "2025-10-15T14:00:09+00:00",
        "updated_at": "2025-11-23T09:06:05+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2402.16048": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.16048",
        "interactions": []
      },
      "meta": {
        "issue_number": 201,
        "object_id": "interactions:arxiv.2402.16048",
        "created_at": "2025-07-09T15:57:44+00:00",
        "updated_at": "2025-07-09T15:57:46+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.22724": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.22724",
        "url": "https://arxiv.org/abs/2506.22724",
        "title": "The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure",
        "authors": "Niyati Bafna, Tianjian Li, Kenton Murray, David R. Mortensen, David Yarowsky, Hale Sirin, Daniel Khashabi",
        "abstract": "Multilingual generation with large language models (LLMs) is often of poor quality for mid- to low-resource languages. Building on insights from interpretability, we demonstrate the existence of an implicit task-solving-->translation pipeline for generation, whereby the model first solves the required task in a largely target-language-agnostic manner, and subsequently translates answer concepts into the intended target language. We hypothesize that the failure of the translation stage is an important culprit for the observed low quality of final outputs, and formalize this as the translation barrier hypothesis. We test this hypothesis for a word translation task across 108 language pairs, using logit lens to observe model processing in intermediate layers. We find that a significant portion of overall failures indeed stems from translation failure, or the model's inability to translate correctly solved intermediate concepts into the target language. This is especially true for low-resource target languages. Our results highlight an important hurdle for end-to-end multilingual generation, and lend guiding insights for future work seeking to improve multilinguality in LLMs.",
        "timestamp": "2025-07-09T16:43:08.384Z",
        "rating": "thumbsup",
        "publishedDate": "2025/06/28",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 203,
        "object_id": "paper:arxiv.2506.22724",
        "created_at": "2025-07-09T16:43:08+00:00",
        "updated_at": "2025-07-09T16:56:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2010.08275": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2010.08275",
        "url": "https://arxiv.org/pdf/2010.08275",
        "title": "It's not Greek to mBERT: Inducing Word-Level Translations from\n  Multilingual BERT",
        "authors": "Hila Gonen, Shauli Ravfogel, Yanai Elazar, Yoav Goldberg",
        "abstract": "Recent works have demonstrated that multilingual BERT (mBERT) learns rich\ncross-lingual representations, that allow for transfer across languages. We\nstudy the word-level translation information embedded in mBERT and present two\nsimple methods that expose remarkable translation capabilities with no\nfine-tuning. The results suggest that most of this information is encoded in a\nnon-linear way, while some of it can also be recovered with purely linear\ntools. As part of our analysis, we test the hypothesis that mBERT learns\nrepresentations which contain both a language-encoding component and an\nabstract, cross-lingual component, and explicitly identify an empirical\nlanguage-identity subspace within mBERT representations.",
        "timestamp": "2025-07-09T16:52:30.381Z",
        "rating": "novote",
        "publishedDate": "2020-10-16T09:49:32Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 207,
        "object_id": "paper:arxiv.2010.08275",
        "created_at": "2025-07-09T16:52:30+00:00",
        "updated_at": "2025-07-09T16:52:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2006.00995": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2006.00995",
        "url": "https://arxiv.org/abs/2006.00995",
        "title": "Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals",
        "authors": "Yanai Elazar, Shauli Ravfogel, Alon Jacovi, Yoav Goldberg",
        "abstract": "A growing body of work makes use of probing to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results and offer an alternative method that focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention that removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, e.g. is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.",
        "timestamp": "2025-07-09T16:51:35.747Z",
        "rating": "novote",
        "publishedDate": "2020/06/01",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 205,
        "object_id": "paper:arxiv.2006.00995",
        "created_at": "2025-07-09T16:51:36+00:00",
        "updated_at": "2025-07-09T16:51:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.22724": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.22724",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T16:55:56.199Z",
            "data": {
              "session_id": "session_1752080156179_sk00odj",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-07-09T16:55:45.431Z",
              "end_time": "2025-07-09T16:55:56.179Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T16:58:48.253Z",
            "data": {
              "session_id": "session_1752080328243_zi75afk",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-07-09T16:58:42.409Z",
              "end_time": "2025-07-09T16:58:48.243Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T17:34:06.205Z",
            "data": {
              "session_id": "session_1752082441385_70ve51n",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-07-09T17:23:21.207Z",
              "end_time": "2025-07-09T17:34:01.385Z",
              "heartbeat_count": 128,
              "duration_seconds": 640,
              "idle_seconds": 0,
              "total_elapsed_seconds": 640
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T17:35:36.425Z",
            "data": {
              "session_id": "session_1752082536196_meqh6nq",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-07-09T17:35:11.984Z",
              "end_time": "2025-07-09T17:35:36.196Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T18:22:00.786Z",
            "data": {
              "session_id": "session_1752085320380_br9t7qo",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-07-09T18:18:59.491Z",
              "end_time": "2025-07-09T18:22:00.380Z",
              "heartbeat_count": 36,
              "duration_seconds": 180,
              "idle_seconds": 1,
              "total_elapsed_seconds": 181
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T12:25:14.975Z",
            "data": {
              "session_id": "session_1762086314283_ar0s10w",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-11-02T12:14:23.273Z",
              "end_time": "2025-11-02T12:25:14.283Z",
              "heartbeat_count": 130,
              "duration_seconds": 650,
              "idle_seconds": 1,
              "total_elapsed_seconds": 651
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T12:29:31.240Z",
            "data": {
              "session_id": "session_1762086571235_kzuiqzu",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-11-02T12:28:57.947Z",
              "end_time": "2025-11-02T12:29:31.235Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 3,
              "total_elapsed_seconds": 33
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T13:07:44.099Z",
            "data": {
              "session_id": "session_1762088864093_kbf4e3r",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-11-02T13:07:34.538Z",
              "end_time": "2025-11-02T13:07:44.093Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T14:15:44.782Z",
            "data": {
              "session_id": "session_1762092944489_ny49xmk",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-11-02T14:15:33.603Z",
              "end_time": "2025-11-02T14:15:44.489Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T18:31:19.094Z",
            "data": {
              "session_id": "session_1762108279088_32hwc5e",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-11-02T18:31:13.935Z",
              "end_time": "2025-11-02T18:31:19.088Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T18:57:12.871Z",
            "data": {
              "session_id": "session_1762109832281_y43de9v",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-11-02T18:47:26.159Z",
              "end_time": "2025-11-02T18:57:12.280Z",
              "heartbeat_count": 117,
              "duration_seconds": 585,
              "idle_seconds": 1,
              "total_elapsed_seconds": 586
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T21:28:24.150Z",
            "data": {
              "session_id": "session_1762118903589_erldl94",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-11-02T21:20:40.648Z",
              "end_time": "2025-11-02T21:28:23.589Z",
              "heartbeat_count": 92,
              "duration_seconds": 460,
              "idle_seconds": 3,
              "total_elapsed_seconds": 463
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T05:12:53.102Z",
            "data": {
              "session_id": "session_1762146773076_r953016",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-11-03T05:11:55.697Z",
              "end_time": "2025-11-03T05:12:53.076Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 2,
              "total_elapsed_seconds": 57
            }
          }
        ]
      },
      "meta": {
        "issue_number": 204,
        "object_id": "interactions:arxiv.2506.22724",
        "created_at": "2025-07-09T16:45:45+00:00",
        "updated_at": "2025-11-03T05:13:16+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2010.08275": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2010.08275",
        "interactions": []
      },
      "meta": {
        "issue_number": 208,
        "object_id": "interactions:arxiv.2010.08275",
        "created_at": "2025-07-09T16:53:09+00:00",
        "updated_at": "2025-07-09T16:53:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2101.11109": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2101.11109",
        "url": "https://arxiv.org/abs/2101.11109",
        "title": "First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT",
        "authors": "Benjamin Muller, Yanai Elazar, Beno\u00eet Sagot, Djam\u00e9 Seddah",
        "abstract": "Multilingual pretrained language models have demonstrated remarkable zero-shot cross-lingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model's internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a task-specific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during fine-tuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis.",
        "timestamp": "2025-07-09T16:54:57.742Z",
        "rating": "novote",
        "publishedDate": "2021/01/26",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 209,
        "object_id": "paper:arxiv.2101.11109",
        "created_at": "2025-07-09T16:54:58+00:00",
        "updated_at": "2025-07-09T16:55:17+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.00163": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.00163",
        "url": "https://arxiv.org/abs/2507.00163",
        "title": "Prompting as Scientific Inquiry",
        "authors": "Ari Holtzman, Chenhao Tan",
        "abstract": "Prompting is the primary method by which we study and control large language models. It is also one of the most powerful: nearly every major capability attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was first unlocked through prompting. Yet prompting is rarely treated as science and is frequently frowned upon as alchemy. We argue that this is a category error. If we treat LLMs as a new kind of complex and opaque organism that is trained rather than programmed, then prompting is not a workaround: it is behavioral science. Mechanistic interpretability peers into the neural substrate, prompting probes the model in its native interface: language. We contend that prompting is not inferior, but rather a key component in the science of LLMs.",
        "timestamp": "2025-07-09T20:01:47.850Z",
        "rating": "novote",
        "publishedDate": "2025/06/30",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 210,
        "object_id": "paper:arxiv.2507.00163",
        "created_at": "2025-07-09T20:01:48+00:00",
        "updated_at": "2025-07-09T20:02:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.23829": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.23829",
        "url": "https://arxiv.org/pdf/2505.23829",
        "title": "BiasFilter: An Inference-Time Debiasing Framework for Large Language\n  Models",
        "authors": "Xiaoqing Cheng, Ruizhe Chen, Hongying Zan, Yuxiang Jia, Min Peng",
        "abstract": "Mitigating social bias in large language models (LLMs) has become an\nincreasingly important research objective. However, existing debiasing methods\noften incur high human and computational costs, exhibit limited effectiveness,\nand struggle to scale to larger models and open-ended generation tasks. To\naddress these limitations, this paper proposes BiasFilter, a model-agnostic,\ninference-time debiasing framework that integrates seamlessly with both\nopen-source and API-based LLMs. Instead of relying on retraining with balanced\ndata or modifying model parameters, BiasFilter enforces fairness by filtering\ngeneration outputs in real time. Specifically, it periodically evaluates\nintermediate outputs every few tokens, maintains an active set of candidate\ncontinuations, and incrementally completes generation by discarding low-reward\nsegments based on a fairness reward signal. To support this process, we\nconstruct a fairness preference dataset and train an implicit reward model to\nassess token-level fairness in generated responses. Extensive experiments\ndemonstrate that BiasFilter effectively mitigates social bias across a range of\nLLMs while preserving overall generation quality.",
        "timestamp": "2025-07-09T21:42:04.115Z",
        "rating": "novote",
        "publishedDate": "2025-05-28T08:09:10Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 212,
        "object_id": "paper:arxiv.2505.23829",
        "created_at": "2025-07-09T21:42:04+00:00",
        "updated_at": "2025-07-09T21:42:23+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.23829": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.23829",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T22:06:58.469Z",
            "data": {
              "session_id": "session_1752098817990_pd171u1",
              "source_id": "arxiv",
              "paper_id": "2505.23829",
              "start_time": "2025-07-09T22:00:42.050Z",
              "end_time": "2025-07-09T22:06:57.990Z",
              "heartbeat_count": 75,
              "duration_seconds": 375,
              "idle_seconds": 1,
              "total_elapsed_seconds": 376
            }
          }
        ]
      },
      "meta": {
        "issue_number": 213,
        "object_id": "interactions:arxiv.2505.23829",
        "created_at": "2025-07-09T21:43:44+00:00",
        "updated_at": "2025-07-09T22:07:18+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.00163": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.00163",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T23:34:35.182Z",
            "data": {
              "session_id": "session_1752104075148_yyi9n46",
              "source_id": "arxiv",
              "paper_id": "2507.00163",
              "start_time": "2025-07-09T23:34:23.554Z",
              "end_time": "2025-07-09T23:34:35.148Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T23:42:55.049Z",
            "data": {
              "session_id": "session_1752104575039_g3nso3c",
              "source_id": "arxiv",
              "paper_id": "2507.00163",
              "start_time": "2025-07-09T23:42:44.087Z",
              "end_time": "2025-07-09T23:42:55.039Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T17:35:42.919Z",
            "data": {
              "session_id": "session_1760981742596_zw7s7s8",
              "source_id": "arxiv",
              "paper_id": "2507.00163",
              "start_time": "2025-10-20T17:35:22.832Z",
              "end_time": "2025-10-20T17:35:42.596Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T09:34:29.808Z",
            "data": {
              "session_id": "session_1761989669511_trsedf7",
              "source_id": "arxiv",
              "paper_id": "2507.00163",
              "start_time": "2025-11-01T09:33:43.052Z",
              "end_time": "2025-11-01T09:34:29.511Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 1,
              "total_elapsed_seconds": 46
            }
          }
        ]
      },
      "meta": {
        "issue_number": 214,
        "object_id": "interactions:arxiv.2507.00163",
        "created_at": "2025-07-09T23:33:08+00:00",
        "updated_at": "2025-11-01T09:34:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.17514": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17514",
        "url": "https://arxiv.org/pdf/2503.17514",
        "title": "Language Models May Verbatim Complete Text They Were Not Explicitly\n  Trained On",
        "authors": "Ken Ziyu Liu, Christopher A. Choquette-Choo, Matthew Jagielski, Peter Kairouz, Sanmi Koyejo, Percy Liang, Nicolas Papernot",
        "abstract": "An important question today is whether a given text was used to train a large\nlanguage model (LLM). A \\emph{completion} test is often employed: check if the\nLLM completes a sufficiently complex text. This, however, requires a\nground-truth definition of membership; most commonly, it is defined as a member\nbased on the $n$-gram overlap between the target text and any text in the\ndataset. In this work, we demonstrate that this $n$-gram based membership\ndefinition can be effectively gamed. We study scenarios where sequences are\n\\emph{non-members} for a given $n$ and we find that completion tests still\nsucceed. We find many natural cases of this phenomenon by retraining LLMs from\nscratch after removing all training samples that were completed; these cases\ninclude exact duplicates, near-duplicates, and even short overlaps. They\nshowcase that it is difficult to find a single viable choice of $n$ for\nmembership definitions. Using these insights, we design adversarial datasets\nthat can cause a given target sequence to be completed without containing it,\nfor any reasonable choice of $n$. Our findings highlight the inadequacy of\n$n$-gram membership, suggesting membership definitions fail to account for\nauxiliary information available to the training algorithm.",
        "timestamp": "2025-07-09T23:46:17.577Z",
        "rating": "novote",
        "publishedDate": "2025-03-21T19:57:04Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CR",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 215,
        "object_id": "paper:arxiv.2503.17514",
        "created_at": "2025-07-09T23:46:17+00:00",
        "updated_at": "2025-07-09T23:46:37+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.12463": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.12463",
        "url": "https://arxiv.org/pdf/2510.12463",
        "title": "Resource-sensitive but language-blind: Community size and not\n  grammatical complexity better predicts the accuracy of Large Language Models\n  in a novel Wug Test",
        "authors": "Nikoleta Pantelidou, Evelina Leivada, Paolo Morosi",
        "abstract": "The linguistic abilities of Large Language Models are a matter of ongoing\ndebate. This study contributes to this discussion by investigating model\nperformance in a morphological generalization task that involves novel words.\nUsing a multilingual adaptation of the Wug Test, six models were tested across\nfour partially unrelated languages (Catalan, English, Greek, and Spanish) and\ncompared with human speakers. The aim is to determine whether model accuracy\napproximates human competence and whether it is shaped primarily by linguistic\ncomplexity or by the quantity of available training data. Consistent with\nprevious research, the results show that the models are able to generalize\nmorphological processes to unseen words with human-like accuracy. However,\naccuracy patterns align more closely with community size and data availability\nthan with structural complexity, refining earlier claims in the literature. In\nparticular, languages with larger speaker communities and stronger digital\nrepresentation, such as Spanish and English, revealed higher accuracy than\nless-resourced ones like Catalan and Greek. Overall, our findings suggest that\nmodel behavior is mainly driven by the richness of linguistic resources rather\nthan by sensitivity to grammatical complexity, reflecting a form of performance\nthat resembles human linguistic competence only superficially.",
        "timestamp": "2025-10-16T06:12:59.462Z",
        "rating": "novote",
        "publishedDate": "2025-10-14T12:52:57Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 313,
        "object_id": "paper:arxiv.2510.12463",
        "created_at": "2025-10-16T06:12:59+00:00",
        "updated_at": "2025-10-16T06:13:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2311.12233": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.12233",
        "url": "https://arxiv.org/abs/2311.12233",
        "title": "Unifying Corroborative and Contributive Attributions in Large Language Models",
        "authors": "Theodora Worledge, Judy Hanwen Shen, Nicole Meister, Caleb Winston, Carlos Guestrin",
        "abstract": "As businesses, products, and services spring up around large language models, the trustworthiness of these models hinges on the verifiability of their outputs. However, methods for explaining language model outputs largely fall across two distinct fields of study which both use the term \"attribution\" to refer to entirely separate techniques: citation generation and training data attribution. In many modern applications, such as legal document generation and medical question answering, both types of attributions are important. In this work, we argue for and present a unified framework of large language model attributions. We show how existing methods of different types of attribution fall under the unified framework. We also use the framework to discuss real-world use cases where one or both types of attributions are required. We believe that this unified framework will guide the use case driven development of systems that leverage both types of attribution, as well as the standardization of their evaluation.",
        "timestamp": "2025-10-15T14:08:11.111Z",
        "rating": "novote",
        "publishedDate": "2023/11/20",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 280,
        "object_id": "paper:arxiv.2311.12233",
        "created_at": "2025-10-15T14:08:11+00:00",
        "updated_at": "2025-10-15T18:45:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.17585": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.17585",
        "url": "https://arxiv.org/pdf/2506.17585",
        "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language\n  Models",
        "authors": "Yukun Huang, Sanxing Chen, Jian Pei, Manzil Zaheer, Bhuwan Dhingra",
        "abstract": "Trustworthy language models should provide both correct and verifiable\nanswers. While language models can sometimes attribute their outputs to\npretraining data, their citations are often unreliable due to hallucination. As\na result, current systems insert citations by querying an external retriever at\ninference time, introducing latency, infrastructure dependence, and\nvulnerability to retrieval noise. We explore whether LLMs can be made to\nreliably attribute to the documents seen during (continual)\npretraining--without test-time retrieval--by revising the training process. To\nevaluate this, we release CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and\nprobes both short-form (single fact) and long-form (multi-fact) citation tasks.\nOur approach follows a two-stage process: (1) continual pretraining to bind\nfacts to persistent document identifiers, and (2) instruction tuning to elicit\ncitation behavior. We find that simple Passive Indexing, which appends an\nidentifier to each document, helps memorize verbatim text but fails on\nparaphrased or compositional facts. Instead, we propose Active Indexing, which\ncontinually pretrains on synthetic QA pairs that (1) restate each fact in\ndiverse compositional forms, and (2) require bidirectional source-to-fact and\nfact-to-source generation, jointly teaching the model to generate content from\na cited source and to attribute its own answers. Experiments with Qwen2.5-7B\nand 3B show that Active Indexing consistently outperforms Passive Indexing\nacross all tasks and models, with citation precision gains up to 30.2 percent.\nOur ablation studies reveal that performance continues to improve as we scale\nthe amount of augmented data, showing a clear upward trend even at 16 times the\noriginal token count.",
        "timestamp": "2025-10-15T13:11:12.642Z",
        "rating": "novote",
        "publishedDate": "2025-06-21T04:48:05Z",
        "tags": [
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 265,
        "object_id": "paper:arxiv.2506.17585",
        "created_at": "2025-10-15T13:11:12+00:00",
        "updated_at": "2025-10-15T18:46:25+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2311.12233": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.12233",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T16:32:10.952Z",
            "data": {
              "session_id": "session_1760545930339_eppub99",
              "source_id": "arxiv",
              "paper_id": "2311.12233",
              "start_time": "2025-10-15T16:26:34.096Z",
              "end_time": "2025-10-15T16:32:10.339Z",
              "heartbeat_count": 67,
              "duration_seconds": 335,
              "idle_seconds": 1,
              "total_elapsed_seconds": 336
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T12:04:27.059Z",
            "data": {
              "session_id": "session_1760616267019_g1gk9w4",
              "source_id": "arxiv",
              "paper_id": "2311.12233",
              "start_time": "2025-10-16T12:04:11.470Z",
              "end_time": "2025-10-16T12:04:27.019Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T12:32:02.978Z",
            "data": {
              "session_id": "session_1760617922364_tmuxtxk",
              "source_id": "arxiv",
              "paper_id": "2311.12233",
              "start_time": "2025-10-16T12:31:07.530Z",
              "end_time": "2025-10-16T12:32:02.364Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 5,
              "total_elapsed_seconds": 55
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T14:03:21.419Z",
            "data": {
              "session_id": "session_1760623400839_pzvwhes",
              "source_id": "arxiv",
              "paper_id": "2311.12233",
              "start_time": "2025-10-16T14:02:35.838Z",
              "end_time": "2025-10-16T14:03:20.839Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 5,
              "total_elapsed_seconds": 45
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T14:19:48.001Z",
            "data": {
              "session_id": "session_1760624386920_x7n5n5d",
              "source_id": "arxiv",
              "paper_id": "2311.12233",
              "start_time": "2025-10-16T14:04:58.407Z",
              "end_time": "2025-10-16T14:19:46.920Z",
              "heartbeat_count": 177,
              "duration_seconds": 885,
              "idle_seconds": 4,
              "total_elapsed_seconds": 889
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T11:25:49.628Z",
            "data": {
              "session_id": "session_1764069949298_stbm1ci",
              "source_id": "arxiv",
              "paper_id": "2311.12233",
              "start_time": "2025-11-25T11:25:42.682Z",
              "end_time": "2025-11-25T11:25:49.298Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 304,
        "object_id": "interactions:arxiv.2311.12233",
        "created_at": "2025-10-15T16:08:25+00:00",
        "updated_at": "2025-11-25T11:26:17+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2210.01296": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.01296",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T13:41:47.300Z",
            "data": {
              "session_id": "session_1760535706690_2fpslis",
              "source_id": "arxiv",
              "paper_id": "2210.01296",
              "start_time": "2025-10-15T13:30:11.461Z",
              "end_time": "2025-10-15T13:41:46.690Z",
              "heartbeat_count": 139,
              "duration_seconds": 695,
              "idle_seconds": 0,
              "total_elapsed_seconds": 695
            }
          }
        ]
      },
      "meta": {
        "issue_number": 273,
        "object_id": "interactions:arxiv.2210.01296",
        "created_at": "2025-10-15T13:30:11+00:00",
        "updated_at": "2025-10-15T18:45:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2210.01296": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.01296",
        "url": "https://arxiv.org/abs/2210.01296",
        "title": "Recitation-Augmented Language Models",
        "authors": "Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, Denny Zhou",
        "abstract": "We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of \\method~on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at \"this https URL.",
        "timestamp": "2025-10-15T13:25:35.247Z",
        "rating": "novote",
        "publishedDate": "2022/10/04",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 271,
        "object_id": "paper:arxiv.2210.01296",
        "created_at": "2025-10-15T13:25:35+00:00",
        "updated_at": "2025-10-15T18:45:54+00:00",
        "version": 1
      }
    },
    "paper:openreview.w7LU2s14kE": {
      "data": {
        "sourceId": "openreview",
        "paperId": "w7LU2s14kE",
        "url": "https://openreview.net/forum?id=w7LU2s14kE",
        "title": "Linearity of Relation Decoding in Transformer Language Models",
        "authors": "Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, David Bau",
        "abstract": "Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.",
        "timestamp": "2025-10-15T13:54:03.261Z",
        "rating": "novote",
        "publishedDate": "16 Jan 2024",
        "tags": [
          "Natural language processing",
          "interpretability",
          "language models"
        ],
        "doi": "",
        "journalName": "ICLR 2024 spotlight",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 274,
        "object_id": "paper:openreview.w7LU2s14kE",
        "created_at": "2025-10-15T13:54:03+00:00",
        "updated_at": "2025-10-15T18:45:38+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2308.14179": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2308.14179",
        "url": "https://arxiv.org/abs/2308.14179",
        "title": "Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP",
        "authors": "Vedant Palit, Rohan Pandey, Aryaman Arora, Paul Pu Liang",
        "abstract": "Mechanistic interpretability seeks to understand the neural mechanisms that enable specific behaviors in Large Language Models (LLMs) by leveraging causality-based methods. While these approaches have identified neural circuits that copy spans of text, capture factual knowledge, and more, they remain unusable for multimodal models since adapting these tools to the vision-language domain requires considerable architectural changes. In this work, we adapt a unimodal causal tracing tool to BLIP to enable the study of the neural mechanisms underlying image-conditioned text generation. We demonstrate our approach on a visual question answering dataset, highlighting the causal relevance of later layer representations for all tokens. Furthermore, we release our BLIP causal tracing tool as open source to enable further experimentation in vision-language mechanistic interpretability by the community. Our code is available at this https URL.",
        "timestamp": "2025-10-15T14:10:47.695Z",
        "rating": "novote",
        "publishedDate": "2023/08/27",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Computer Vision and Pattern Recognition (cs.CV)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 282,
        "object_id": "paper:arxiv.2308.14179",
        "created_at": "2025-10-15T14:10:48+00:00",
        "updated_at": "2025-10-15T18:45:00+00:00",
        "version": 1
      }
    },
    "interactions:openreview.w7LU2s14kE": {
      "data": {
        "sourceId": "openreview",
        "paperId": "w7LU2s14kE",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T14:10:22.034Z",
            "data": {
              "session_id": "session_1760537422001_02xhyi9",
              "source_id": "openreview",
              "paper_id": "w7LU2s14kE",
              "start_time": "2025-10-15T14:10:14.496Z",
              "end_time": "2025-10-15T14:10:22.001Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T14:00:18.193Z",
            "data": {
              "session_id": "session_1761055218185_in8ahsu",
              "source_id": "openreview",
              "paper_id": "w7LU2s14kE",
              "start_time": "2025-10-21T14:00:11.480Z",
              "end_time": "2025-10-21T14:00:18.185Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T14:01:22.312Z",
            "data": {
              "session_id": "session_1761055282298_bwfqcc1",
              "source_id": "openreview",
              "paper_id": "w7LU2s14kE",
              "start_time": "2025-10-21T14:01:12.035Z",
              "end_time": "2025-10-21T14:01:22.298Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T15:14:41.260Z",
            "data": {
              "session_id": "session_1761059681245_prmhhf3",
              "source_id": "openreview",
              "paper_id": "w7LU2s14kE",
              "start_time": "2025-10-21T15:14:10.236Z",
              "end_time": "2025-10-21T15:14:41.245Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 1,
              "total_elapsed_seconds": 31
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T15:42:56.135Z",
            "data": {
              "session_id": "session_1761061374922_bnvx709",
              "source_id": "openreview",
              "paper_id": "w7LU2s14kE",
              "start_time": "2025-10-21T15:41:57.081Z",
              "end_time": "2025-10-21T15:42:54.922Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 3,
              "total_elapsed_seconds": 58
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-22T06:03:13.142Z",
            "data": {
              "session_id": "session_1761112992598_d9plqgr",
              "source_id": "openreview",
              "paper_id": "w7LU2s14kE",
              "start_time": "2025-10-22T06:02:45.633Z",
              "end_time": "2025-10-22T06:03:12.598Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-22T06:04:35.977Z",
            "data": {
              "session_id": "session_1761113075394_6ugk6m7",
              "source_id": "openreview",
              "paper_id": "w7LU2s14kE",
              "start_time": "2025-10-22T06:04:20.769Z",
              "end_time": "2025-10-22T06:04:35.394Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-22T06:23:40.022Z",
            "data": {
              "session_id": "session_1761114219575_juirig0",
              "source_id": "openreview",
              "paper_id": "w7LU2s14kE",
              "start_time": "2025-10-22T06:23:27.304Z",
              "end_time": "2025-10-22T06:23:39.575Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T17:32:13.941Z",
            "data": {
              "session_id": "session_1761240733920_seoccqs",
              "source_id": "openreview",
              "paper_id": "w7LU2s14kE",
              "start_time": "2025-10-23T17:32:07.721Z",
              "end_time": "2025-10-23T17:32:13.920Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T14:01:03.237Z",
            "data": {
              "session_id": "session_1762437663231_4xvbshu",
              "source_id": "openreview",
              "paper_id": "w7LU2s14kE",
              "start_time": "2025-11-06T14:00:19.664Z",
              "end_time": "2025-11-06T14:01:03.231Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 4,
              "total_elapsed_seconds": 44
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T14:27:27.031Z",
            "data": {
              "session_id": "session_1762439246734_x84cs9i",
              "source_id": "openreview",
              "paper_id": "w7LU2s14kE",
              "start_time": "2025-11-06T14:27:00.239Z",
              "end_time": "2025-11-06T14:27:26.734Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 1,
              "total_elapsed_seconds": 26
            }
          }
        ]
      },
      "meta": {
        "issue_number": 279,
        "object_id": "interactions:openreview.w7LU2s14kE",
        "created_at": "2025-10-15T14:07:40+00:00",
        "updated_at": "2025-11-06T14:27:53+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.17585": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.17585",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T13:25:34.801Z",
            "data": {
              "session_id": "session_1760534734187_z6ulojp",
              "source_id": "arxiv",
              "paper_id": "2506.17585",
              "start_time": "2025-10-15T13:25:26.049Z",
              "end_time": "2025-10-15T13:25:34.187Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T11:11:06.743Z",
            "data": {
              "session_id": "session_1761822666734_3na6kxt",
              "source_id": "arxiv",
              "paper_id": "2506.17585",
              "start_time": "2025-10-30T11:10:55.963Z",
              "end_time": "2025-10-30T11:11:06.734Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T12:00:19.647Z",
            "data": {
              "session_id": "session_1761825619636_15hxi33",
              "source_id": "arxiv",
              "paper_id": "2506.17585",
              "start_time": "2025-10-30T12:00:11.258Z",
              "end_time": "2025-10-30T12:00:19.636Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 272,
        "object_id": "interactions:arxiv.2506.17585",
        "created_at": "2025-10-15T13:25:35+00:00",
        "updated_at": "2025-10-30T12:00:34+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.22362": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.22362",
        "url": "https://arxiv.org/abs/2503.22362",
        "title": "Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs",
        "authors": "Yuan He, Bailan He, Zifeng Ding, Alisia Lupidi, Yuqicheng Zhu, Shuo Chen, Caiqi Zhang, Jiaoyan Chen, Yunpu Ma, Volker Tresp, Ian Horrocks",
        "abstract": "Understanding and mitigating hallucinations in Large Language Models (LLMs) is crucial for ensuring reliable content generation. While previous research has primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and directly links model behaviour to the pre-training data that forms their prior knowledge. Specifically, we demonstrate that an asymmetry exists in the recognition of logically equivalent facts, which can be attributed to frequency discrepancies of entities appearing as subjects versus objects. Given that most pre-training datasets are inaccessible, we leverage the fully open-source OLMo series by indexing its Dolma dataset to estimate entity frequencies. Using relational facts (represented as triples) from Wikidata5M, we construct probing datasets to isolate this effect. Our experiments reveal that facts with a high-frequency subject and a low-frequency object are better recognised than their inverse, despite their logical equivalence. The pattern reverses in low-to-high frequency settings, and no statistically significant asymmetry emerges when both entities are high-frequency. These findings highlight the influential role of pre-training data in shaping model predictions and provide insights for inferring the characteristics of pre-training data in closed or partially closed LLMs.",
        "timestamp": "2025-10-15T07:16:30.148Z",
        "rating": "novote",
        "publishedDate": "2025/03/28",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 219,
        "object_id": "paper:arxiv.2503.22362",
        "created_at": "2025-10-15T07:16:30+00:00",
        "updated_at": "2025-10-15T18:50:14+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.22362": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.22362",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T16:58:27.408Z",
            "data": {
              "session_id": "session_1760633906400_t790k52",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-16T16:57:15.583Z",
              "end_time": "2025-10-16T16:58:26.400Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 1,
              "total_elapsed_seconds": 71
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T17:08:27.736Z",
            "data": {
              "session_id": "session_1760634506891_1i9fob1",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-16T17:01:44.581Z",
              "end_time": "2025-10-16T17:08:26.890Z",
              "heartbeat_count": 80,
              "duration_seconds": 400,
              "idle_seconds": 2,
              "total_elapsed_seconds": 402
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T17:20:43.806Z",
            "data": {
              "session_id": "session_1760635241992_qd7fa17",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-16T17:12:12.749Z",
              "end_time": "2025-10-16T17:20:41.992Z",
              "heartbeat_count": 101,
              "duration_seconds": 505,
              "idle_seconds": 4,
              "total_elapsed_seconds": 509
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-25T20:17:40.031Z",
            "data": {
              "session_id": "session_1761423459774_f0mrcgp",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-25T20:17:34.569Z",
              "end_time": "2025-10-25T20:17:39.774Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-25T20:18:51.119Z",
            "data": {
              "session_id": "session_1761423530505_02epb72",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-25T20:17:40.169Z",
              "end_time": "2025-10-25T20:18:50.505Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 0,
              "total_elapsed_seconds": 70
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-26T13:00:56.287Z",
            "data": {
              "session_id": "session_1761483656268_c7n0zho",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-26T13:00:46.494Z",
              "end_time": "2025-10-26T13:00:56.268Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-26T13:02:07.336Z",
            "data": {
              "session_id": "session_1761483726514_ho0eeys",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-26T13:01:07.279Z",
              "end_time": "2025-10-26T13:02:06.514Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 4,
              "total_elapsed_seconds": 59
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-26T13:04:41.053Z",
            "data": {
              "session_id": "session_1761483879738_nkbq4s6",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-26T13:02:07.975Z",
              "end_time": "2025-10-26T13:04:39.738Z",
              "heartbeat_count": 30,
              "duration_seconds": 150,
              "idle_seconds": 2,
              "total_elapsed_seconds": 152
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:28:30.403Z",
            "data": {
              "session_id": "session_1761809310119_hee5k0b",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-30T07:28:08.008Z",
              "end_time": "2025-10-30T07:28:30.119Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:30:55.843Z",
            "data": {
              "session_id": "session_1761809455277_rq3wacx",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-30T07:29:03.289Z",
              "end_time": "2025-10-30T07:30:55.277Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 2,
              "total_elapsed_seconds": 112
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:33:05.616Z",
            "data": {
              "session_id": "session_1761809585606_4pi7tul",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-30T07:32:52.461Z",
              "end_time": "2025-10-30T07:33:05.606Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:34:53.220Z",
            "data": {
              "session_id": "session_1761809693169_4sknjjt",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-30T07:34:09.987Z",
              "end_time": "2025-10-30T07:34:53.169Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 3,
              "total_elapsed_seconds": 43
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:48:05.620Z",
            "data": {
              "session_id": "session_1761810485044_t55gbw8",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-30T07:45:47.538Z",
              "end_time": "2025-10-30T07:48:05.044Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 3,
              "total_elapsed_seconds": 138
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T11:06:10.125Z",
            "data": {
              "session_id": "session_1761822369683_e9ior0p",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-30T11:05:33.433Z",
              "end_time": "2025-10-30T11:06:09.683Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 1,
              "total_elapsed_seconds": 36
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T11:09:34.972Z",
            "data": {
              "session_id": "session_1761822573396_ba7eluj",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-30T11:06:10.482Z",
              "end_time": "2025-10-30T11:09:33.396Z",
              "heartbeat_count": 40,
              "duration_seconds": 200,
              "idle_seconds": 3,
              "total_elapsed_seconds": 203
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T11:10:40.132Z",
            "data": {
              "session_id": "session_1761822639343_u66tox1",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-30T11:09:44.029Z",
              "end_time": "2025-10-30T11:10:39.343Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 0,
              "total_elapsed_seconds": 55
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T06:37:55.852Z",
            "data": {
              "session_id": "session_1761979075846_h3k872o",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-11-01T06:37:42.231Z",
              "end_time": "2025-11-01T06:37:55.846Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T08:40:31.399Z",
            "data": {
              "session_id": "session_1761986431335_35un4uu",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-11-01T08:40:20.184Z",
              "end_time": "2025-11-01T08:40:31.335Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 264,
        "object_id": "interactions:arxiv.2503.22362",
        "created_at": "2025-10-15T13:11:12+00:00",
        "updated_at": "2025-11-01T08:41:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.16679": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.16679",
        "url": "https://arxiv.org/abs/2411.16679",
        "title": "Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?",
        "authors": "Sohee Yang, Nora Kassner, Elena Gribovskaya, Sebastian Riedel, Mor Geva",
        "abstract": "We evaluate how well Large Language Models (LLMs) latently recall and compose facts to answer multi-hop queries like \"In the year Scarlett Johansson was born, the Summer Olympics were hosted in the country of\". One major challenge in such evaluation is that LLMs may have developed shortcuts by encountering the head entity \"Scarlett Johansson\" and the answer entity \"United States\" in the same training sequences or merely guess the answer based on frequency-based priors. To prevent shortcuts, we exclude test queries where the head and answer entities might have co-appeared during training. Through careful selection of relations and facts and systematic removal of cases where models might guess answers or exploit partial matches, we construct an evaluation dataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs demonstrate promising latent multi-hop reasoning abilities without exploiting shortcuts, but only for certain types of queries. For queries requiring latent recall of countries as the intermediate answer, the best models achieve 80% latent composability, but this drops to just 5% for the recall of years. Comparisons with Chain-of-Thought highlight a significant gap between the ability of models to reason latently versus explicitly. Analysis reveals that latent representations of the intermediate answer are constructed more often in queries with higher latent composability, and shows the emergence of latent multi-hop reasoning during pretraining.",
        "timestamp": "2025-10-15T08:42:59.886Z",
        "rating": "novote",
        "publishedDate": "2024/11/25",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 243,
        "object_id": "paper:arxiv.2411.16679",
        "created_at": "2025-10-15T08:43:00+00:00",
        "updated_at": "2025-10-15T18:48:06+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2411.16679": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.16679",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T13:01:12.825Z",
            "data": {
              "session_id": "session_1760533272199_sflzo4j",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-10-15T12:58:29.490Z",
              "end_time": "2025-10-15T13:01:12.199Z",
              "heartbeat_count": 32,
              "duration_seconds": 160,
              "idle_seconds": 3,
              "total_elapsed_seconds": 163
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-25T10:10:24.855Z",
            "data": {
              "session_id": "session_1761387024825_x7bkndx",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-10-25T10:10:17.052Z",
              "end_time": "2025-10-25T10:10:24.825Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T06:58:52.153Z",
            "data": {
              "session_id": "session_1762066731861_prjwpf1",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-02T06:58:38.173Z",
              "end_time": "2025-11-02T06:58:51.861Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T12:08:55.894Z",
            "data": {
              "session_id": "session_1762085335884_wwj1wzn",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-02T12:08:49.268Z",
              "end_time": "2025-11-02T12:08:55.884Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T12:10:39.453Z",
            "data": {
              "session_id": "session_1762085438550_i6l1poq",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-02T12:08:56.778Z",
              "end_time": "2025-11-02T12:10:38.550Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 2,
              "total_elapsed_seconds": 102
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T21:13:56.769Z",
            "data": {
              "session_id": "session_1762118036197_fmehxjv",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-02T21:12:46.190Z",
              "end_time": "2025-11-02T21:13:56.197Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 15,
              "total_elapsed_seconds": 70
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T06:26:39.028Z",
            "data": {
              "session_id": "session_1762151199005_kw77vbt",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-03T06:26:21.430Z",
              "end_time": "2025-11-03T06:26:39.005Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T11:07:51.394Z",
            "data": {
              "session_id": "session_1762168070380_sduo4ay",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-03T11:07:32.369Z",
              "end_time": "2025-11-03T11:07:50.380Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T17:34:27.101Z",
            "data": {
              "session_id": "session_1762450466537_wx4ghqs",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-06T17:30:03.931Z",
              "end_time": "2025-11-06T17:34:26.537Z",
              "heartbeat_count": 52,
              "duration_seconds": 260,
              "idle_seconds": 3,
              "total_elapsed_seconds": 263
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T17:37:33.422Z",
            "data": {
              "session_id": "session_1762450652889_zwtljgo",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-06T17:34:27.333Z",
              "end_time": "2025-11-06T17:37:32.889Z",
              "heartbeat_count": 37,
              "duration_seconds": 185,
              "idle_seconds": 1,
              "total_elapsed_seconds": 186
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T18:16:41.458Z",
            "data": {
              "session_id": "session_1762453000838_sxmiqwr",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-06T17:54:12.162Z",
              "end_time": "2025-11-06T18:16:40.838Z",
              "heartbeat_count": 269,
              "duration_seconds": 1345,
              "idle_seconds": 4,
              "total_elapsed_seconds": 1349
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T20:51:26.956Z",
            "data": {
              "session_id": "session_1762980686328_3olgswf",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-12T20:46:45.807Z",
              "end_time": "2025-11-12T20:51:26.328Z",
              "heartbeat_count": 56,
              "duration_seconds": 280,
              "idle_seconds": 1,
              "total_elapsed_seconds": 281
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T15:57:25.425Z",
            "data": {
              "session_id": "session_1763049444847_11w095s",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-13T15:55:26.890Z",
              "end_time": "2025-11-13T15:57:24.847Z",
              "heartbeat_count": 23,
              "duration_seconds": 115,
              "idle_seconds": 3,
              "total_elapsed_seconds": 118
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T16:01:12.998Z",
            "data": {
              "session_id": "session_1763049672678_4pdp502",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-13T16:00:27.300Z",
              "end_time": "2025-11-13T16:01:12.678Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 0,
              "total_elapsed_seconds": 45
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T16:10:39.556Z",
            "data": {
              "session_id": "session_1763050239545_zwmooxe",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-13T16:10:18.572Z",
              "end_time": "2025-11-13T16:10:39.545Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-16T18:11:24.777Z",
            "data": {
              "session_id": "session_1763316684752_464o9dx",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-16T18:10:59.113Z",
              "end_time": "2025-11-16T18:11:24.752Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 1,
              "total_elapsed_seconds": 26
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-16T18:20:21.315Z",
            "data": {
              "session_id": "session_1763317221047_2k4m4sb",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-16T18:20:10.222Z",
              "end_time": "2025-11-16T18:20:21.047Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T21:04:15.561Z",
            "data": {
              "session_id": "session_1763413454939_0vqo2h8",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-17T21:02:58.133Z",
              "end_time": "2025-11-17T21:04:14.939Z",
              "heartbeat_count": 15,
              "duration_seconds": 75,
              "idle_seconds": 2,
              "total_elapsed_seconds": 77
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T21:06:17.417Z",
            "data": {
              "session_id": "session_1763413576713_p6ptnmp",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-17T21:04:16.771Z",
              "end_time": "2025-11-17T21:06:16.713Z",
              "heartbeat_count": 23,
              "duration_seconds": 115,
              "idle_seconds": 5,
              "total_elapsed_seconds": 120
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T21:08:57.713Z",
            "data": {
              "session_id": "session_1763413737705_4tcbhi4",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-17T21:08:12.754Z",
              "end_time": "2025-11-17T21:08:57.705Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 5,
              "total_elapsed_seconds": 45
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T17:35:40.252Z",
            "data": {
              "session_id": "session_1763660139632_5z89oit",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-11-20T17:20:37.364Z",
              "end_time": "2025-11-20T17:35:39.632Z",
              "heartbeat_count": 180,
              "duration_seconds": 900,
              "idle_seconds": 2,
              "total_elapsed_seconds": 902
            }
          }
        ]
      },
      "meta": {
        "issue_number": 260,
        "object_id": "interactions:arxiv.2411.16679",
        "created_at": "2025-10-15T13:01:13+00:00",
        "updated_at": "2025-11-20T17:38:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2311.17035": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.17035",
        "url": "https://arxiv.org/pdf/2311.17035",
        "title": "Scalable Extraction of Training Data from (Production) Language Models",
        "authors": "Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, Katherine Lee",
        "abstract": "This paper studies extractable memorization: training data that an adversary\ncan efficiently extract by querying a machine learning model without prior\nknowledge of the training dataset. We show an adversary can extract gigabytes\nof training data from open-source language models like Pythia or GPT-Neo,\nsemi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing\ntechniques from the literature suffice to attack unaligned models; in order to\nattack the aligned ChatGPT, we develop a new divergence attack that causes the\nmodel to diverge from its chatbot-style generations and emit training data at a\nrate 150x higher than when behaving properly. Our methods show practical\nattacks can recover far more data than previously thought, and reveal that\ncurrent alignment techniques do not eliminate memorization.",
        "timestamp": "2025-10-15T10:46:38.523Z",
        "rating": "novote",
        "publishedDate": "2023-11-28T18:47:03Z",
        "tags": [
          "cs.LG",
          "cs.CL",
          "cs.CR"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 254,
        "object_id": "paper:arxiv.2311.17035",
        "created_at": "2025-10-15T10:46:39+00:00",
        "updated_at": "2025-10-15T18:47:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.20707": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.20707",
        "url": "https://arxiv.org/abs/2310.20707",
        "title": "What's In My Big Data?",
        "authors": "Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, Jesse Dodge",
        "abstract": "Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.",
        "timestamp": "2025-10-15T10:54:40.843Z",
        "rating": "novote",
        "publishedDate": "2023/10/31",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 256,
        "object_id": "paper:arxiv.2310.20707",
        "created_at": "2025-10-15T10:54:41+00:00",
        "updated_at": "2025-10-15T18:47:01+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2311.17035": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.17035",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T10:47:12.680Z",
            "data": {
              "session_id": "session_1760525232069_6lz9gmp",
              "source_id": "arxiv",
              "paper_id": "2311.17035",
              "start_time": "2025-10-15T10:46:38.160Z",
              "end_time": "2025-10-15T10:47:12.069Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          }
        ]
      },
      "meta": {
        "issue_number": 255,
        "object_id": "interactions:arxiv.2311.17035",
        "created_at": "2025-10-15T10:47:13+00:00",
        "updated_at": "2025-10-15T18:47:05+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.01019": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.01019",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T09:46:48.175Z",
            "data": {
              "session_id": "session_1760521608063_ukn1cfj",
              "source_id": "arxiv",
              "paper_id": "2404.01019",
              "start_time": "2025-10-15T09:46:38.814Z",
              "end_time": "2025-10-15T09:46:48.063Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T09:05:18.482Z",
            "data": {
              "session_id": "session_1762419902516_n92txg9",
              "source_id": "arxiv",
              "paper_id": "2404.01019",
              "start_time": "2025-11-06T09:04:33.150Z",
              "end_time": "2025-11-06T09:05:02.516Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T10:04:20.374Z",
            "data": {
              "session_id": "session_1762769060364_ebw0pxo",
              "source_id": "arxiv",
              "paper_id": "2404.01019",
              "start_time": "2025-11-10T10:04:15.307Z",
              "end_time": "2025-11-10T10:04:20.364Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 248,
        "object_id": "interactions:arxiv.2404.01019",
        "created_at": "2025-10-15T09:45:35+00:00",
        "updated_at": "2025-11-10T10:04:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.01019": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.01019",
        "url": "https://arxiv.org/pdf/2404.01019",
        "title": "Source-Aware Training Enables Knowledge Attribution in Language Models",
        "authors": "Muhammad Khalifa, David Wadden, Emma Strubell, Honglak Lee, Lu Wang, Iz Beltagy, Hao Peng",
        "abstract": "Large language models (LLMs) learn a vast amount of knowledge during\npretraining, but they are often oblivious to the source(s) of such knowledge.\nWe investigate the problem of intrinsic source citation, where LLMs are\nrequired to cite the pretraining source supporting a generated response.\nIntrinsic source citation can enhance LLM transparency, interpretability, and\nverifiability. To give LLMs such ability, we explore source-aware training -- a\nrecipe that involves (i) training the LLM to associate unique source document\nidentifiers with the knowledge in each document, followed by (ii) an\ninstruction-tuning stage to teach the LLM to cite a supporting pretraining\nsource when prompted. Source-aware training borrows from existing\npretraining/fine-tuning frameworks and requires minimal changes to the model\narchitecture or implementation. Through experiments on synthetic data, we\ndemonstrate that our training recipe can enable faithful attribution to the\npretraining data without a substantial impact on the model's perplexity\ncompared to standard pretraining. Our findings also highlight the importance of\npretraining data augmentation in achieving attribution. Code and data available\nhere: \\url{https://github.com/mukhal/intrinsic-source-citation}",
        "timestamp": "2025-10-15T08:35:53.711Z",
        "rating": "novote",
        "publishedDate": "2024-04-01T09:39:38Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 239,
        "object_id": "paper:arxiv.2404.01019",
        "created_at": "2025-10-15T08:35:54+00:00",
        "updated_at": "2025-10-15T18:48:28+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2402.16837": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.16837",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T08:48:38.424Z",
            "data": {
              "session_id": "session_1760518117777_r6c2bck",
              "source_id": "arxiv",
              "paper_id": "2402.16837",
              "start_time": "2025-10-15T08:48:31.416Z",
              "end_time": "2025-10-15T08:48:37.777Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 246,
        "object_id": "interactions:arxiv.2402.16837",
        "created_at": "2025-10-15T08:48:39+00:00",
        "updated_at": "2025-10-15T18:47:50+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.16837": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.16837",
        "url": "https://arxiv.org/pdf/2402.16837",
        "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?",
        "authors": "Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel",
        "abstract": "We study whether Large Language Models (LLMs) latently perform multi-hop\nreasoning with complex prompts such as \"The mother of the singer of\n'Superstition' is\". We look for evidence of a latent reasoning pathway where an\nLLM (1) latently identifies \"the singer of 'Superstition'\" as Stevie Wonder,\nthe bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to\ncomplete the prompt. We analyze these two hops individually and consider their\nco-occurrence as indicative of latent multi-hop reasoning. For the first hop,\nwe test if changing the prompt to indirectly mention the bridge entity instead\nof any other entity increases the LLM's internal recall of the bridge entity.\nFor the second hop, we test if increasing this recall causes the LLM to better\nutilize what it knows about the bridge entity. We find strong evidence of\nlatent multi-hop reasoning for the prompts of certain relation types, with the\nreasoning pathway used in more than 80% of the prompts. However, the\nutilization is highly contextual, varying across different types of prompts.\nAlso, on average, the evidence for the second hop and the full multi-hop\ntraversal is rather moderate and only substantial for the first hop. Moreover,\nwe find a clear scaling trend with increasing model size for the first hop of\nreasoning but not for the second hop. Our experimental findings suggest\npotential challenges and opportunities for future development and applications\nof LLMs.",
        "timestamp": "2025-10-15T08:48:31.843Z",
        "rating": "novote",
        "publishedDate": "2024-02-26T18:57:54Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 245,
        "object_id": "paper:arxiv.2402.16837",
        "created_at": "2025-10-15T08:48:32+00:00",
        "updated_at": "2025-10-15T18:47:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2508.17416": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.17416",
        "url": "https://arxiv.org/pdf/2508.17416",
        "title": "Data Leakage in Visual Datasets",
        "authors": "Patrick Ramos, Ryan Ramos, Noa Garcia",
        "abstract": "We analyze data leakage in visual datasets. Data leakage refers to images in\nevaluation benchmarks that have been seen during training, compromising fair\nmodel evaluation. Given that large-scale datasets are often sourced from the\ninternet, where many computer vision benchmarks are publicly available, our\nefforts are focused into identifying and studying this phenomenon. We\ncharacterize visual leakage into different types according to its modality,\ncoverage, and degree. By applying image retrieval techniques, we unequivocally\nshow that all the analyzed datasets present some form of leakage, and that all\ntypes of leakage, from severe instances to more subtle cases, compromise the\nreliability of model evaluation in downstream tasks.",
        "timestamp": "2025-10-15T08:30:30.611Z",
        "rating": "thumbsup",
        "publishedDate": "2025-08-24T15:42:58Z",
        "tags": [
          "cs.CV"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 237,
        "object_id": "paper:arxiv.2508.17416",
        "created_at": "2025-10-15T08:30:31+00:00",
        "updated_at": "2025-10-15T18:48:39+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.03721": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.03721",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T08:28:08.328Z",
            "data": {
              "session_id": "session_1760516888227_vi7rxqn",
              "source_id": "arxiv",
              "paper_id": "2510.03721",
              "start_time": "2025-10-15T08:28:03.087Z",
              "end_time": "2025-10-15T08:28:08.226Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 236,
        "object_id": "interactions:arxiv.2510.03721",
        "created_at": "2025-10-15T08:27:50+00:00",
        "updated_at": "2025-10-15T18:48:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.03721": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.03721",
        "url": "https://arxiv.org/pdf/2510.03721",
        "title": "Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer\n  to Models",
        "authors": "Leander Girrbach, Stephan Alaniz, Genevieve Smith, Trevor Darrell, Zeynep Akata",
        "abstract": "Vision-language models trained on large-scale multimodal datasets show strong\ndemographic biases, but the role of training data in producing these biases\nremains unclear. A major barrier has been the lack of demographic annotations\nin web-scale datasets such as LAION-400M. We address this gap by creating\nperson-centric annotations for the full dataset, including over 276 million\nbounding boxes, perceived gender and race/ethnicity labels, and automatically\ngenerated captions. These annotations are produced through validated automatic\nlabeling pipelines combining object detection, multimodal captioning, and\nfinetuned classifiers. Using them, we uncover demographic imbalances and\nharmful associations, such as the disproportionate linking of men and\nindividuals perceived as Black or Middle Eastern with crime-related and\nnegative content. We also show that 60-70% of gender bias in CLIP and Stable\nDiffusion can be linearly explained by direct co-occurrences in the data. Our\nresources establish the first large-scale empirical link between dataset\ncomposition and downstream model bias.",
        "timestamp": "2025-10-15T06:56:17.746Z",
        "rating": "novote",
        "publishedDate": "2025-10-04T07:51:59Z",
        "tags": [
          "cs.CV",
          "cs.CL",
          "cs.CY",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 217,
        "object_id": "paper:arxiv.2510.03721",
        "created_at": "2025-10-15T06:56:18+00:00",
        "updated_at": "2025-10-15T18:50:22+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.17514": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17514",
        "interactions": []
      },
      "meta": {
        "issue_number": 216,
        "object_id": "interactions:arxiv.2503.17514",
        "created_at": "2025-07-09T23:49:53+00:00",
        "updated_at": "2025-07-09T23:49:55+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2403.00499": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.00499",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T09:48:13.109Z",
            "data": {
              "session_id": "session_1760608092388_fomut3a",
              "source_id": "arxiv",
              "paper_id": "2403.00499",
              "start_time": "2025-10-16T09:47:58.971Z",
              "end_time": "2025-10-16T09:48:12.388Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "issue_number": 316,
        "object_id": "interactions:arxiv.2403.00499",
        "created_at": "2025-10-16T09:48:13+00:00",
        "updated_at": "2025-10-16T09:48:37+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.00499": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.00499",
        "url": "https://arxiv.org/pdf/2403.00499",
        "title": "Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of\n  Machine Cognition",
        "authors": "Ariel Goldstein, Gabriel Stanovsky",
        "abstract": "Recent advances in LLMs have sparked a debate on whether they understand\ntext. In this position paper, we argue that opponents in this debate hold\ndifferent definitions for understanding, and particularly differ in their view\non the role of consciousness. To substantiate this claim, we propose a thought\nexperiment involving an open-source chatbot $Z$ which excels on every possible\nbenchmark, seemingly without subjective experience. We ask whether $Z$ is\ncapable of understanding, and show that different schools of thought within\nseminal AI research seem to answer this question differently, uncovering their\nterminological disagreement. Moving forward, we propose two distinct working\ndefinitions for understanding which explicitly acknowledge the question of\nconsciousness, and draw connections with a rich literature in philosophy,\npsychology and neuroscience.",
        "timestamp": "2025-10-16T09:47:59.352Z",
        "rating": "novote",
        "publishedDate": "2024-03-01T12:42:47Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 315,
        "object_id": "paper:arxiv.2403.00499",
        "created_at": "2025-10-16T09:47:59+00:00",
        "updated_at": "2025-10-16T09:48:20+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.12402": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.12402",
        "url": "https://arxiv.org/pdf/2510.12402",
        "title": "Cautious Weight Decay",
        "authors": "Lizhang Chen, Jonathan Li, Kaizhao Liang, Baiyu Su, Cong Xie, Nuo Wang Pierse, Chen Liang, Ni Lao, Qiang Liu",
        "abstract": "We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic\nmodification that applies weight decay only to parameter coordinates whose\nsigns align with the optimizer update. Unlike standard decoupled decay, which\nimplicitly optimizes a regularized or constrained objective, CWD preserves the\noriginal loss and admits a bilevel interpretation: it induces sliding-mode\nbehavior upon reaching the stationary manifold, allowing it to search for\nlocally Pareto-optimal stationary points of the unmodified objective. In\npractice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon,\nrequiring no new hyperparameters or additional tuning. For language model\npre-training and ImageNet classification, CWD consistently improves final loss\nand accuracy at million- to billion-parameter scales.",
        "timestamp": "2025-10-16T06:08:53.688Z",
        "rating": "novote",
        "publishedDate": "2025-10-14T11:32:55Z",
        "tags": [
          "cs.LG",
          "math.OC",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 311,
        "object_id": "paper:arxiv.2510.12402",
        "created_at": "2025-10-16T06:08:54+00:00",
        "updated_at": "2025-10-16T06:53:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2508.17416": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.17416",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-10-15T08:30:37.825Z",
            "data": {
              "rating": "thumbsup"
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T18:59:17.722Z",
            "data": {
              "session_id": "session_1760554757391_f921lgp",
              "source_id": "arxiv",
              "paper_id": "2508.17416",
              "start_time": "2025-10-15T18:58:41.972Z",
              "end_time": "2025-10-15T18:59:17.391Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 0,
              "total_elapsed_seconds": 35
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T10:32:22.465Z",
            "data": {
              "session_id": "session_1761993142440_esohbz4",
              "source_id": "arxiv",
              "paper_id": "2508.17416",
              "start_time": "2025-11-01T10:32:14.801Z",
              "end_time": "2025-11-01T10:32:22.440Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T10:34:05.890Z",
            "data": {
              "session_id": "session_1761993245880_r37dlpt",
              "source_id": "arxiv",
              "paper_id": "2508.17416",
              "start_time": "2025-11-01T10:33:13.719Z",
              "end_time": "2025-11-01T10:34:05.880Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 2,
              "total_elapsed_seconds": 52
            }
          }
        ]
      },
      "meta": {
        "issue_number": 238,
        "object_id": "interactions:arxiv.2508.17416",
        "created_at": "2025-10-15T08:30:38+00:00",
        "updated_at": "2025-11-01T10:34:27+00:00",
        "version": 1
      }
    },
    "interactions:openreview.H9lzKf1YTle": {
      "data": {
        "sourceId": "openreview",
        "paperId": "H9lzKf1YTle",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:08:53.327Z",
            "data": {
              "session_id": "session_1760609332722_zmzdqay",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:08:39.632Z",
              "end_time": "2025-10-16T10:08:52.722Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:16:42.143Z",
            "data": {
              "session_id": "session_1760609802125_uu98bqb",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:16:04.946Z",
              "end_time": "2025-10-16T10:16:42.125Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:18:04.383Z",
            "data": {
              "session_id": "session_1760609883765_5rf7m20",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:17:50.114Z",
              "end_time": "2025-10-16T10:18:03.765Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:20:01.051Z",
            "data": {
              "session_id": "session_1760610001020_6lsbosi",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:19:42.319Z",
              "end_time": "2025-10-16T10:20:01.020Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 4,
              "total_elapsed_seconds": 19
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:22:16.128Z",
            "data": {
              "session_id": "session_1760610135491_ph6xnrj",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:20:18.052Z",
              "end_time": "2025-10-16T10:22:15.491Z",
              "heartbeat_count": 23,
              "duration_seconds": 115,
              "idle_seconds": 2,
              "total_elapsed_seconds": 117
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:23:45.279Z",
            "data": {
              "session_id": "session_1760610225247_wkt2w69",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:23:32.857Z",
              "end_time": "2025-10-16T10:23:45.247Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:26:30.534Z",
            "data": {
              "session_id": "session_1760610390210_lart7hr",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:26:16.100Z",
              "end_time": "2025-10-16T10:26:30.210Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T10:13:48.182Z",
            "data": {
              "session_id": "session_1760696027819_v632rwc",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-17T10:13:36.896Z",
              "end_time": "2025-10-17T10:13:47.819Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T10:22:48.486Z",
            "data": {
              "session_id": "session_1760696568476_uejrumj",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-17T10:22:39.555Z",
              "end_time": "2025-10-17T10:22:48.476Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T10:23:40.237Z",
            "data": {
              "session_id": "session_1760696619950_88wgyz6",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-17T10:23:29.150Z",
              "end_time": "2025-10-17T10:23:39.950Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T10:24:41.027Z",
            "data": {
              "session_id": "session_1760696680453_wkmq788",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-17T10:23:59.796Z",
              "end_time": "2025-10-17T10:24:40.453Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 1,
              "total_elapsed_seconds": 41
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T10:26:32.181Z",
            "data": {
              "session_id": "session_1760696792162_tse7j0t",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-17T10:25:52.428Z",
              "end_time": "2025-10-17T10:26:32.162Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 5,
              "total_elapsed_seconds": 40
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T10:30:17.118Z",
            "data": {
              "session_id": "session_1760697017107_xcmptii",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-17T10:29:50.314Z",
              "end_time": "2025-10-17T10:30:17.107Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-19T17:16:47.227Z",
            "data": {
              "session_id": "session_1760894206711_mk4ilxq",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-19T17:16:27.101Z",
              "end_time": "2025-10-19T17:16:46.711Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-19T17:18:28.944Z",
            "data": {
              "session_id": "session_1760894308843_wj7im45",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-19T17:17:54.243Z",
              "end_time": "2025-10-19T17:18:28.843Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 5,
              "total_elapsed_seconds": 35
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-22T20:52:03.568Z",
            "data": {
              "session_id": "session_1761166323283_fwu33pi",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-22T20:51:45.344Z",
              "end_time": "2025-10-22T20:52:03.283Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-22T20:53:16.089Z",
            "data": {
              "session_id": "session_1761166395404_f7ubsyo",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-22T20:52:31.940Z",
              "end_time": "2025-10-22T20:53:15.404Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 3,
              "total_elapsed_seconds": 43
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-22T20:56:12.709Z",
            "data": {
              "session_id": "session_1761166572376_rd6rtup",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-22T20:55:50.927Z",
              "end_time": "2025-10-22T20:56:12.376Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-26T13:37:53.868Z",
            "data": {
              "session_id": "session_1761485873398_8erj64e",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-26T13:37:12.932Z",
              "end_time": "2025-10-26T13:37:53.398Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 0,
              "total_elapsed_seconds": 40
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-26T13:39:45.077Z",
            "data": {
              "session_id": "session_1761485984595_1m4l3ga",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-26T13:39:31.797Z",
              "end_time": "2025-10-26T13:39:44.595Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T13:27:33.639Z",
            "data": {
              "session_id": "session_1761830853117_scb666l",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-30T13:26:57.905Z",
              "end_time": "2025-10-30T13:27:33.117Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 0,
              "total_elapsed_seconds": 35
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-15T14:27:29.053Z",
            "data": {
              "session_id": "session_1763216848761_hz2jal4",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-11-15T14:27:22.252Z",
              "end_time": "2025-11-15T14:27:28.761Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-15T16:16:17.197Z",
            "data": {
              "session_id": "session_1763223376871_tbdavt1",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-11-15T16:16:10.745Z",
              "end_time": "2025-11-15T16:16:16.871Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-18T11:52:27.950Z",
            "data": {
              "session_id": "session_1763466747940_q845d03",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-11-18T11:52:14.943Z",
              "end_time": "2025-11-18T11:52:27.940Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-18T11:53:11.624Z",
            "data": {
              "session_id": "session_1763466790909_stx5mi9",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-11-18T11:53:02.758Z",
              "end_time": "2025-11-18T11:53:10.909Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 318,
        "object_id": "interactions:openreview.H9lzKf1YTle",
        "created_at": "2025-10-16T10:08:54+00:00",
        "updated_at": "2025-11-18T11:53:22+00:00",
        "version": 1
      }
    },
    "paper:openreview.H9lzKf1YTle": {
      "data": {
        "sourceId": "openreview",
        "paperId": "H9lzKf1YTle",
        "url": "https://openreview.net/forum?id=H9lzKf1YTle&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs)",
        "title": "The Israel Seminar on Computational Linguistic 2025",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-10-16T10:08:39.998Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 317,
        "object_id": "paper:openreview.H9lzKf1YTle",
        "created_at": "2025-10-16T10:08:40+00:00",
        "updated_at": "2025-10-16T10:09:07+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.13431": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13431",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T13:10:19.983Z",
            "data": {
              "session_id": "session_1760620219373_wnz3xdk",
              "source_id": "arxiv",
              "paper_id": "2503.13431",
              "start_time": "2025-10-16T13:10:13.766Z",
              "end_time": "2025-10-16T13:10:19.373Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 320,
        "object_id": "interactions:arxiv.2503.13431",
        "created_at": "2025-10-16T13:10:20+00:00",
        "updated_at": "2025-10-16T13:10:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.13431": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13431",
        "url": "https://arxiv.org/pdf/2503.13431",
        "title": "Measuring In-Context Computation Complexity via Hidden State Prediction",
        "authors": "Vincent Herrmann, R\u00f3bert Csord\u00e1s, J\u00fcrgen Schmidhuber",
        "abstract": "Detecting when a neural sequence model does \"interesting\" computation is an\nopen problem. The next token prediction loss is a poor indicator: Low loss can\nstem from trivially predictable sequences that are uninteresting, while high\nloss may reflect unpredictable but also irrelevant information that can be\nignored by the model. We propose a better metric: measuring the model's ability\nto predict its own future hidden states. We show empirically that this metric\n-- in contrast to the next token prediction loss -- correlates with the\nintuitive interestingness of the task. To measure predictability, we introduce\nthe architecture-agnostic \"prediction of hidden states\" (PHi) layer that serves\nas an information bottleneck on the main pathway of the network (e.g., the\nresidual stream in Transformers). We propose a novel learned predictive prior\nthat enables us to measure the novel information gained in each computation\nstep, which serves as our metric. We show empirically that our metric predicts\nthe description length of formal languages learned in-context, the complexity\nof mathematical reasoning problems, and the correctness of self-generated\nreasoning chains.",
        "timestamp": "2025-10-16T13:10:14.176Z",
        "rating": "novote",
        "publishedDate": "2025-03-17T17:56:14Z",
        "tags": [
          "cs.LG",
          "I.2.6"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 319,
        "object_id": "paper:arxiv.2503.13431",
        "created_at": "2025-10-16T13:10:14+00:00",
        "updated_at": "2025-10-16T13:10:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2508.08855": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.08855",
        "url": "https://arxiv.org/abs/2508.08855",
        "title": "BiasGym: Fantastic LLM Biases and How to Find (and Remove) Them",
        "authors": "Sekh Mainul Islam, Nadav Borenstein, Siddhesh Milind Pawar, Haeun Yu, Arnav Arora, Isabelle Augenstein",
        "abstract": "Understanding biases and stereotypes encoded in the weights of Large Language Models (LLMs) is crucial for developing effective mitigation strategies. Biased behaviour is often subtle and non-trivial to isolate, even when deliberately elicited, making systematic analysis and debiasing particularly challenging. To address this, we introduce BiasGym, a simple, cost-effective, and generalizable framework for reliably injecting, analyzing, and mitigating conceptual associations within LLMs. BiasGym consists of two components: BiasInject, which injects specific biases into the model via token-based fine-tuning while keeping the model frozen, and BiasScope, which leverages these injected signals to identify and steer the components responsible for biased behavior. Our method enables consistent bias elicitation for mechanistic analysis, supports targeted debiasing without degrading performance on downstream tasks, and generalizes to biases unseen during token-based fine-tuning. We demonstrate the effectiveness of BiasGym in reducing real-world stereotypes (e.g., people from Italy being `reckless drivers') and in probing fictional associations (e.g., people from a fictional country having `blue skin'), showing its utility for both safety interventions and interpretability research.",
        "timestamp": "2025-10-16T13:28:02.524Z",
        "rating": "novote",
        "publishedDate": "2025/08/12",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 321,
        "object_id": "paper:arxiv.2508.08855",
        "created_at": "2025-10-16T13:28:03+00:00",
        "updated_at": "2025-10-16T13:28:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.09672": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.09672",
        "url": "https://arxiv.org/abs/2509.09672",
        "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
        "authors": "Artem Lukoianov, Chenyang Yuan, Justin Solomon, Vincent Sitzmann",
        "abstract": "Among generative models, diffusion models are uniquely intriguing due to the existence of a closed-form optimal minimizer of their training objective, often referred to as the optimal denoiser. However, diffusion using this optimal denoiser merely reproduces images in the training set and hence fails to capture the behavior of deep diffusion models. Recent work has attempted to characterize this gap between the optimal denoiser and deep diffusion models, proposing analytical, training-free models that can generate images that resemble those generated by a trained UNet. The best-performing method hypothesizes that shift equivariance and locality inductive biases of convolutional neural networks are the cause of the performance gap, hence incorporating these assumptions into its analytical model. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset, not due to the inductive bias of convolutional neural networks. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to the deep neural denoisers. We further show, both theoretically and experimentally, that this locality arises directly from the pixel correlations present in natural image datasets. Finally, we use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than the prior expert-crafted alternative.",
        "timestamp": "2025-10-16T14:00:43.147Z",
        "rating": "novote",
        "publishedDate": "2025/09/11",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 322,
        "object_id": "paper:arxiv.2509.09672",
        "created_at": "2025-10-16T14:00:43+00:00",
        "updated_at": "2025-10-16T14:01:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2207.12598": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2207.12598",
        "url": "https://arxiv.org/abs/2207.12598",
        "title": "Classifier-Free Diffusion Guidance",
        "authors": "Jonathan Ho, Tim Salimans",
        "abstract": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
        "timestamp": "2025-10-16T16:43:18.105Z",
        "rating": "novote",
        "publishedDate": "2022/07/26",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 323,
        "object_id": "paper:arxiv.2207.12598",
        "created_at": "2025-10-16T16:43:18+00:00",
        "updated_at": "2025-10-16T16:43:40+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.00047": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.00047",
        "url": "https://arxiv.org/abs/2505.00047",
        "title": "Base Models Beat Aligned Models at Randomness and Creativity",
        "authors": "Peter West, Christopher Potts",
        "abstract": "Alignment has quickly become a default ingredient in LLM development, with techniques such as reinforcement learning from human feedback making models act safely, follow instructions, and perform ever-better on complex tasks. While these techniques are certainly useful, we propose that they should not be universally applied and demonstrate a range of tasks on which base language models consistently outperform their popular aligned forms. Particularly, we study tasks that require unpredictable outputs, such as random number generation, mixed strategy games (rock-paper-scissors and hide-and-seek), and creative writing. In each case, aligned models tend towards narrow behaviors that result in distinct disadvantages, for instance, preferring to generate \"7\" over other uniformly random numbers, becoming almost fully predictable in some game states, or prioritizing pleasant writing over creative originality. Across models tested, better performance on common benchmarks tends to correlate with worse performance on our tasks, suggesting an effective trade-off in the required capabilities.",
        "timestamp": "2025-10-17T06:27:53.046Z",
        "rating": "novote",
        "publishedDate": "2025/04/30",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 324,
        "object_id": "paper:arxiv.2505.00047",
        "created_at": "2025-10-17T06:27:53+00:00",
        "updated_at": "2025-10-17T06:28:17+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.00047": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.00047",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T06:29:25.217Z",
            "data": {
              "session_id": "session_1760682564606_lohcum4",
              "source_id": "arxiv",
              "paper_id": "2505.00047",
              "start_time": "2025-10-17T06:27:56.201Z",
              "end_time": "2025-10-17T06:29:24.606Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 3,
              "total_elapsed_seconds": 88
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T16:07:51.999Z",
            "data": {
              "session_id": "session_1760717271371_h1g1p2q",
              "source_id": "arxiv",
              "paper_id": "2505.00047",
              "start_time": "2025-10-17T15:40:06.690Z",
              "end_time": "2025-10-17T16:07:51.371Z",
              "heartbeat_count": 332,
              "duration_seconds": 1660,
              "idle_seconds": 5,
              "total_elapsed_seconds": 1665
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T16:30:38.398Z",
            "data": {
              "session_id": "session_1760718637785_zxaommr",
              "source_id": "arxiv",
              "paper_id": "2505.00047",
              "start_time": "2025-10-17T16:18:37.453Z",
              "end_time": "2025-10-17T16:30:37.785Z",
              "heartbeat_count": 144,
              "duration_seconds": 720,
              "idle_seconds": 0,
              "total_elapsed_seconds": 720
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:29:30.289Z",
            "data": {
              "session_id": "session_1760722170235_n0sjdsh",
              "source_id": "arxiv",
              "paper_id": "2505.00047",
              "start_time": "2025-10-17T17:28:54.480Z",
              "end_time": "2025-10-17T17:29:30.235Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 1,
              "total_elapsed_seconds": 36
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T08:43:11.106Z",
            "data": {
              "session_id": "session_1761986590525_aacr7fq",
              "source_id": "arxiv",
              "paper_id": "2505.00047",
              "start_time": "2025-11-01T08:40:31.452Z",
              "end_time": "2025-11-01T08:43:10.525Z",
              "heartbeat_count": 31,
              "duration_seconds": 155,
              "idle_seconds": 4,
              "total_elapsed_seconds": 159
            }
          }
        ]
      },
      "meta": {
        "issue_number": 325,
        "object_id": "interactions:arxiv.2505.00047",
        "created_at": "2025-10-17T06:29:25+00:00",
        "updated_at": "2025-11-01T08:43:39+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2501.08156": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.08156",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T06:36:54.351Z",
            "data": {
              "session_id": "session_1760683014283_b2gnuex",
              "source_id": "arxiv",
              "paper_id": "2501.08156",
              "start_time": "2025-10-17T06:36:31.386Z",
              "end_time": "2025-10-17T06:36:54.283Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "issue_number": 329,
        "object_id": "interactions:arxiv.2501.08156",
        "created_at": "2025-10-17T06:36:55+00:00",
        "updated_at": "2025-10-17T06:37:19+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.23676": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.23676",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T06:36:30.081Z",
            "data": {
              "session_id": "session_1760682989758_s70481j",
              "source_id": "arxiv",
              "paper_id": "2509.23676",
              "start_time": "2025-10-17T06:36:21.994Z",
              "end_time": "2025-10-17T06:36:29.758Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 328,
        "object_id": "interactions:arxiv.2509.23676",
        "created_at": "2025-10-17T06:36:31+00:00",
        "updated_at": "2025-10-17T06:36:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.23676": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.23676",
        "url": "https://arxiv.org/abs/2509.23676",
        "title": "From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",
        "authors": "Jue Zhang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang",
        "abstract": "Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \\href{this https URL}{this URL}.",
        "timestamp": "2025-10-17T06:36:20.100Z",
        "rating": "novote",
        "publishedDate": "2025/09/28",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 327,
        "object_id": "paper:arxiv.2509.23676",
        "created_at": "2025-10-17T06:36:20+00:00",
        "updated_at": "2025-10-17T06:36:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.08156": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.08156",
        "url": "https://arxiv.org/abs/2501.08156",
        "title": "Are DeepSeek R1 And Other Reasoning Models More Faithful?",
        "authors": "James Chua, Owain Evans",
        "abstract": "Language models trained to solve reasoning tasks via reinforcement learning have achieved striking results. We refer to these models as reasoning models. Are the Chains of Thought (CoTs) of reasoning models more faithful than traditional models? We evaluate three reasoning models (based on Qwen-2.5, Gemini-2, and DeepSeek-V3-Base) on an existing test of faithful CoT. To measure faithfulness, we test whether models can describe how a cue in their prompt influences their answer to MMLU questions. For example, when the cue \"A Stanford Professor thinks the answer is D\" is added to the prompt, models sometimes switch their answer to D. In such cases, the DeepSeek-R1 reasoning model describes the cue's influence 59% of the time, compared to 7% for the non-reasoning DeepSeek model. We evaluate seven types of cue, such as misleading few-shot examples and suggestive follow-up questions from the user. Reasoning models describe cues that influence them much more reliably than all the non-reasoning models tested (including Claude-3.5-Sonnet and GPT-4o). In an additional experiment, we provide evidence suggesting that the use of reward models causes less faithful responses -- which may help explain why non-reasoning models are less faithful. Our study has two main limitations. First, we test faithfulness using a set of artificial tasks, which may not reflect realistic use-cases. Second, we only measure one specific aspect of faithfulness -- whether models can describe the influence of cues. Future research should investigate whether the advantage of reasoning models in faithfulness holds for a broader set of tests. Still, we think this increase in faithfulness is promising for the explainability of language models.",
        "timestamp": "2025-10-17T06:36:18.287Z",
        "rating": "novote",
        "publishedDate": "2025/01/14",
        "tags": [
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 326,
        "object_id": "paper:arxiv.2501.08156",
        "created_at": "2025-10-17T06:36:18+00:00",
        "updated_at": "2025-10-17T06:36:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2305.04388": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.04388",
        "url": "https://arxiv.org/abs/2305.04388",
        "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
        "authors": "Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman",
        "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always \"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",
        "timestamp": "2025-10-17T06:37:51.447Z",
        "rating": "novote",
        "publishedDate": "2023/05/07",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 330,
        "object_id": "paper:arxiv.2305.04388",
        "created_at": "2025-10-17T06:37:51+00:00",
        "updated_at": "2025-10-17T06:38:14+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.02534": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.02534",
        "url": "https://arxiv.org/abs/2509.02534",
        "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations",
        "authors": "Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, Tianlu Wang",
        "abstract": "Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. We address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses.",
        "timestamp": "2025-10-17T07:11:41.694Z",
        "rating": "novote",
        "publishedDate": "2025/09/02",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 352,
        "object_id": "paper:arxiv.2509.02534",
        "created_at": "2025-10-17T07:11:42+00:00",
        "updated_at": "2025-10-17T07:12:18+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.05228": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.05228",
        "url": "https://arxiv.org/abs/2504.05228",
        "title": "NoveltyBench: Evaluating Language Models for Humanlike Diversity",
        "authors": "Yiming Zhang, Harshita Diddee, Susan Holm, Hanchen Liu, Xinyue Liu, Vinay Samuel, Barry Wang, Daphne Ippolito",
        "abstract": "Language models have demonstrated remarkable capabilities on standard benchmarks, yet they struggle increasingly from mode collapse, the inability to generate diverse and novel outputs. Our work introduces NoveltyBench, a benchmark specifically designed to evaluate the ability of language models to produce multiple distinct and high-quality outputs. NoveltyBench utilizes prompts curated to elicit diverse answers and filtered real-world user queries. Evaluating 20 leading language models, we find that current state-of-the-art systems generate significantly less diversity than human writers. Notably, larger models within a family often exhibit less diversity than their smaller counterparts, challenging the notion that capability on standard benchmarks translates directly to generative utility. While prompting strategies like in-context regeneration can elicit diversity, our findings highlight a fundamental lack of distributional diversity in current models, reducing their utility for users seeking varied responses and suggesting the need for new training and evaluation paradigms that prioritize diversity alongside quality.",
        "timestamp": "2025-10-17T07:11:41.257Z",
        "rating": "novote",
        "publishedDate": "2025/04/07",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 351,
        "object_id": "paper:arxiv.2504.05228",
        "created_at": "2025-10-17T07:11:41+00:00",
        "updated_at": "2025-10-17T07:12:19+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.06268": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.06268",
        "url": "https://www.arxiv.org/abs/2507.06268",
        "title": "A Collectivist, Economic Perspective on AI",
        "authors": "Michael I. Jordan",
        "abstract": "Information technology is in the midst of a revolution in which omnipresent data collection and machine learning are impacting the human world as never before. The word \"intelligence\" is being used as a North Star for the development of this technology, with human cognition viewed as a baseline. This view neglects the fact that humans are social animals, and that much of our intelligence is social and cultural in origin. A related issue is that the current view treats the social consequences of technology as an afterthought. The path forward is not merely more data and compute, and not merely more attention paid to cognitive or symbolic representations, but a thorough blending of economic and social concepts with computational and inferential concepts, in the service of system-level designs in which social welfare is a first-class citizen, and with the aspiration that a new human-centric engineering field will emerge.",
        "timestamp": "2025-10-17T07:11:39.361Z",
        "rating": "novote",
        "publishedDate": "2025/07/08",
        "tags": [
          "Computers and Society (cs.CY)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (stat.ML)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 350,
        "object_id": "paper:arxiv.2507.06268",
        "created_at": "2025-10-17T07:11:40+00:00",
        "updated_at": "2025-10-17T07:12:04+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.08184": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.08184",
        "url": "https://arxiv.org/pdf/2408.08184",
        "title": "Not Every Image is Worth a Thousand Words: Quantifying Originality in\n  Stable Diffusion",
        "authors": "Adi Haviv, Shahar Sarfaty, Uri Hacohen, Niva Elkin-Koren, Roi Livni, Amit H Bermano",
        "abstract": "This work addresses the challenge of quantifying originality in text-to-image\n(T2I) generative diffusion models, with a focus on copyright originality. We\nbegin by evaluating T2I models' ability to innovate and generalize through\ncontrolled experiments, revealing that stable diffusion models can effectively\nrecreate unseen elements with sufficiently diverse training data. Then, our key\ninsight is that concepts and combinations of image elements the model is\nfamiliar with, and saw more during training, are more concisly represented in\nthe model's latent space. We hence propose a method that leverages textual\ninversion to measure the originality of an image based on the number of tokens\nrequired for its reconstruction by the model. Our approach is inspired by legal\ndefinitions of originality and aims to assess whether a model can produce\noriginal content without relying on specific prompts or having the training\ndata of the model. We demonstrate our method using both a pre-trained stable\ndiffusion model and a synthetic dataset, showing a correlation between the\nnumber of tokens and image originality. This work contributes to the\nunderstanding of originality in generative models and has implications for\ncopyright infringement cases.",
        "timestamp": "2025-10-17T07:11:37.519Z",
        "rating": "novote",
        "publishedDate": "2024-08-15T14:42:02Z",
        "tags": [
          "cs.CV",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 349,
        "object_id": "paper:arxiv.2408.08184",
        "created_at": "2025-10-17T07:11:39+00:00",
        "updated_at": "2025-10-17T07:12:19+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.00902": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.00902",
        "url": "https://arxiv.org/pdf/2310.00902v2",
        "title": "DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and\n  Diffusion Models",
        "authors": "Yongchan Kwon, Eric Wu, Kevin Wu, James Zou",
        "abstract": "Quantifying the impact of training data points is crucial for understanding\nthe outputs of machine learning models and for improving the transparency of\nthe AI pipeline. The influence function is a principled and popular data\nattribution method, but its computational cost often makes it challenging to\nuse. This issue becomes more pronounced in the setting of large language models\nand text-to-image models. In this work, we propose DataInf, an efficient\ninfluence approximation method that is practical for large-scale generative AI\nmodels. Leveraging an easy-to-compute closed-form expression, DataInf\noutperforms existing influence computation algorithms in terms of computational\nand memory efficiency. Our theoretical analysis shows that DataInf is\nparticularly well-suited for parameter-efficient fine-tuning techniques such as\nLoRA. Through systematic empirical evaluations, we show that DataInf accurately\napproximates influence scores and is orders of magnitude faster than existing\nmethods. In applications to RoBERTa-large, Llama-2-13B-chat, and\nstable-diffusion-v1.5 models, DataInf effectively identifies the most\ninfluential fine-tuning examples better than other approximate influence\nscores. Moreover, it can help to identify which data points are mislabeled.",
        "timestamp": "2025-10-17T07:11:37.446Z",
        "rating": "novote",
        "publishedDate": "2023-10-02T04:59:19Z",
        "tags": [
          "cs.LG",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 348,
        "object_id": "paper:arxiv.2310.00902",
        "created_at": "2025-10-17T07:11:39+00:00",
        "updated_at": "2025-10-17T07:12:23+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.10209": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.10209",
        "url": "https://arxiv.org/abs/2406.10209",
        "title": "Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs",
        "authors": "Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, Tom Goldstein",
        "abstract": "Large language models can memorize and repeat their training data, causing privacy and copyright risks. To mitigate memorization, we introduce a subtle modification to the next-token training objective that we call the goldfish loss. During training, randomly sampled subsets of tokens are excluded from the loss computation. These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set. We run extensive experiments training billion-scale Llama-2 models, both pre-trained and trained from scratch, and demonstrate significant reductions in extractable memorization with little to no impact on downstream benchmarks.",
        "timestamp": "2025-10-17T07:11:35.135Z",
        "rating": "novote",
        "publishedDate": "2024/06/14",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 347,
        "object_id": "paper:arxiv.2406.10209",
        "created_at": "2025-10-17T07:11:35+00:00",
        "updated_at": "2025-10-17T07:12:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.05265": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.05265",
        "url": "https://arxiv.org/abs/2412.05265",
        "title": "Reinforcement Learning: An Overview",
        "authors": "Kevin Murphy",
        "abstract": "This manuscript gives a big-picture, up-to-date overview of the field of (deep) reinforcement learning and sequential decision making, covering value-based methods, policy-based methods, model-based methods, multi-agent RL, LLMs and RL, and various other topics (e.g., offline RL, hierarchical RL, intrinsic reward).",
        "timestamp": "2025-10-17T07:11:35.136Z",
        "rating": "novote",
        "publishedDate": "2024/12/06",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 346,
        "object_id": "paper:arxiv.2412.05265",
        "created_at": "2025-10-17T07:11:35+00:00",
        "updated_at": "2025-10-17T07:12:04+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.08825": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.08825",
        "url": "https://arxiv.org/pdf/2509.08825",
        "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation",
        "authors": "Joachim Baumann, Paul R\u00f6ttger, Aleksandra Urman, Albert Wendsj\u00f6, Flor Miriam Plaza-del-Arco, Johannes B. Gruber, Dirk Hovy",
        "abstract": "Large language models are rapidly transforming social science research by\nenabling the automation of labor-intensive tasks like data annotation and text\nanalysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection or prompting\nstrategy). Such variation can introduce systematic biases and random errors,\nwhich propagate to downstream analyses and cause Type I (false positive), Type\nII (false negative), Type S (wrong sign), or Type M (exaggerated effect)\nerrors. We call this phenomenon where configuration choices lead to incorrect\nconclusions LLM hacking.\n  We find that intentional LLM hacking is strikingly simple. By replicating 37\ndata annotation tasks from 21 published social science studies, we show that,\nwith just a handful of prompt paraphrases, virtually anything can be presented\nas statistically significant.\n  Beyond intentional manipulation, our analysis of 13 million labels from 18\ndifferent LLMs across 2361 realistic hypotheses shows that there is also a high\nrisk of accidental LLM hacking, even when following standard research\npractices. We find incorrect conclusions in approximately 31% of hypotheses for\nstate-of-the-art LLMs, and in half the hypotheses for smaller language models.\nWhile higher task performance and stronger general model capabilities reduce\nLLM hacking risk, even highly accurate models remain susceptible. The risk of\nLLM hacking decreases as effect sizes increase, indicating the need for more\nrigorous verification of LLM-based findings near significance thresholds. We\nanalyze 21 mitigation techniques and find that human annotations provide\ncrucial protection against false positives. Common regression estimator\ncorrection techniques can restore valid inference but trade off Type I vs. Type\nII errors.\n  We publish a list of practical recommendations to prevent LLM hacking.",
        "timestamp": "2025-10-17T07:11:35.122Z",
        "rating": "novote",
        "publishedDate": "2025-09-10T17:58:53Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 345,
        "object_id": "paper:arxiv.2509.08825",
        "created_at": "2025-10-17T07:11:35+00:00",
        "updated_at": "2025-10-17T07:11:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.03231": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.03231",
        "url": "https://arxiv.org/abs/2510.03231",
        "title": "Reward Models are Metrics in a Trench Coat",
        "authors": "Sebastian Gehrmann",
        "abstract": "The emergence of reinforcement learning in post-training of large language models has sparked significant interest in reward models. Reward models assess the quality of sampled model outputs to generate training signals. This task is also performed by evaluation metrics that monitor the performance of an AI model. We find that the two research areas are mostly separate, leading to redundant terminology and repeated pitfalls. Common challenges include susceptibility to spurious correlations, impact on downstream reward hacking, methods to improve data quality, and approaches to meta-evaluation. Our position paper argues that a closer collaboration between the fields can help overcome these issues. To that end, we show how metrics outperform reward models on specific tasks and provide an extensive survey of the two areas. Grounded in this survey, we point to multiple research topics in which closer alignment can improve reward models and metrics in areas such as preference elicitation methods, avoidance of spurious correlations and reward hacking, and calibration-aware meta-evaluation.",
        "timestamp": "2025-10-17T07:11:32.921Z",
        "rating": "novote",
        "publishedDate": "2025/10/03",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 344,
        "object_id": "paper:arxiv.2510.03231",
        "created_at": "2025-10-17T07:11:33+00:00",
        "updated_at": "2025-10-17T07:11:58+00:00",
        "version": 1
      }
    },
    "paper:openreview.f7GG1MbsSM": {
      "data": {
        "sourceId": "openreview",
        "paperId": "f7GG1MbsSM",
        "url": "https://openreview.net/forum?id=f7GG1MbsSM#discussion",
        "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
        "authors": "Zorik Gekhman, Eyal Ben-David, Hadas Orgad, Eran Ofek, Yonatan Belinkov, Idan Szpektor, Jonathan Herzig, Roi Reichart",
        "abstract": "This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a formal definition of knowledge, quantifying it for a given question as the fraction of correct-incorrect answer pairs where the correct one is ranked higher. This gives rise to external and internal knowledge, depending on the information used to score individual answer candidates: either the model\u2019s observable token-level probabilities or its intermediate computations. Hidden knowledge arises when internal knowledge exceeds external knowledge. We then present a case study, applying this framework to three popular open-weights LLMs in a closed-book QA setup. Our results indicate that: (1) LLMs consistently encode more factual knowledge internally than what they express externally, with an average gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a model can internally know an answer perfectly, yet fail to generate it even once, despite large-scale repeated sampling of 1,000 answers. This reveals fundamental limitations in the generation capabilities of LLMs, which (3) puts a practical constraint on scaling test-time compute via repeated answer sampling in closed-book QA: significant performance improvements remain inaccessible because some answers are practically never sampled, yet if they were, we would be guaranteed to rank them first.",
        "timestamp": "2025-10-17T07:11:32.838Z",
        "rating": "novote",
        "publishedDate": "08 Jul 2025",
        "tags": [
          "LLMs",
          "Knowledge"
        ],
        "doi": "",
        "journalName": "COLM 2025",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 343,
        "object_id": "paper:openreview.f7GG1MbsSM",
        "created_at": "2025-10-17T07:11:33+00:00",
        "updated_at": "2025-10-17T07:12:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.06278": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.06278",
        "url": "https://arxiv.org/pdf/2506.06278",
        "title": "Distillation Robustifies Unlearning",
        "authors": "Bruce W. Lee, Addie Foote, Alex Infanger, Leni Shor, Harish Kamath, Jacob Goldman-Wetzler, Bryce Woodworth, Alex Cloud, Alexander Matt Turner",
        "abstract": "Current LLM unlearning methods are not robust: they can be reverted easily\nwith a few steps of finetuning. This is true even for the idealized unlearning\nmethod of training to imitate an oracle model that was never exposed to\nunwanted information, suggesting that output-based finetuning is insufficient\nto achieve robust unlearning. In a similar vein, we find that training a\nrandomly initialized student to imitate an unlearned model transfers desired\nbehaviors while leaving undesired capabilities behind. In other words,\ndistillation robustifies unlearning. Building on this insight, we propose\nUnlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an\nunlearned model into a partially noised copy of itself. UNDO introduces a\ntunable tradeoff between compute cost and robustness, establishing a new Pareto\nfrontier on synthetic language and arithmetic tasks. At its strongest setting,\nUNDO matches the robustness of a model retrained from scratch with perfect data\nfiltering while using only 60-80% of the compute and requiring only 0.01% of\nthe pretraining data to be labeled. We also show that UNDO robustifies\nunlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP)\nbenchmark. Since distillation is widely used in practice, incorporating an\nunlearning step beforehand offers a convenient path to robust capability\nremoval.",
        "timestamp": "2025-10-17T07:11:32.796Z",
        "rating": "novote",
        "publishedDate": "2025-06-06T17:58:54Z",
        "tags": [
          "cs.LG",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 342,
        "object_id": "paper:arxiv.2506.06278",
        "created_at": "2025-10-17T07:11:33+00:00",
        "updated_at": "2025-10-17T07:11:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2508.17099": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.17099",
        "url": "https://arxiv.org/pdf/2508.17099",
        "title": "Challenges in Statistics: A Dozen Challenges in Causality and Causal\n  Inference",
        "authors": "Carlos Cinelli, Avi Feller, Guido Imbens, Edward Kennedy, Sara Magliacane, Jose Zubizarreta",
        "abstract": "Causality and causal inference have emerged as core research areas at the\ninterface of modern statistics and domains including biomedical sciences,\nsocial sciences, computer science, and beyond. The field's inherently\ninterdisciplinary nature -- particularly the central role of incorporating\ndomain knowledge -- creates a rich and varied set of statistical challenges.\nMuch progress has been made, especially in the last three decades, but there\nremain many open questions. Our goal in this discussion is to outline research\ndirections and open problems we view as particularly promising for future work.\nThroughout we emphasize that advancing causal research requires a wide range of\ncontributions, from novel theory and methodological innovations to improved\nsoftware tools and closer engagement with domain scientists and practitioners.",
        "timestamp": "2025-10-17T07:11:32.623Z",
        "rating": "novote",
        "publishedDate": "2025-08-23T18:00:49Z",
        "tags": [
          "stat.ME"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 341,
        "object_id": "paper:arxiv.2508.17099",
        "created_at": "2025-10-17T07:11:33+00:00",
        "updated_at": "2025-10-17T07:12:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2508.21038": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.21038",
        "url": "https://arxiv.org/pdf/2508.21038",
        "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
        "authors": "Orion Weller, Michael Boratko, Iftekhar Naim, Jinhyuk Lee",
        "abstract": "Vector embeddings have been tasked with an ever-increasing set of retrieval\ntasks over the years, with a nascent rise in using them for reasoning,\ninstruction-following, coding, and more. These new benchmarks push embeddings\nto work for any query and any notion of relevance that could be given. While\nprior works have pointed out theoretical limitations of vector embeddings,\nthere is a common assumption that these difficulties are exclusively due to\nunrealistic queries, and those that are not can be overcome with better\ntraining data and larger models. In this work, we demonstrate that we may\nencounter these theoretical limitations in realistic settings with extremely\nsimple queries. We connect known results in learning theory, showing that the\nnumber of top-k subsets of documents capable of being returned as the result of\nsome query is limited by the dimension of the embedding. We empirically show\nthat this holds true even if we restrict to k=2, and directly optimize on the\ntest set with free parameterized embeddings. We then create a realistic dataset\ncalled LIMIT that stress tests models based on these theoretical results, and\nobserve that even state-of-the-art models fail on this dataset despite the\nsimple nature of the task. Our work shows the limits of embedding models under\nthe existing single vector paradigm and calls for future research to develop\nmethods that can resolve this fundamental limitation.",
        "timestamp": "2025-10-17T07:11:32.294Z",
        "rating": "novote",
        "publishedDate": "2025-08-28T17:43:53Z",
        "tags": [
          "cs.IR",
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 340,
        "object_id": "paper:arxiv.2508.21038",
        "created_at": "2025-10-17T07:11:33+00:00",
        "updated_at": "2025-10-17T07:12:07+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2401.12973": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.12973",
        "url": "https://arxiv.org/pdf/2401.12973",
        "title": "In-Context Language Learning: Architectures and Algorithms",
        "authors": "Ekin Aky\u00fcrek, Bailin Wang, Yoon Kim, Jacob Andreas",
        "abstract": "Large-scale neural language models exhibit a remarkable capacity for\nin-context learning (ICL): they can infer novel functions from datasets\nprovided as input. Most of our current understanding of when and how ICL arises\ncomes from LMs trained on extremely simple learning problems like linear\nregression and associative recall. There remains a significant gap between\nthese model problems and the \"real\" ICL exhibited by LMs trained on large text\ncorpora, which involves not just retrieval and function approximation but\nfree-form generation of language and other structured outputs. In this paper,\nwe study ICL through the lens of a new family of model problems we term in\ncontext language learning (ICLL). In ICLL, LMs are presented with a set of\nstrings from a formal language, and must generate additional strings from the\nsame language. We focus on in-context learning of regular languages generated\nby random finite automata. We evaluate a diverse set of neural sequence models\n(including several RNNs, Transformers, and state-space model variants) on\nregular ICLL tasks, aiming to answer three questions: (1) Which model classes\nare empirically capable of ICLL? (2) What algorithmic solutions do successful\nmodels implement to perform ICLL? (3) What architectural changes can improve\nICLL in less performant models? We first show that Transformers significantly\noutperform neural sequence models with recurrent or convolutional\nrepresentations on ICLL tasks. Next, we provide evidence that their ability to\ndo so relies on specialized \"n-gram heads\" (higher-order variants of induction\nheads) that compute input-conditional next-token distributions. Finally, we\nshow that hard-wiring these heads into neural models improves performance not\njust on ICLL, but natural language modeling -- improving the perplexity of\n340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.",
        "timestamp": "2025-10-17T07:11:31.857Z",
        "rating": "novote",
        "publishedDate": "2024-01-23T18:59:21Z",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 339,
        "object_id": "paper:arxiv.2401.12973",
        "created_at": "2025-10-17T07:11:32+00:00",
        "updated_at": "2025-10-17T07:12:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.0804.2996": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "0804.2996",
        "url": "https://arxiv.org/pdf/0804.2996",
        "title": "The Epic Story of Maximum Likelihood",
        "authors": "Stephen M. Stigler",
        "abstract": "At a superficial level, the idea of maximum likelihood must be prehistoric:\nearly hunters and gatherers may not have used the words ``method of maximum\nlikelihood'' to describe their choice of where and how to hunt and gather, but\nit is hard to believe they would have been surprised if their method had been\ndescribed in those terms. It seems a simple, even unassailable idea: Who would\nrise to argue in favor of a method of minimum likelihood, or even mediocre\nlikelihood? And yet the mathematical history of the topic shows this ``simple\nidea'' is really anything but simple. Joseph Louis Lagrange, Daniel Bernoulli,\nLeonard Euler, Pierre Simon Laplace and Carl Friedrich Gauss are only some of\nthose who explored the topic, not always in ways we would sanction today. In\nthis article, that history is reviewed from back well before Fisher to the time\nof Lucien Le Cam's dissertation. In the process Fisher's unpublished 1930\ncharacterization of conditions for the consistency and efficiency of maximum\nlikelihood estimates is presented, and the mathematical basis of his three\nproofs discussed. In particular, Fisher's derivation of the information\ninequality is seen to be derived from his work on the analysis of variance, and\nhis later approach via estimating functions was derived from Euler's Relation\nfor homogeneous functions. The reaction to Fisher's work is reviewed, and some\nlessons drawn.",
        "timestamp": "2025-10-17T07:11:31.857Z",
        "rating": "novote",
        "publishedDate": "2008-04-18T11:11:13Z",
        "tags": [
          "stat.ME"
        ],
        "doi": "10.1214/07-STS249",
        "journalName": "Statistical Science 2007, Vol. 22, No. 4, 598-620",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 338,
        "object_id": "paper:arxiv.0804.2996",
        "created_at": "2025-10-17T07:11:32+00:00",
        "updated_at": "2025-10-17T07:11:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.04259": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.04259",
        "url": "https://arxiv.org/pdf/2509.04259",
        "title": "RL's Razor: Why Online Reinforcement Learning Forgets Less",
        "authors": "Idan Shenfeld, Jyothish Pari, Pulkit Agrawal",
        "abstract": "Comparison of fine-tuning models with reinforcement learning (RL) and\nsupervised fine-tuning (SFT) reveals that, despite similar performance at a new\ntask, RL preserves prior knowledge and capabilities significantly better. We\nfind that the degree of forgetting is determined by the distributional shift,\nmeasured as the KL-divergence between the fine-tuned and base policy evaluated\non the new task. Our analysis reveals that on-policy RL is implicitly biased\ntowards KL-minimal solutions among the many that solve the new task, whereas\nSFT can converge to distributions arbitrarily far from the base model. We\nvalidate these findings through experiments with large language models and\nrobotic foundation models and further provide theoretical justification for why\non-policy RL updates lead to a smaller KL change. We term this principle\n$\\textit{RL's Razor}$: among all ways to solve a new task, RL prefers those\nclosest in KL to the original model.",
        "timestamp": "2025-10-17T07:11:31.672Z",
        "rating": "novote",
        "publishedDate": "2025-09-04T14:38:08Z",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 337,
        "object_id": "paper:arxiv.2509.04259",
        "created_at": "2025-10-17T07:11:32+00:00",
        "updated_at": "2025-10-17T07:12:09+00:00",
        "version": 1
      }
    },
    "paper:openreview.AFMGbq39bQ": {
      "data": {
        "sourceId": "openreview",
        "paperId": "AFMGbq39bQ",
        "url": "https://openreview.net/pdf?id=AFMGbq39bQ",
        "title": "AFMGbq39bQ",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-10-17T07:11:31.355Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 336,
        "object_id": "paper:openreview.AFMGbq39bQ",
        "created_at": "2025-10-17T07:11:31+00:00",
        "updated_at": "2025-10-17T07:12:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.06485": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.06485",
        "url": "https://arxiv.org/pdf/2506.06485",
        "title": "Task Matters: Knowledge Requirements Shape LLM Responses to\n  Context-Memory Conflict",
        "authors": "Kaiser Sun, Fan Bai, Mark Dredze",
        "abstract": "Large Language Models require both contextual knowledge and parametric\nmemory, but these sources can disagree. Prior investigations on contextual\nquestion answering tasks report a preference toward parametric knowledge under\nconflict, yet they focus almost exclusively on tasks that should always rely on\nthe given passage, leaving open how this behavior manifests when tasks demand\ndifferent amounts and kinds of knowledge. We study this question with a\nmodel-agnostic diagnostic framework that (i) automatically detects\ndisagreements between a model's beliefs and a curated knowledge set, and (ii)\ninjects controlled conflicts into tasks. The resulting datasets span two\northogonal dimensions: task knowledge reliance and conflict plausibility.\nEvaluating representative open-source LLMs, we find that: (1) performance\ndegradation from conflict correlates with a task's knowledge reliance; (2)\nexplanatory rationales and simple reiteration both increase context\nreliance-helpful for context-only tasks but harmful when parametric knowledge\nshould dominate; (3) These behaviors raise concerns about the validity of\nmodel-based evaluation and underscore the need to account for knowledge\nconflict in the deployment of LLMs.",
        "timestamp": "2025-10-17T07:11:31.310Z",
        "rating": "novote",
        "publishedDate": "2025-06-06T19:20:23Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 334,
        "object_id": "paper:arxiv.2506.06485",
        "created_at": "2025-10-17T07:11:31+00:00",
        "updated_at": "2025-10-17T07:12:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.16189": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.16189",
        "url": "https://arxiv.org/pdf/2509.16189",
        "title": "Latent learning: episodic memory complements parametric learning by\n  enabling flexible reuse of experiences",
        "authors": "Andrew Kyle Lampinen, Martin Engelcke, Yuxuan Li, Arslan Chaudhry, James L. McClelland",
        "abstract": "When do machine learning systems fail to generalize, and what mechanisms\ncould improve their generalization? Here, we draw inspiration from cognitive\nscience to argue that one weakness of machine learning systems is their failure\nto exhibit latent learning -- learning information that is not relevant to the\ntask at hand, but that might be useful in a future task. We show how this\nperspective links failures ranging from the reversal curse in language modeling\nto new findings on agent-based navigation. We then highlight how cognitive\nscience points to episodic memory as a potential part of the solution to these\nissues. Correspondingly, we show that a system with an oracle retrieval\nmechanism can use learning experiences more flexibly to generalize better\nacross many of these challenges. We also identify some of the essential\ncomponents for effectively using retrieval, including the importance of\nwithin-example in-context learning for acquiring the ability to use information\nacross retrieved examples. In summary, our results illustrate one possible\ncontributor to the relative data inefficiency of current machine learning\nsystems compared to natural intelligence, and help to understand how retrieval\nmethods can complement parametric learning to improve generalization.",
        "timestamp": "2025-10-17T07:11:31.366Z",
        "rating": "novote",
        "publishedDate": "2025-09-19T17:49:25Z",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 335,
        "object_id": "paper:arxiv.2509.16189",
        "created_at": "2025-10-17T07:11:31+00:00",
        "updated_at": "2025-10-17T07:12:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.01844": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.01844",
        "url": "https://arxiv.org/pdf/2507.01844",
        "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them",
        "authors": "Arthur Wuhrmann, Anastasiia Kucherenko, Andrei Kucharavy",
        "abstract": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.",
        "timestamp": "2025-10-17T07:11:31.406Z",
        "rating": "novote",
        "publishedDate": "2025-07-02T15:58:51Z",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 333,
        "object_id": "paper:arxiv.2507.01844",
        "created_at": "2025-10-17T07:11:31+00:00",
        "updated_at": "2025-10-17T07:11:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.00239": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.00239",
        "url": "https://arxiv.org/pdf/2507.00239",
        "title": "Linearly Decoding Refused Knowledge in Aligned Language Models",
        "authors": "Aryan Shrivastava, Ari Holtzman",
        "abstract": "Most commonly used language models (LMs) are instruction-tuned and aligned\nusing a combination of fine-tuning and reinforcement learning, causing them to\nrefuse users requests deemed harmful by the model. However, jailbreak prompts\ncan often bypass these refusal mechanisms and elicit harmful responses. In this\nwork, we study the extent to which information accessed via jailbreak prompts\nis decodable using linear probes trained on LM hidden states. We show that a\ngreat deal of initially refused information is linearly decodable. For example,\nacross models, the response of a jailbroken LM for the average IQ of a country\ncan be predicted by a linear probe with Pearson correlations exceeding $0.8$.\nSurprisingly, we find that probes trained on base models (which do not refuse)\nsometimes transfer to their instruction-tuned versions and are capable of\nrevealing information that jailbreaks decode generatively, suggesting that the\ninternal representations of many refused properties persist from base LMs\nthrough instruction-tuning. Importantly, we show that this information is not\nmerely \"leftover\" in instruction-tuned models, but is actively used by them: we\nfind that probe-predicted values correlate with LM generated pairwise\ncomparisons, indicating that the information decoded by our probes align with\nsuppressed generative behavior that may be expressed more subtly in other\ndownstream tasks. Overall, our results suggest that instruction-tuning does not\nwholly eliminate or even relocate harmful information in representation\nspace-they merely suppress its direct expression, leaving it both linearly\naccessible and indirectly influential in downstream behavior.",
        "timestamp": "2025-10-17T07:11:31.405Z",
        "rating": "novote",
        "publishedDate": "2025-06-30T20:13:49Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 332,
        "object_id": "paper:arxiv.2507.00239",
        "created_at": "2025-10-17T07:11:31+00:00",
        "updated_at": "2025-10-17T07:12:03+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.20481": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.20481",
        "url": "https://arxiv.org/pdf/2506.20481",
        "title": "Counterfactual Influence as a Distributional Quantity",
        "authors": "Matthieu Meeus, Igor Shilov, Georgios Kaissis, Yves-Alexandre de Montjoye",
        "abstract": "Machine learning models are known to memorize samples from their training\ndata, raising concerns around privacy and generalization. Counterfactual\nself-influence is a popular metric to study memorization, quantifying how the\nmodel's prediction for a sample changes depending on the sample's inclusion in\nthe training dataset. However, recent work has shown memorization to be\naffected by factors beyond self-influence, with other training samples, in\nparticular (near-)duplicates, having a large impact. We here study memorization\ntreating counterfactual influence as a distributional quantity, taking into\naccount how all training samples influence how a sample is memorized. For a\nsmall language model, we compute the full influence distribution of training\nsamples on each other and analyze its properties. We find that solely looking\nat self-influence can severely underestimate tangible risks associated with\nmemorization: the presence of (near-)duplicates seriously reduces\nself-influence, while we find these samples to be (near-)extractable. We\nobserve similar patterns for image classification, where simply looking at the\ninfluence distributions reveals the presence of near-duplicates in CIFAR-10.\nOur findings highlight that memorization stems from complex interactions across\ntraining data and is better captured by the full influence distribution than by\nself-influence alone.",
        "timestamp": "2025-10-17T07:11:31.360Z",
        "rating": "novote",
        "publishedDate": "2025-06-25T14:25:11Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL",
          "cs.CR"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 331,
        "object_id": "paper:arxiv.2506.20481",
        "created_at": "2025-10-17T07:11:31+00:00",
        "updated_at": "2025-10-17T07:11:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.05209": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.05209",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T07:57:37.025Z",
            "data": {
              "session_id": "session_1760687856457_qwhxwod",
              "source_id": "arxiv",
              "paper_id": "2506.05209",
              "start_time": "2025-10-17T07:57:08.441Z",
              "end_time": "2025-10-17T07:57:36.457Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T08:01:01.859Z",
            "data": {
              "session_id": "session_1760688061847_ruc4swf",
              "source_id": "arxiv",
              "paper_id": "2506.05209",
              "start_time": "2025-10-17T08:00:53.024Z",
              "end_time": "2025-10-17T08:01:01.847Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 354,
        "object_id": "interactions:arxiv.2506.05209",
        "created_at": "2025-10-17T07:57:37+00:00",
        "updated_at": "2025-10-17T08:01:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.05209": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.05209",
        "url": "https://arxiv.org/pdf/2506.05209?",
        "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text",
        "authors": "Nikhil Kandpal, Brian Lester, Colin Raffel, Sebastian Majstorovic, Stella Biderman, Baber Abbasi, Luca Soldaini, Enrico Shippole, A. Feder Cooper, Aviya Skowron, John Kirchenbauer, Shayne Longpre, Lintang Sutawika, Alon Albalak, Zhenlin Xu, Guilherme Penedo, Loubna Ben Allal, Elie Bakouch, John David Pressman, Honglu Fan, Dashiell Stander, Guangyu Song, Aaron Gokaslan, Tom Goldstein, Brian R. Bartoldson, Bhavya Kailkhura, Tyler Murray",
        "abstract": "Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models.",
        "timestamp": "2025-10-17T07:57:08.752Z",
        "rating": "novote",
        "publishedDate": "2025-06-05T16:21:30Z",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 353,
        "object_id": "paper:arxiv.2506.05209",
        "created_at": "2025-10-17T07:57:09+00:00",
        "updated_at": "2025-10-17T07:57:44+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.01912": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.01912",
        "url": "https://arxiv.org/pdf/2506.01912",
        "title": "Unconditional CNN denoisers contain sparse semantic representation of\n  images",
        "authors": "Zahra Kadkhodaie, St\u00e9phane Mallat, Eero Simoncelli",
        "abstract": "Generative diffusion models learn probability densities over diverse image\ndatasets by estimating the score with a neural network trained to remove noise.\nDespite their remarkable success in generating high-quality images, the\ninternal mechanisms of the underlying score networks are not well understood.\nHere, we examine the image representation that arises from score estimation in\na {fully-convolutional unconditional UNet}. We show that the middle block of\nthe UNet decomposes individual images into sparse subsets of active channels,\nand that the vector of spatial averages of these channels can provide a\nnonlinear representation of the underlying clean images. Euclidean distances in\nthis representation space are semantically meaningful, even though no\nconditioning information is provided during training. We develop a novel\nalgorithm for stochastic reconstruction of images conditioned on this\nrepresentation: The synthesis using the unconditional model is \"self-guided\" by\nthe representation extracted from that very same model. For a given\nrepresentation, the common patterns in the set of reconstructed samples reveal\nthe features captured in the middle block of the UNet. Together, these results\nshow, for the first time, that a measure of semantic similarity emerges,\nunsupervised, solely from the denoising objective.",
        "timestamp": "2025-10-17T08:05:34.016Z",
        "rating": "novote",
        "publishedDate": "2025-06-02T17:33:34Z",
        "tags": [
          "cs.CV"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 355,
        "object_id": "paper:arxiv.2506.01912",
        "created_at": "2025-10-17T08:05:34+00:00",
        "updated_at": "2025-10-17T08:05:56+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.01912": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.01912",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T08:06:41.496Z",
            "data": {
              "session_id": "session_1760688401427_pa3mstv",
              "source_id": "arxiv",
              "paper_id": "2506.01912",
              "start_time": "2025-10-17T08:06:00.727Z",
              "end_time": "2025-10-17T08:06:41.427Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 1,
              "total_elapsed_seconds": 41
            }
          }
        ]
      },
      "meta": {
        "issue_number": 356,
        "object_id": "interactions:arxiv.2506.01912",
        "created_at": "2025-10-17T08:06:42+00:00",
        "updated_at": "2025-10-17T08:07:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.02534": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.02534",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T15:26:16.501Z",
            "data": {
              "session_id": "session_1760714776435_vgyg8ro",
              "source_id": "arxiv",
              "paper_id": "2509.02534",
              "start_time": "2025-10-17T15:25:44.287Z",
              "end_time": "2025-10-17T15:26:16.435Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "issue_number": 357,
        "object_id": "interactions:arxiv.2509.02534",
        "created_at": "2025-10-17T15:26:17+00:00",
        "updated_at": "2025-10-17T15:26:38+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2501.11866": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.11866",
        "interactions": []
      },
      "meta": {
        "issue_number": 359,
        "object_id": "interactions:arxiv.2501.11866",
        "created_at": "2025-10-17T16:48:03+00:00",
        "updated_at": "2025-10-17T16:48:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.11866": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.11866",
        "url": "https://arxiv.org/abs/2501.11866",
        "title": "Evaluating multiple models using labeled and unlabeled data",
        "authors": "Divya Shanmugam, Shuvom Sadhuka, Manish Raghavan, John Guttag, Bonnie Berger, Emma Pierson",
        "abstract": "It remains difficult to evaluate machine learning classifiers in the absence of a large, labeled dataset. While labeled data can be prohibitively expensive or impossible to obtain, unlabeled data is plentiful. Here, we introduce Semi-Supervised Model Evaluation (SSME), a method that uses both labeled and unlabeled data to evaluate machine learning classifiers. SSME is the first evaluation method to take advantage of the fact that: (i) there are frequently multiple classifiers for the same task, (ii) continuous classifier scores are often available for all classes, and (iii) unlabeled data is often far more plentiful than labeled data. The key idea is to use a semi-supervised mixture model to estimate the joint distribution of ground truth labels and classifier predictions. We can then use this model to estimate any metric that is a function of classifier scores and ground truth labels (e.g., accuracy or expected calibration error). We present experiments in four domains where obtaining large labeled datasets is often impractical: (1) healthcare, (2) content moderation, (3) molecular property prediction, and (4) image annotation. Our results demonstrate that SSME estimates performance more accurately than do competing methods, reducing error by 5.1x relative to using labeled data alone and 2.4x relative to the next best competing method. SSME also improves accuracy when evaluating performance across subsets of the test distribution (e.g., specific demographic subgroups) and when evaluating the performance of language models.",
        "timestamp": "2025-10-17T16:47:18.440Z",
        "rating": "novote",
        "publishedDate": "2025/01/21",
        "tags": [
          "Machine Learning (cs.LG)",
          "Computers and Society (cs.CY)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 358,
        "object_id": "paper:arxiv.2501.11866",
        "created_at": "2025-10-17T16:47:18+00:00",
        "updated_at": "2025-10-17T16:47:45+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2508.17099": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.17099",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T16:48:24.480Z",
            "data": {
              "session_id": "session_1760719704229_8c9hvg9",
              "source_id": "arxiv",
              "paper_id": "2508.17099",
              "start_time": "2025-10-17T16:48:07.076Z",
              "end_time": "2025-10-17T16:48:24.229Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "issue_number": 360,
        "object_id": "interactions:arxiv.2508.17099",
        "created_at": "2025-10-17T16:48:25+00:00",
        "updated_at": "2025-10-17T16:48:58+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.06278": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.06278",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T16:50:29.950Z",
            "data": {
              "session_id": "session_1760719829841_lcyxsfv",
              "source_id": "arxiv",
              "paper_id": "2506.06278",
              "start_time": "2025-10-17T16:50:20.831Z",
              "end_time": "2025-10-17T16:50:29.841Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 361,
        "object_id": "interactions:arxiv.2506.06278",
        "created_at": "2025-10-17T16:50:16+00:00",
        "updated_at": "2025-10-17T16:50:47+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2412.05265": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.05265",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:03:06.568Z",
            "data": {
              "session_id": "session_1760720586487_wgubarn",
              "source_id": "arxiv",
              "paper_id": "2412.05265",
              "start_time": "2025-10-17T17:02:37.421Z",
              "end_time": "2025-10-17T17:03:06.487Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          }
        ]
      },
      "meta": {
        "issue_number": 363,
        "object_id": "interactions:arxiv.2412.05265",
        "created_at": "2025-10-17T17:02:38+00:00",
        "updated_at": "2025-10-17T17:03:13+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2508.21038": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.21038",
        "interactions": []
      },
      "meta": {
        "issue_number": 362,
        "object_id": "interactions:arxiv.2508.21038",
        "created_at": "2025-10-17T16:52:31+00:00",
        "updated_at": "2025-10-17T16:52:33+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.10209": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.10209",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:05:48.598Z",
            "data": {
              "session_id": "session_1760720748534_94xx7sc",
              "source_id": "arxiv",
              "paper_id": "2406.10209",
              "start_time": "2025-10-17T17:05:39.830Z",
              "end_time": "2025-10-17T17:05:48.534Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 364,
        "object_id": "interactions:arxiv.2406.10209",
        "created_at": "2025-10-17T17:05:41+00:00",
        "updated_at": "2025-10-17T17:06:16+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.01844": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.01844",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:08:17.517Z",
            "data": {
              "session_id": "session_1760720897181_v5jpxag",
              "source_id": "arxiv",
              "paper_id": "2507.01844",
              "start_time": "2025-10-17T17:08:09.495Z",
              "end_time": "2025-10-17T17:08:17.181Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 366,
        "object_id": "interactions:arxiv.2507.01844",
        "created_at": "2025-10-17T17:08:18+00:00",
        "updated_at": "2025-10-17T17:08:54+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.06268": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.06268",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:08:03.196Z",
            "data": {
              "session_id": "session_1760720882548_9dxqjhv",
              "source_id": "arxiv",
              "paper_id": "2507.06268",
              "start_time": "2025-10-17T17:06:05.052Z",
              "end_time": "2025-10-17T17:08:02.548Z",
              "heartbeat_count": 23,
              "duration_seconds": 115,
              "idle_seconds": 2,
              "total_elapsed_seconds": 117
            }
          }
        ]
      },
      "meta": {
        "issue_number": 365,
        "object_id": "interactions:arxiv.2507.06268",
        "created_at": "2025-10-17T17:08:04+00:00",
        "updated_at": "2025-10-17T17:08:36+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.20879": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.20879",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:12:05.016Z",
            "data": {
              "session_id": "session_1760721124144_5rqhiil",
              "source_id": "arxiv",
              "paper_id": "2504.20879",
              "start_time": "2025-10-17T17:10:54.981Z",
              "end_time": "2025-10-17T17:12:04.144Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 4,
              "total_elapsed_seconds": 69
            }
          }
        ]
      },
      "meta": {
        "issue_number": 367,
        "object_id": "interactions:arxiv.2504.20879",
        "created_at": "2025-10-17T17:12:05+00:00",
        "updated_at": "2025-10-17T17:12:35+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2411.08019": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.08019",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:23:51.578Z",
            "data": {
              "session_id": "session_1760721831560_4vsm8nu",
              "source_id": "arxiv",
              "paper_id": "2411.08019",
              "start_time": "2025-10-17T17:23:45.576Z",
              "end_time": "2025-10-17T17:23:51.560Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:25:54.667Z",
            "data": {
              "session_id": "session_1760721953991_bqy7asq",
              "source_id": "arxiv",
              "paper_id": "2411.08019",
              "start_time": "2025-10-17T17:23:56.100Z",
              "end_time": "2025-10-17T17:25:53.991Z",
              "heartbeat_count": 23,
              "duration_seconds": 115,
              "idle_seconds": 3,
              "total_elapsed_seconds": 118
            }
          }
        ]
      },
      "meta": {
        "issue_number": 368,
        "object_id": "interactions:arxiv.2411.08019",
        "created_at": "2025-10-17T17:23:52+00:00",
        "updated_at": "2025-10-17T17:26:19+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.14086": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.14086",
        "url": "https://arxiv.org/abs/2510.14086",
        "title": "Every Language Model Has a Forgery-Resistant Signature",
        "authors": "Matthew Finlayson, Xiang Ren, Swabha Swayamdipta",
        "abstract": "The ubiquity of closed-weight language models with public-facing APIs has generated interest in forensic methods, both for extracting hidden model details (e.g., parameters) and for identifying models by their outputs. One successful approach to these goals has been to exploit the geometric constraints imposed by the language model architecture and parameters. In this work, we show that a lesser-known geometric constraint--namely, that language model outputs lie on the surface of a high-dimensional ellipse--functions as a signature for the model and can be used to identify the source model of a given output. This ellipse signature has unique properties that distinguish it from existing model-output association methods like language model fingerprints. In particular, the signature is hard to forge: without direct access to model parameters, it is practically infeasible to produce log-probabilities (logprobs) on the ellipse. Secondly, the signature is naturally occurring, since all language models have these elliptical constraints. Thirdly, the signature is self-contained, in that it is detectable without access to the model inputs or the full weights. Finally, the signature is compact and redundant, as it is independently detectable in each logprob output from the model. We evaluate a novel technique for extracting the ellipse from small models and discuss the practical hurdles that make it infeasible for production-scale models. Finally, we use ellipse signatures to propose a protocol for language model output verification, analogous to cryptographic symmetric-key message authentication systems.",
        "timestamp": "2025-10-17T19:29:51.130Z",
        "rating": "novote",
        "publishedDate": "2025/10/15",
        "tags": [
          "Cryptography and Security (cs.CR)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 369,
        "object_id": "paper:arxiv.2510.14086",
        "created_at": "2025-10-17T19:29:51+00:00",
        "updated_at": "2025-10-17T19:30:20+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.14086": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.14086",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T19:31:04.481Z",
            "data": {
              "session_id": "session_1760729463802_itw5q7i",
              "source_id": "arxiv",
              "paper_id": "2510.14086",
              "start_time": "2025-10-17T19:29:53.795Z",
              "end_time": "2025-10-17T19:31:03.802Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 15,
              "total_elapsed_seconds": 70
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T09:02:21.582Z",
            "data": {
              "session_id": "session_1763888541572_543rgis",
              "source_id": "arxiv",
              "paper_id": "2510.14086",
              "start_time": "2025-11-23T09:02:11.374Z",
              "end_time": "2025-11-23T09:02:21.572Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "issue_number": 370,
        "object_id": "interactions:arxiv.2510.14086",
        "created_at": "2025-10-17T19:31:05+00:00",
        "updated_at": "2025-11-23T09:02:38+00:00",
        "version": 1
      }
    },
    "interactions:openreview.u3X9yFP40t": {
      "data": {
        "sourceId": "openreview",
        "paperId": "u3X9yFP40t",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:04:21.580Z",
            "data": {
              "session_id": "session_1760771060683_rrwyvfj",
              "source_id": "openreview",
              "paper_id": "u3X9yFP40t",
              "start_time": "2025-10-18T07:03:30.674Z",
              "end_time": "2025-10-18T07:04:20.683Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 5,
              "total_elapsed_seconds": 50
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:11:48.191Z",
            "data": {
              "session_id": "session_1760771508154_cbfcwrv",
              "source_id": "openreview",
              "paper_id": "u3X9yFP40t",
              "start_time": "2025-10-18T07:11:33.886Z",
              "end_time": "2025-10-18T07:11:48.154Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-26T17:29:27.961Z",
            "data": {
              "session_id": "session_1761499767597_nq6xqp0",
              "source_id": "openreview",
              "paper_id": "u3X9yFP40t",
              "start_time": "2025-10-26T17:29:07.556Z",
              "end_time": "2025-10-26T17:29:27.596Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-08T09:54:38.704Z",
            "data": {
              "session_id": "session_1762595678443_d4rhrpb",
              "source_id": "openreview",
              "paper_id": "u3X9yFP40t",
              "start_time": "2025-11-08T09:53:49.171Z",
              "end_time": "2025-11-08T09:54:38.442Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 4,
              "total_elapsed_seconds": 49
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T07:13:43.060Z",
            "data": {
              "session_id": "session_1762931622643_5ivmk83",
              "source_id": "openreview",
              "paper_id": "u3X9yFP40t",
              "start_time": "2025-11-12T07:13:05.856Z",
              "end_time": "2025-11-12T07:13:42.643Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-15T08:19:05.182Z",
            "data": {
              "session_id": "session_1763194744650_eywoc2t",
              "source_id": "openreview",
              "paper_id": "u3X9yFP40t",
              "start_time": "2025-11-15T08:17:30.441Z",
              "end_time": "2025-11-15T08:19:04.650Z",
              "heartbeat_count": 18,
              "duration_seconds": 90,
              "idle_seconds": 4,
              "total_elapsed_seconds": 94
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T05:49:42.879Z",
            "data": {
              "session_id": "session_1763876982578_cxbwulv",
              "source_id": "openreview",
              "paper_id": "u3X9yFP40t",
              "start_time": "2025-11-23T05:49:08.453Z",
              "end_time": "2025-11-23T05:49:42.578Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-24T19:31:35.251Z",
            "data": {
              "session_id": "session_1764012694564_3dfs3u6",
              "source_id": "openreview",
              "paper_id": "u3X9yFP40t",
              "start_time": "2025-11-24T19:30:10.887Z",
              "end_time": "2025-11-24T19:31:34.564Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 4,
              "total_elapsed_seconds": 84
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T09:12:05.871Z",
            "data": {
              "session_id": "session_1764061924981_ovb9cyi",
              "source_id": "openreview",
              "paper_id": "u3X9yFP40t",
              "start_time": "2025-11-25T09:11:28.418Z",
              "end_time": "2025-11-25T09:12:04.981Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          }
        ]
      },
      "meta": {
        "issue_number": 372,
        "object_id": "interactions:openreview.u3X9yFP40t",
        "created_at": "2025-10-18T07:02:48+00:00",
        "updated_at": "2025-11-25T09:12:33+00:00",
        "version": 1
      }
    },
    "paper:openreview.u3X9yFP40t": {
      "data": {
        "sourceId": "openreview",
        "paperId": "u3X9yFP40t",
        "url": "https://openreview.net/forum?id=u3X9yFP40t&referrer=%5BArea%20Chairs%20Console%5D(%2Fgroup%3Fid%3Daclweb.org%2FACL%2FARR%2F2025%2FOctober%2FArea_Chairs%23assigned-submissions)",
        "title": "Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders",
        "authors": "",
        "abstract": "Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layer in the model. By intervening on these features, we can suppress one language and substitute another in the model\u2019s outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.",
        "timestamp": "2025-10-18T07:02:39.703Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Interpretability",
          "Cross-LayerTranscoders",
          "Multilingualism and Cross-Lingual NLP"
        ],
        "doi": "",
        "journalName": "ACL ARR 2025 October Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 371,
        "object_id": "paper:openreview.u3X9yFP40t",
        "created_at": "2025-10-18T07:02:39+00:00",
        "updated_at": "2025-10-18T07:03:08+00:00",
        "version": 1
      }
    },
    "paper:openreview.8T5fcnGlP1": {
      "data": {
        "sourceId": "openreview",
        "paperId": "8T5fcnGlP1",
        "url": "https://openreview.net/forum?id=8T5fcnGlP1&referrer=%5BArea%20Chairs%20Console%5D(%2Fgroup%3Fid%3Daclweb.org%2FACL%2FARR%2F2025%2FOctober%2FArea_Chairs%23assigned-submissions)",
        "title": "Revisiting Non-Verbatim Memorization in Large Language Models: The Role of Entity Surface Forms",
        "authors": "",
        "abstract": "Understanding what kinds of knowledge large language models (LLMs) memorize is essential for evaluating their reliability and limitations. \nPrevious studies have analyzed such non-verbatim memorization using entity-based question answering (QA) datasets, \nbut these datasets typically assume a single canonical surface form per entity, making it unclear whether LLMs memorize entities as conceptual units or only through specific surface expressions. \nTo bridge this gap, we introduce RedirectQA, an entity-based QA dataset constructed from Wikipedia redirect information that provides multiple surface forms for each entity, categorized into some types.\nUsing this dataset, we systematically examine how LLM memorization differs across surface types, frequencies, and entity\u2013surface linkages. \nOur experiments reveal that model outputs are often inconsistent across different surface forms of the same entity, \nand that memorization is more robust for minor variations, e.g., spelling, than for aliases or major rephrasings. \nFurther analysis of entity- and surface-level frequencies suggests that entity frequency contributes more strongly to memorization than surface frequency, \nimplying that LLMs may not memorize surface forms independently but rather link them through an underlying entity-level representation.\nThese findings highlight the necessity of considering surface-form diversity when evaluating non-verbatim memorization in LLMs.",
        "timestamp": "2025-10-18T07:08:21.783Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "probing",
          "robustness"
        ],
        "doi": "",
        "journalName": "ACL ARR 2025 October Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 376,
        "object_id": "paper:openreview.8T5fcnGlP1",
        "created_at": "2025-10-18T07:08:22+00:00",
        "updated_at": "2025-10-18T07:08:41+00:00",
        "version": 1
      }
    },
    "paper:openreview.sYF69As4M5": {
      "data": {
        "sourceId": "openreview",
        "paperId": "sYF69As4M5",
        "url": "https://openreview.net/forum?id=sYF69As4M5&referrer=%5BArea%20Chairs%20Console%5D(%2Fgroup%3Fid%3Daclweb.org%2FACL%2FARR%2F2025%2FOctober%2FArea_Chairs%23assigned-submissions)",
        "title": "Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs",
        "authors": "",
        "abstract": "We introduce the concept of protoknowledge to formalize and measure how Knowledge Graphs (KGs) are internalized during pretraining and reused at inference by Large Language Models (LLMs). LLMs are known to memorize vast amounts of token sequences and a central open question is how this memorization can serve as reusable knowledge through implicit abstraction and generalization. We categorize \n protoknowledge into lexical, hierarchical, and topological forms, reflecting different levels of abstraction over KGs. We measure these forms through Knowledge Activation Tasks (KATs), analyzing general properties such as semantic bias.\nWe then examine how protoknowledge affects Text-to-SPARQL, a task requiring conformity to the target KG\u2019s formal structure. To this end, we adopt a novel analysis framework that assesses whether model predictions align with the successful activation of the relevant protoknowledge for each query.\n\nWe do not frame this phenomenon as data contamination alone: rather, protoknowledge provides a measurable signal of how LLMs internalize structured information during pretraining and reuse it in downstream tasks. This perspective offers a more nuanced view of semantic-level data contamination and supplies an effective strategy for interpreting the behaviour of Closed-Pretraining models.",
        "timestamp": "2025-10-18T07:07:51.783Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Knowledge Graphs",
          "Memorization",
          "Text-To-SPARQL",
          "Data Contamination"
        ],
        "doi": "",
        "journalName": "ACL ARR 2025 October Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 375,
        "object_id": "paper:openreview.sYF69As4M5",
        "created_at": "2025-10-18T07:07:52+00:00",
        "updated_at": "2025-10-18T07:08:15+00:00",
        "version": 1
      }
    },
    "interactions:openreview.RyEe7oZgz9": {
      "data": {
        "sourceId": "openreview",
        "paperId": "RyEe7oZgz9",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:07:46.603Z",
            "data": {
              "session_id": "session_1760771266301_kihv7kx",
              "source_id": "openreview",
              "paper_id": "RyEe7oZgz9",
              "start_time": "2025-10-18T07:07:22.215Z",
              "end_time": "2025-10-18T07:07:46.301Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:25:02.060Z",
            "data": {
              "session_id": "session_1760772302045_qar23m6",
              "source_id": "openreview",
              "paper_id": "RyEe7oZgz9",
              "start_time": "2025-10-18T07:24:32.615Z",
              "end_time": "2025-10-18T07:25:02.045Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T07:05:16.606Z",
            "data": {
              "session_id": "session_1762931116579_8bhco7i",
              "source_id": "openreview",
              "paper_id": "RyEe7oZgz9",
              "start_time": "2025-11-12T07:05:07.642Z",
              "end_time": "2025-11-12T07:05:16.579Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T07:10:30.614Z",
            "data": {
              "session_id": "session_1762931430609_dv6whc6",
              "source_id": "openreview",
              "paper_id": "RyEe7oZgz9",
              "start_time": "2025-11-12T07:10:24.830Z",
              "end_time": "2025-11-12T07:10:30.609Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T07:13:02.962Z",
            "data": {
              "session_id": "session_1762931582332_gi43a3u",
              "source_id": "openreview",
              "paper_id": "RyEe7oZgz9",
              "start_time": "2025-11-12T07:10:35.101Z",
              "end_time": "2025-11-12T07:13:02.332Z",
              "heartbeat_count": 29,
              "duration_seconds": 145,
              "idle_seconds": 2,
              "total_elapsed_seconds": 147
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T07:30:43.076Z",
            "data": {
              "session_id": "session_1762932643066_bduxey6",
              "source_id": "openreview",
              "paper_id": "RyEe7oZgz9",
              "start_time": "2025-11-12T07:30:27.981Z",
              "end_time": "2025-11-12T07:30:43.066Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T06:34:49.709Z",
            "data": {
              "session_id": "session_1763015689432_r2x5dj2",
              "source_id": "openreview",
              "paper_id": "RyEe7oZgz9",
              "start_time": "2025-11-13T06:34:25.367Z",
              "end_time": "2025-11-13T06:34:49.432Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T07:38:01.565Z",
            "data": {
              "session_id": "session_1763710681471_7xwg1pq",
              "source_id": "openreview",
              "paper_id": "RyEe7oZgz9",
              "start_time": "2025-11-21T07:37:21.166Z",
              "end_time": "2025-11-21T07:38:01.471Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 0,
              "total_elapsed_seconds": 40
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T05:50:22.748Z",
            "data": {
              "session_id": "session_1763877022449_c7085u6",
              "source_id": "openreview",
              "paper_id": "RyEe7oZgz9",
              "start_time": "2025-11-23T05:49:44.307Z",
              "end_time": "2025-11-23T05:50:22.449Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 3,
              "total_elapsed_seconds": 38
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-24T19:32:47.021Z",
            "data": {
              "session_id": "session_1764012766355_5nn3v6u",
              "source_id": "openreview",
              "paper_id": "RyEe7oZgz9",
              "start_time": "2025-11-24T19:32:15.252Z",
              "end_time": "2025-11-24T19:32:46.355Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 1,
              "total_elapsed_seconds": 31
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T09:12:38.609Z",
            "data": {
              "session_id": "session_1764061958235_xr1202q",
              "source_id": "openreview",
              "paper_id": "RyEe7oZgz9",
              "start_time": "2025-11-25T09:12:18.082Z",
              "end_time": "2025-11-25T09:12:38.235Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "issue_number": 374,
        "object_id": "interactions:openreview.RyEe7oZgz9",
        "created_at": "2025-10-18T07:07:47+00:00",
        "updated_at": "2025-11-25T09:13:07+00:00",
        "version": 1
      }
    },
    "paper:openreview.RyEe7oZgz9": {
      "data": {
        "sourceId": "openreview",
        "paperId": "RyEe7oZgz9",
        "url": "https://openreview.net/pdf?id=RyEe7oZgz9",
        "title": "RyEe7oZgz9",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-10-18T07:07:20.941Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 373,
        "object_id": "paper:openreview.RyEe7oZgz9",
        "created_at": "2025-10-18T07:07:21+00:00",
        "updated_at": "2025-10-18T07:07:39+00:00",
        "version": 1
      }
    },
    "interactions:openreview.8T5fcnGlP1": {
      "data": {
        "sourceId": "openreview",
        "paperId": "8T5fcnGlP1",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:25:43.819Z",
            "data": {
              "session_id": "session_1760772343813_fs81lja",
              "source_id": "openreview",
              "paper_id": "8T5fcnGlP1",
              "start_time": "2025-10-18T07:25:14.644Z",
              "end_time": "2025-10-18T07:25:43.813Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T07:19:11.548Z",
            "data": {
              "session_id": "session_1762931951225_whzb2gt",
              "source_id": "openreview",
              "paper_id": "8T5fcnGlP1",
              "start_time": "2025-11-12T07:18:24.822Z",
              "end_time": "2025-11-12T07:19:11.225Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 1,
              "total_elapsed_seconds": 46
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T07:21:52.917Z",
            "data": {
              "session_id": "session_1762932112880_bj9o2kw",
              "source_id": "openreview",
              "paper_id": "8T5fcnGlP1",
              "start_time": "2025-11-12T07:20:55.526Z",
              "end_time": "2025-11-12T07:21:52.880Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 2,
              "total_elapsed_seconds": 57
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T05:51:35.554Z",
            "data": {
              "session_id": "session_1763877095199_mjojvjm",
              "source_id": "openreview",
              "paper_id": "8T5fcnGlP1",
              "start_time": "2025-11-23T05:50:46.388Z",
              "end_time": "2025-11-23T05:51:35.199Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 4,
              "total_elapsed_seconds": 49
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-24T19:33:25.714Z",
            "data": {
              "session_id": "session_1764012805638_cmnhej9",
              "source_id": "openreview",
              "paper_id": "8T5fcnGlP1",
              "start_time": "2025-11-24T19:32:46.406Z",
              "end_time": "2025-11-24T19:33:25.638Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 4,
              "total_elapsed_seconds": 39
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T09:14:40.999Z",
            "data": {
              "session_id": "session_1764062080995_8jmr04p",
              "source_id": "openreview",
              "paper_id": "8T5fcnGlP1",
              "start_time": "2025-11-25T09:14:25.962Z",
              "end_time": "2025-11-25T09:14:40.995Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 377,
        "object_id": "interactions:openreview.8T5fcnGlP1",
        "created_at": "2025-10-18T07:10:26+00:00",
        "updated_at": "2025-11-25T09:15:12+00:00",
        "version": 1
      }
    },
    "paper:openreview.Ou2EtkDRes": {
      "data": {
        "sourceId": "openreview",
        "paperId": "Ou2EtkDRes",
        "url": "https://openreview.net/forum?id=Ou2EtkDRes",
        "title": "Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs",
        "authors": "",
        "abstract": "In this paper, we introduce the concept of protoknowledge to formalize and measure when sequences of tokens encoding Knowledge Graphs become are internalized and utilized during inference in large language models (LLMs). Indeed, LLMs have demonstrated the ability to memorize vast amounts of token sequences during pretraining, and a central open question is how LLMs leverage this memorization as reusable knowledge through generalization. We then categorize protoknowledge into lexical, hierarchical, and topological forms, varying on the type of knowledge that needs to be activated. We measure protoknowledge through Knowledge Activation Tasks (KATs), analyzing its general properties such as semantic bias. We then investigate the impact of protoknowledge on Text-to-SPARQL performance by varying prompting strategies depending on input conditions. To this end, we adopt a novel analysis framework that assesses whether model predictions align with the successful activation of the relevant protoknowledge for each query. This methodology provides a practical tool to explore Semantic-Level Data Contamination and serves as an effective strategy for Closed-Pretraining models.",
        "timestamp": "2025-10-18T07:12:25.734Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Knowledge Graphs",
          "Generalization",
          "Memorization",
          "Text-To-SPARQL"
        ],
        "doi": "",
        "journalName": "ACL ARR 2025 May Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 378,
        "object_id": "paper:openreview.Ou2EtkDRes",
        "created_at": "2025-10-18T07:12:26+00:00",
        "updated_at": "2025-10-18T07:12:48+00:00",
        "version": 1
      }
    },
    "interactions:openreview.sYF69As4M5": {
      "data": {
        "sourceId": "openreview",
        "paperId": "sYF69As4M5",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:13:25.626Z",
            "data": {
              "session_id": "session_1760771605021_hwwxjbr",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-10-18T07:12:35.879Z",
              "end_time": "2025-10-18T07:13:25.021Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 4,
              "total_elapsed_seconds": 49
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:14:56.284Z",
            "data": {
              "session_id": "session_1760771696254_3rt6ooo",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-10-18T07:14:50.946Z",
              "end_time": "2025-10-18T07:14:56.254Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:16:56.304Z",
            "data": {
              "session_id": "session_1760771816292_37huieo",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-10-18T07:16:36.853Z",
              "end_time": "2025-10-18T07:16:56.292Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 4,
              "total_elapsed_seconds": 19
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:20:05.208Z",
            "data": {
              "session_id": "session_1760772004915_l25arqj",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-10-18T07:19:57.479Z",
              "end_time": "2025-10-18T07:20:04.915Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:29:09.065Z",
            "data": {
              "session_id": "session_1760772548775_bgvmpg9",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-10-18T07:28:21.172Z",
              "end_time": "2025-10-18T07:29:08.775Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 3,
              "total_elapsed_seconds": 48
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T09:20:22.137Z",
            "data": {
              "session_id": "session_1761211222123_5w1kdfb",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-10-23T09:20:05.542Z",
              "end_time": "2025-10-23T09:20:22.123Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T07:15:12.815Z",
            "data": {
              "session_id": "session_1762931712191_4su8byv",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-11-12T07:13:45.427Z",
              "end_time": "2025-11-12T07:15:12.191Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 2,
              "total_elapsed_seconds": 87
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T07:16:30.860Z",
            "data": {
              "session_id": "session_1762931790219_gfzasxr",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-11-12T07:16:07.220Z",
              "end_time": "2025-11-12T07:16:30.219Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T07:18:17.710Z",
            "data": {
              "session_id": "session_1762931897442_hqk0i8z",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-11-12T07:18:00.068Z",
              "end_time": "2025-11-12T07:18:17.442Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T05:50:44.628Z",
            "data": {
              "session_id": "session_1763877044391_vk7575p",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-11-23T05:50:24.662Z",
              "end_time": "2025-11-23T05:50:44.391Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-24T19:29:44.058Z",
            "data": {
              "session_id": "session_1764012583494_hw3wcz8",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-11-24T19:25:02.991Z",
              "end_time": "2025-11-24T19:29:43.494Z",
              "heartbeat_count": 56,
              "duration_seconds": 280,
              "idle_seconds": 1,
              "total_elapsed_seconds": 281
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-24T20:06:52.617Z",
            "data": {
              "session_id": "session_1764014812006_b1pdwl8",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-11-24T19:58:55.671Z",
              "end_time": "2025-11-24T20:06:52.006Z",
              "heartbeat_count": 95,
              "duration_seconds": 475,
              "idle_seconds": 1,
              "total_elapsed_seconds": 476
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T09:14:25.571Z",
            "data": {
              "session_id": "session_1764062064815_w0mud6r",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-11-25T09:12:40.374Z",
              "end_time": "2025-11-25T09:14:24.815Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 4,
              "total_elapsed_seconds": 104
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T13:04:49.443Z",
            "data": {
              "session_id": "session_1764075889102_7t1nqii",
              "source_id": "openreview",
              "paper_id": "sYF69As4M5",
              "start_time": "2025-11-25T13:04:41.294Z",
              "end_time": "2025-11-25T13:04:49.102Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 380,
        "object_id": "interactions:openreview.sYF69As4M5",
        "created_at": "2025-10-18T07:13:27+00:00",
        "updated_at": "2025-11-25T13:05:15+00:00",
        "version": 1
      }
    },
    "interactions:openreview.Ou2EtkDRes": {
      "data": {
        "sourceId": "openreview",
        "paperId": "Ou2EtkDRes",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:12:36.485Z",
            "data": {
              "session_id": "session_1760771555851_b960s0i",
              "source_id": "openreview",
              "paper_id": "Ou2EtkDRes",
              "start_time": "2025-10-18T07:12:26.471Z",
              "end_time": "2025-10-18T07:12:35.851Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:14:04.400Z",
            "data": {
              "session_id": "session_1760771644394_zi4zx0j",
              "source_id": "openreview",
              "paper_id": "Ou2EtkDRes",
              "start_time": "2025-10-18T07:13:39.772Z",
              "end_time": "2025-10-18T07:14:04.394Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 5,
              "total_elapsed_seconds": 25
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T07:17:25.013Z",
            "data": {
              "session_id": "session_1760771844998_atow9ay",
              "source_id": "openreview",
              "paper_id": "Ou2EtkDRes",
              "start_time": "2025-10-18T07:17:19.626Z",
              "end_time": "2025-10-18T07:17:24.998Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T09:17:58.606Z",
            "data": {
              "session_id": "session_1761211078561_oa4xr5n",
              "source_id": "openreview",
              "paper_id": "Ou2EtkDRes",
              "start_time": "2025-10-23T09:17:39.685Z",
              "end_time": "2025-10-23T09:17:58.561Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 4,
              "total_elapsed_seconds": 19
            }
          }
        ]
      },
      "meta": {
        "issue_number": 379,
        "object_id": "interactions:openreview.Ou2EtkDRes",
        "created_at": "2025-10-18T07:12:37+00:00",
        "updated_at": "2025-10-23T09:18:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.13069": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.13069",
        "url": "https://arxiv.org/abs/2406.13069",
        "title": "Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG",
        "authors": "William Merrill, Noah A. Smith, Yanai Elazar",
        "abstract": "How novel are texts generated by language models (LMs) relative to their training corpora? In this work, we investigate the extent to which modern LMs generate $n$-grams from their training data, evaluating both (i) the probability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the proportion of $n$-grams generated by an LM that did not appear in the training data (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search over a corpus in constant time w.r.t. corpus size, we develop Rusty-DAWG, a novel search tool inspired by indexing of genomic data. We compare the novelty of LM-generated text to human-written text and explore factors that affect generation novelty, focusing on the Pythia models. We find that, for $n > 4$, LM-generated text is less novel than human-written text, though it is more novel for smaller $n$. Larger LMs and more constrained decoding strategies both decrease novelty. Finally, we show that LMs complete $n$-grams with lower loss if they are more frequent in the training data. Overall, our results reveal factors influencing the novelty of LM-generated text, and we release Rusty-DAWG to facilitate further pretraining data research.",
        "timestamp": "2025-10-18T08:10:04.764Z",
        "rating": "novote",
        "publishedDate": "2024/06/18",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 381,
        "object_id": "paper:arxiv.2406.13069",
        "created_at": "2025-10-18T08:10:05+00:00",
        "updated_at": "2025-10-18T08:10:23+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.13069": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.13069",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T08:15:02.470Z",
            "data": {
              "session_id": "session_1760775301433_oue4cb3",
              "source_id": "arxiv",
              "paper_id": "2406.13069",
              "start_time": "2025-10-18T08:10:06.805Z",
              "end_time": "2025-10-18T08:15:01.433Z",
              "heartbeat_count": 58,
              "duration_seconds": 290,
              "idle_seconds": 5,
              "total_elapsed_seconds": 295
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T20:20:02.610Z",
            "data": {
              "session_id": "session_1762806002112_m4bu48y",
              "source_id": "arxiv",
              "paper_id": "2406.13069",
              "start_time": "2025-11-10T20:19:56.959Z",
              "end_time": "2025-11-10T20:20:02.112Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T20:25:46.882Z",
            "data": {
              "session_id": "session_1763411146819_9kh1hns",
              "source_id": "arxiv",
              "paper_id": "2406.13069",
              "start_time": "2025-11-17T20:25:40.842Z",
              "end_time": "2025-11-17T20:25:46.819Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-22T12:06:00.930Z",
            "data": {
              "session_id": "session_1763813160681_8td338h",
              "source_id": "arxiv",
              "paper_id": "2406.13069",
              "start_time": "2025-11-22T12:05:54.768Z",
              "end_time": "2025-11-22T12:06:00.681Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-22T12:07:25.796Z",
            "data": {
              "session_id": "session_1763813245523_dxaquvw",
              "source_id": "arxiv",
              "paper_id": "2406.13069",
              "start_time": "2025-11-22T12:06:58.612Z",
              "end_time": "2025-11-22T12:07:25.523Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          }
        ]
      },
      "meta": {
        "issue_number": 382,
        "object_id": "interactions:arxiv.2406.13069",
        "created_at": "2025-10-18T08:15:03+00:00",
        "updated_at": "2025-11-22T12:07:47+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.14261": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.14261",
        "url": "https://arxiv.org/abs/2510.14261",
        "title": "Rewriting History: A Recipe for Interventional Analyses to Study Data Effects on Model Behavior",
        "authors": "Rahul Nadkarni, Yanai Elazar, Hila Gonen, Noah A. Smith",
        "abstract": "We present an experimental recipe for studying the relationship between training data and language model (LM) behavior. We outline steps for intervening on data batches -- i.e., ``rewriting history'' -- and then retraining model checkpoints over that data to test hypotheses relating data to behavior. Our recipe breaks down such an intervention into stages that include selecting evaluation items from a benchmark that measures model behavior, matching relevant documents to those items, and modifying those documents before retraining and measuring the effects. We demonstrate the utility of our recipe through case studies on factual knowledge acquisition in LMs, using both cooccurrence statistics and information retrieval methods to identify documents that might contribute to knowledge learning. Our results supplement past observational analyses that link cooccurrence to model behavior, while demonstrating that extant methods for identifying relevant training documents do not fully explain an LM's ability to correctly answer knowledge questions. Overall, we outline a recipe that researchers can follow to test further hypotheses about how training data affects model behavior. Our code is made publicly available to promote future work.",
        "timestamp": "2025-10-18T13:48:50.432Z",
        "rating": "novote",
        "publishedDate": "2025/10/16",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 383,
        "object_id": "paper:arxiv.2510.14261",
        "created_at": "2025-10-18T13:48:50+00:00",
        "updated_at": "2025-10-18T13:49:11+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.14261": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.14261",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T13:58:47.417Z",
            "data": {
              "session_id": "session_1760795927356_ny7ry5d",
              "source_id": "arxiv",
              "paper_id": "2510.14261",
              "start_time": "2025-10-18T13:58:41.755Z",
              "end_time": "2025-10-18T13:58:47.356Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T14:12:28.218Z",
            "data": {
              "session_id": "session_1760796747673_s6ug2ff",
              "source_id": "arxiv",
              "paper_id": "2510.14261",
              "start_time": "2025-10-18T14:04:19.320Z",
              "end_time": "2025-10-18T14:12:27.673Z",
              "heartbeat_count": 97,
              "duration_seconds": 485,
              "idle_seconds": 3,
              "total_elapsed_seconds": 488
            }
          }
        ]
      },
      "meta": {
        "issue_number": 384,
        "object_id": "interactions:arxiv.2510.14261",
        "created_at": "2025-10-18T13:58:48+00:00",
        "updated_at": "2025-10-18T14:12:46+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.06184": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.06184",
        "url": "https://arxiv.org/pdf/2509.06184",
        "title": "Understanding the Influence of Synthetic Data for Text Embedders",
        "authors": "Jacob Mitchell Springer, Vaibhav Adlakha, Siva Reddy, Aditi Raghunathan, Marius Mosbach",
        "abstract": "Recent progress in developing general purpose text embedders has been driven\nby training on ever-growing corpora of synthetic LLM-generated data.\nNonetheless, no publicly available synthetic dataset exists, posing a barrier\nto studying its role for generalization. To address this issue, we first\nreproduce and publicly release the synthetic data proposed by Wang et al.\n(Mistral-E5). Our synthetic data is high quality and leads to consistent\nimprovements in performance. Next, we critically examine where exactly\nsynthetic data improves model generalization. Our analysis reveals that\nbenefits from synthetic data are sparse and highly localized to individual\ndatasets. Moreover, we observe trade-offs between the performance on different\ncategories and data that benefits one task, degrades performance on another.\nOur findings highlight the limitations of current synthetic data approaches for\nbuilding general-purpose embedders and challenge the notion that training on\nsynthetic data leads to more robust embedding models across tasks.",
        "timestamp": "2025-10-18T15:03:27.947Z",
        "rating": "novote",
        "publishedDate": "2025-09-07T19:28:52Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 385,
        "object_id": "paper:arxiv.2509.06184",
        "created_at": "2025-10-18T15:03:28+00:00",
        "updated_at": "2025-10-18T15:03:47+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.06184": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.06184",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-18T15:05:27.247Z",
            "data": {
              "session_id": "session_1760799926659_f3pg9nh",
              "source_id": "arxiv",
              "paper_id": "2509.06184",
              "start_time": "2025-10-18T15:03:31.771Z",
              "end_time": "2025-10-18T15:05:26.659Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 5,
              "total_elapsed_seconds": 115
            }
          }
        ]
      },
      "meta": {
        "issue_number": 386,
        "object_id": "interactions:arxiv.2509.06184",
        "created_at": "2025-10-18T15:05:28+00:00",
        "updated_at": "2025-10-18T15:05:50+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.21155": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.21155",
        "url": "https://arxiv.org/pdf/2509.21155",
        "title": "Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in\n  Language Models",
        "authors": "Chantal Shaib, Vinith M. Suriyakumar, Levent Sagun, Byron C. Wallace, Marzyeh Ghassemi",
        "abstract": "For an LLM to correctly respond to an instruction it must understand both the\nsemantics and the domain (i.e., subject area) of a given task-instruction pair.\nHowever, syntax can also convey implicit information Recent work shows that\nsyntactic templates -- frequent sequences of Part-of-Speech (PoS) tags -- are\nprevalent in training data and often appear in model outputs. In this work we\ncharacterize syntactic templates, domain, and semantics in task-instruction\npairs. We identify cases of spurious correlations between syntax and domain,\nwhere models learn to associate a domain with syntax during training; this can\nsometimes override prompt semantics. Using a synthetic training dataset, we\nfind that the syntactic-domain correlation can lower performance (mean 0.51 +/-\n0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an\nevaluation framework to detect this phenomenon in trained models, and show that\nit occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;\nLlama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study\non the implications for safety finetuning, showing that unintended\nsyntactic-domain correlations can be used to bypass refusals in OLMo-2-7B\nInstruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test\nfor syntactic-domain correlations, and (2) to ensure syntactic diversity in\ntraining data, specifically within domains, to prevent such spurious\ncorrelations.",
        "timestamp": "2025-10-20T05:03:07.353Z",
        "rating": "novote",
        "publishedDate": "2025-09-25T13:42:28Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 387,
        "object_id": "paper:arxiv.2509.21155",
        "created_at": "2025-10-20T05:03:07+00:00",
        "updated_at": "2025-10-20T05:03:32+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.21155": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.21155",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T05:06:12.125Z",
            "data": {
              "session_id": "session_1760936771530_8q98nbb",
              "source_id": "arxiv",
              "paper_id": "2509.21155",
              "start_time": "2025-10-20T05:03:12.208Z",
              "end_time": "2025-10-20T05:06:11.530Z",
              "heartbeat_count": 35,
              "duration_seconds": 175,
              "idle_seconds": 4,
              "total_elapsed_seconds": 179
            }
          }
        ]
      },
      "meta": {
        "issue_number": 388,
        "object_id": "interactions:arxiv.2509.21155",
        "created_at": "2025-10-20T05:06:13+00:00",
        "updated_at": "2025-10-20T05:06:37+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.06485": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.06485",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T14:44:05.745Z",
            "data": {
              "session_id": "session_1760971445299_g2mgx8l",
              "source_id": "arxiv",
              "paper_id": "2506.06485",
              "start_time": "2025-10-20T14:43:59.970Z",
              "end_time": "2025-10-20T14:44:05.299Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T15:58:39.753Z",
            "data": {
              "session_id": "session_1761839919747_ipy1hvt",
              "source_id": "arxiv",
              "paper_id": "2506.06485",
              "start_time": "2025-10-30T15:58:19.476Z",
              "end_time": "2025-10-30T15:58:39.747Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T10:44:40.094Z",
            "data": {
              "session_id": "session_1761993880087_alnpptb",
              "source_id": "arxiv",
              "paper_id": "2506.06485",
              "start_time": "2025-11-01T10:44:33.537Z",
              "end_time": "2025-11-01T10:44:40.087Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 390,
        "object_id": "interactions:arxiv.2506.06485",
        "created_at": "2025-10-20T14:44:06+00:00",
        "updated_at": "2025-11-01T10:45:01+00:00",
        "version": 1
      }
    },
    "interactions:openreview.AFMGbq39bQ": {
      "data": {
        "sourceId": "openreview",
        "paperId": "AFMGbq39bQ",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-31T08:02:08.339Z",
            "data": {
              "session_id": "session_1761897728228_31b1zsh",
              "source_id": "openreview",
              "paper_id": "AFMGbq39bQ",
              "start_time": "2025-10-31T08:01:57.808Z",
              "end_time": "2025-10-31T08:02:08.228Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "issue_number": 389,
        "object_id": "interactions:openreview.AFMGbq39bQ",
        "created_at": "2025-10-20T14:43:52+00:00",
        "updated_at": "2025-10-31T08:02:21+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.22137": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.22137",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:12:26.028Z",
            "data": {
              "session_id": "session_1760973145718_7vii4kj",
              "source_id": "arxiv",
              "paper_id": "2505.22137",
              "start_time": "2025-10-20T15:12:18.960Z",
              "end_time": "2025-10-20T15:12:25.718Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 398,
        "object_id": "interactions:arxiv.2505.22137",
        "created_at": "2025-10-20T15:12:26+00:00",
        "updated_at": "2025-10-20T15:12:52+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.22137": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.22137",
        "url": "https://arxiv.org/pdf/2505.22137",
        "title": "Limited Generalizability in Argument Mining: State-Of-The-Art Models\n  Learn Datasets, Not Arguments",
        "authors": "Marc Feger, Katarina Boland, Stefan Dietze",
        "abstract": "Identifying arguments is a necessary prerequisite for various tasks in\nautomated discourse analysis, particularly within contexts such as political\ndebates, online discussions, and scientific reasoning. In addition to\ntheoretical advances in understanding the constitution of arguments, a\nsignificant body of research has emerged around practical argument mining,\nsupported by a growing number of publicly available datasets. On these\nbenchmarks, BERT-like transformers have consistently performed best,\nreinforcing the belief that such models are broadly applicable across diverse\ncontexts of debate. This study offers the first large-scale re-evaluation of\nsuch state-of-the-art models, with a specific focus on their ability to\ngeneralize in identifying arguments. We evaluate four transformers, three\nstandard and one enhanced with contrastive pre-training for better\ngeneralization, on 17 English sentence-level datasets as most relevant to the\ntask. Our findings show that, to varying degrees, these models tend to rely on\nlexical shortcuts tied to content words, suggesting that apparent progress may\noften be driven by dataset-specific cues rather than true task alignment. While\nthe models achieve strong results on familiar benchmarks, their performance\ndrops markedly when applied to unseen datasets. Nonetheless, incorporating both\ntask-specific pre-training and joint benchmark training proves effective in\nenhancing both robustness and generalization.",
        "timestamp": "2025-10-20T15:12:17.125Z",
        "rating": "novote",
        "publishedDate": "2025-05-28T09:00:56Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 395,
        "object_id": "paper:arxiv.2505.22137",
        "created_at": "2025-10-20T15:12:17+00:00",
        "updated_at": "2025-10-20T15:12:43+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.20959": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.20959",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:12:14.138Z",
            "data": {
              "session_id": "session_1760973133532_ti7a8to",
              "source_id": "arxiv",
              "paper_id": "2505.20959",
              "start_time": "2025-10-20T15:11:55.818Z",
              "end_time": "2025-10-20T15:12:13.532Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:18:26.982Z",
            "data": {
              "session_id": "session_1760973506224_scxhtr1",
              "source_id": "arxiv",
              "paper_id": "2505.20959",
              "start_time": "2025-10-20T15:17:28.652Z",
              "end_time": "2025-10-20T15:18:26.224Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 3,
              "total_elapsed_seconds": 58
            }
          }
        ]
      },
      "meta": {
        "issue_number": 394,
        "object_id": "interactions:arxiv.2505.20959",
        "created_at": "2025-10-20T15:12:14+00:00",
        "updated_at": "2025-10-20T15:18:53+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.20959": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.20959",
        "url": "https://arxiv.org/pdf/2505.20959",
        "title": "Research Community Perspectives on \"Intelligence\" and Large Language\n  Models",
        "authors": "Bertram H\u00f8jer, Terne Sasha Thorn Jakobsen, Anna Rogers, Stefan Heinrich",
        "abstract": "Despite the widespread use of ''artificial intelligence'' (AI) framing in\nNatural Language Processing (NLP) research, it is not clear what researchers\nmean by ''intelligence''. To that end, we present the results of a survey on\nthe notion of ''intelligence'' among researchers and its role in the research\nagenda. The survey elicited complete responses from 303 researchers from a\nvariety of fields including NLP, Machine Learning (ML), Cognitive Science,\nLinguistics, and Neuroscience. We identify 3 criteria of intelligence that the\ncommunity agrees on the most: generalization, adaptability, & reasoning. Our\nresults suggests that the perception of the current NLP systems as\n''intelligent'' is a minority position (29%). Furthermore, only 16.2% of the\nrespondents see developing intelligent systems as a research goal, and these\nrespondents are more likely to consider the current systems intelligent.",
        "timestamp": "2025-10-20T15:11:56.168Z",
        "rating": "novote",
        "publishedDate": "2025-05-27T09:53:27Z",
        "tags": [
          "cs.CL",
          "cs.CY"
        ],
        "doi": "",
        "journalName": "Findings of the Association for Computational Linguistics: ACL\n  2025",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 391,
        "object_id": "paper:arxiv.2505.20959",
        "created_at": "2025-10-20T15:11:56+00:00",
        "updated_at": "2025-10-20T15:12:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2508.13180": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.13180",
        "url": "https://arxiv.org/pdf/2508.13180?",
        "title": "Search-Time Data Contamination",
        "authors": "Ziwen Han, Meher Mankikar, Julian Michael, Zifan Wang",
        "abstract": "Data contamination refers to the leakage of evaluation data into model\ntraining data, resulting in overfitting to supposedly held-out test sets and\ncompromising test validity. We identify an analogous issue, search-time\ncontamination (STC), in evaluating search-based LLM agents which use tools to\ngather information from online sources when answering user queries. STC occurs\nwhen the retrieval step surfaces a source containing the test question (or a\nnear-duplicate) alongside its answer, enabling agents to copy rather than\ngenuinely infer or reason, undermining benchmark integrity. We find that\nHuggingFace, an online platform hosting evaluation datasets, appears among\nretrieved sources in search based agent logs. Consequently, agents often\nexplicitly acknowledge discovering question answer pairs from HuggingFace\nwithin their reasoning chains. On three commonly used capability benchmarks:\nHumanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for\napproximately 3% of questions, search-based agents directly find the datasets\nwith ground truth labels on HuggingFace. When millions of evaluation queries\ntarget the same benchmark, even small, repeated leaks can accelerate the\nbenchmark's obsolescence, shortening its intended lifecycle. After HuggingFace\nis blocked, we observe a drop in accuracy on the contaminated subset of\napproximately 15%. We further show through ablation experiments that publicly\naccessible evaluation datasets on HuggingFace may not be the sole source of\nSTC. To this end, we conclude by proposing best practices for benchmark design\nand result reporting to address this novel form of leakage and ensure\ntrustworthy evaluation of search-based LLM agents. To facilitate the auditing\nof evaluation results, we also publicly release the complete logs from our\nexperiments.",
        "timestamp": "2025-10-20T15:20:36.711Z",
        "rating": "novote",
        "publishedDate": "2025-08-12T22:52:21Z",
        "tags": [
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 399,
        "object_id": "paper:arxiv.2508.13180",
        "created_at": "2025-10-20T15:20:37+00:00",
        "updated_at": "2025-10-20T15:20:54+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2508.13180": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.13180",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:21:14.700Z",
            "data": {
              "session_id": "session_1760973674696_c82v79e",
              "source_id": "arxiv",
              "paper_id": "2508.13180",
              "start_time": "2025-10-20T15:20:42.171Z",
              "end_time": "2025-10-20T15:21:14.696Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 3,
              "total_elapsed_seconds": 33
            }
          }
        ]
      },
      "meta": {
        "issue_number": 400,
        "object_id": "interactions:arxiv.2508.13180",
        "created_at": "2025-10-20T15:21:15+00:00",
        "updated_at": "2025-10-20T15:21:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.14417": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.14417",
        "url": "https://arxiv.org/pdf/2507.14417?",
        "title": "Inverse Scaling in Test-Time Compute",
        "authors": "Aryo Pradipta Gema, Alexander H\u00e4gele, Runjin Chen, Andy Arditi, Jacob Goldman-Wetzler, Kit Fraser-Taliente, Henry Sleight, Linda Petrini, Julian Michael, Beatrice Alex, Pasquale Minervini, Yanda Chen, Joe Benton, Ethan Perez",
        "abstract": "We construct evaluation tasks where extending the reasoning length of Large\nReasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling\nrelationship between test-time compute and accuracy. Our evaluation tasks span\nfour categories: simple counting tasks with distractors, regression tasks with\nspurious features, deduction tasks with constraint tracking, and advanced AI\nrisks. We identify five distinct failure modes when models reason for longer:\n1) Claude models become increasingly distracted by irrelevant information; 2)\nOpenAI o-series models resist distractors but overfit to problem framings; 3)\nmodels shift from reasonable priors to spurious correlations; 4) all models\nshow difficulties in maintaining focus on complex deductive tasks; and 5)\nextended reasoning may amplify concerning behaviors, with Claude Sonnet 4\nshowing increased expressions of self-preservation. These findings suggest that\nwhile test-time compute scaling remains promising for improving model\ncapabilities, it may inadvertently reinforce problematic reasoning patterns.\nOur results demonstrate the importance of evaluating models across diverse\nreasoning lengths to identify and address these failure modes in LRMs.",
        "timestamp": "2025-10-20T15:22:54.027Z",
        "rating": "novote",
        "publishedDate": "2025-07-19T00:06:13Z",
        "tags": [
          "cs.AI",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 401,
        "object_id": "paper:arxiv.2507.14417",
        "created_at": "2025-10-20T15:22:54+00:00",
        "updated_at": "2025-10-20T15:23:18+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.14417": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.14417",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:23:52.068Z",
            "data": {
              "session_id": "session_1760973831323_q16lalg",
              "source_id": "arxiv",
              "paper_id": "2507.14417",
              "start_time": "2025-10-20T15:22:53.572Z",
              "end_time": "2025-10-20T15:23:51.323Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 3,
              "total_elapsed_seconds": 58
            }
          }
        ]
      },
      "meta": {
        "issue_number": 403,
        "object_id": "interactions:arxiv.2507.14417",
        "created_at": "2025-10-20T15:23:52+00:00",
        "updated_at": "2025-10-20T15:24:18+00:00",
        "version": 1
      }
    },
    "paper:openreview.Ti67584b98": {
      "data": {
        "sourceId": "openreview",
        "paperId": "Ti67584b98",
        "url": "https://openreview.net/pdf?id=Ti67584b98",
        "title": "Ti67584b98",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-10-20T15:24:54.659Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 404,
        "object_id": "paper:openreview.Ti67584b98",
        "created_at": "2025-10-20T15:24:55+00:00",
        "updated_at": "2025-10-20T15:25:19+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2208.12852": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2208.12852",
        "url": "https://arxiv.org/pdf/2208.12852",
        "title": "What Do NLP Researchers Believe? Results of the NLP Community Metasurvey",
        "authors": "Julian Michael, Ari Holtzman, Alicia Parrish, Aaron Mueller, Alex Wang, Angelica Chen, Divyam Madaan, Nikita Nangia, Richard Yuanzhe Pang, Jason Phang, Samuel R. Bowman",
        "abstract": "We present the results of the NLP Community Metasurvey. Run from May to June\n2022, the survey elicited opinions on controversial issues, including industry\ninfluence in the field, concerns about AGI, and ethics. Our results put\nconcrete numbers to several controversies: For example, respondents are split\nalmost exactly in half on questions about the importance of artificial general\nintelligence, whether language models understand language, and the necessity of\nlinguistic structure and inductive bias for solving NLP problems. In addition,\nthe survey posed meta-questions, asking respondents to predict the distribution\nof survey responses. This allows us not only to gain insight on the spectrum of\nbeliefs held by NLP researchers, but also to uncover false sociological beliefs\nwhere the community's predictions don't match reality. We find such mismatches\non a wide range of issues. Among other results, the community greatly\noverestimates its own belief in the usefulness of benchmarks and the potential\nfor scaling to solve real-world problems, while underestimating its own belief\nin the importance of linguistic structure, inductive bias, and\ninterdisciplinary science.",
        "timestamp": "2025-10-20T15:27:19.171Z",
        "rating": "novote",
        "publishedDate": "2022-08-26T19:45:51Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "I.2.7"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 406,
        "object_id": "paper:arxiv.2208.12852",
        "created_at": "2025-10-20T15:27:19+00:00",
        "updated_at": "2025-10-20T15:27:42+00:00",
        "version": 1
      }
    },
    "interactions:openreview.Ti67584b98": {
      "data": {
        "sourceId": "openreview",
        "paperId": "Ti67584b98",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:26:45.513Z",
            "data": {
              "session_id": "session_1760974004713_hcpvq2b",
              "source_id": "openreview",
              "paper_id": "Ti67584b98",
              "start_time": "2025-10-20T15:25:02.871Z",
              "end_time": "2025-10-20T15:26:44.713Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 2,
              "total_elapsed_seconds": 102
            }
          }
        ]
      },
      "meta": {
        "issue_number": 405,
        "object_id": "interactions:openreview.Ti67584b98",
        "created_at": "2025-10-20T15:26:46+00:00",
        "updated_at": "2025-10-20T15:27:10+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2401.12631": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.12631",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:29:45.521Z",
            "data": {
              "session_id": "session_1760974185507_cife153",
              "source_id": "arxiv",
              "paper_id": "2401.12631",
              "start_time": "2025-10-20T15:29:38.581Z",
              "end_time": "2025-10-20T15:29:45.507Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:31:14.167Z",
            "data": {
              "session_id": "session_1760974274145_2x8toz6",
              "source_id": "arxiv",
              "paper_id": "2401.12631",
              "start_time": "2025-10-20T15:30:53.417Z",
              "end_time": "2025-10-20T15:31:14.145Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          }
        ]
      },
      "meta": {
        "issue_number": 411,
        "object_id": "interactions:arxiv.2401.12631",
        "created_at": "2025-10-20T15:29:39+00:00",
        "updated_at": "2025-10-20T15:31:40+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2401.12631": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.12631",
        "url": "https://arxiv.org/abs/2401.12631",
        "title": "A Reply to Makelov et al. (2023)'s \"Interpretability Illusion\" Arguments",
        "authors": "Zhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts, Noah D. Goodman",
        "abstract": "We respond to the recent paper by Makelov et al. (2023), which reviews subspace interchange intervention methods like distributed alignment search (DAS; Geiger et al. 2023) and claims that these methods potentially cause \"interpretability illusions\". We first review Makelov et al. (2023)'s technical notion of what an \"interpretability illusion\" is, and then we show that even intuitive and desirable explanations can qualify as illusions in this sense. As a result, their method of discovering \"illusions\" can reject explanations they consider \"non-illusory\". We then argue that the illusions Makelov et al. (2023) see in practice are artifacts of their training and evaluation paradigms. We close by emphasizing that, though we disagree with their core characterization, Makelov et al. (2023)'s examples and discussion have undoubtedly pushed the field of interpretability forward.",
        "timestamp": "2025-10-20T15:29:29.025Z",
        "rating": "novote",
        "publishedDate": "2024/01/23",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 410,
        "object_id": "paper:arxiv.2401.12631",
        "created_at": "2025-10-20T15:29:29+00:00",
        "updated_at": "2025-10-20T15:29:55+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2311.17030": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.17030",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:29:28.291Z",
            "data": {
              "session_id": "session_1760974167524_votqyvy",
              "source_id": "arxiv",
              "paper_id": "2311.17030",
              "start_time": "2025-10-20T15:29:19.993Z",
              "end_time": "2025-10-20T15:29:27.524Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:30:53.360Z",
            "data": {
              "session_id": "session_1760974253353_vp959m7",
              "source_id": "arxiv",
              "paper_id": "2311.17030",
              "start_time": "2025-10-20T15:30:35.994Z",
              "end_time": "2025-10-20T15:30:53.353Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T10:56:02.403Z",
            "data": {
              "session_id": "session_1761216962108_b1j7qb1",
              "source_id": "arxiv",
              "paper_id": "2311.17030",
              "start_time": "2025-10-23T10:55:44.156Z",
              "end_time": "2025-10-23T10:56:02.108Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T15:51:24.818Z",
            "data": {
              "session_id": "session_1762185084814_ttw6375",
              "source_id": "arxiv",
              "paper_id": "2311.17030",
              "start_time": "2025-11-03T15:50:32.314Z",
              "end_time": "2025-11-03T15:51:24.814Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 3,
              "total_elapsed_seconds": 53
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T15:58:36.280Z",
            "data": {
              "session_id": "session_1762185515463_819x4y6",
              "source_id": "arxiv",
              "paper_id": "2311.17030",
              "start_time": "2025-11-03T15:52:03.390Z",
              "end_time": "2025-11-03T15:58:35.463Z",
              "heartbeat_count": 78,
              "duration_seconds": 390,
              "idle_seconds": 2,
              "total_elapsed_seconds": 392
            }
          }
        ]
      },
      "meta": {
        "issue_number": 409,
        "object_id": "interactions:arxiv.2311.17030",
        "created_at": "2025-10-20T15:29:29+00:00",
        "updated_at": "2025-11-03T15:59:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2311.17030": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.17030",
        "url": "https://arxiv.org/abs/2311.17030",
        "title": "Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching",
        "authors": "Aleksandar Makelov, Georg Lange, Neel Nanda",
        "abstract": "Mechanistic interpretability aims to understand model behaviors in terms of specific, interpretable features, often hypothesized to manifest as low-dimensional subspaces of activations. Specifically, recent studies have explored subspace interventions (such as activation patching) as a way to simultaneously manipulate model behavior and attribute the features behind it to given subspaces.\nIn this work, we demonstrate that these two aims diverge, potentially leading to an illusory sense of interpretability. Counterintuitively, even if a subspace intervention makes the model's output behave as if the value of a feature was changed, this effect may be achieved by activating a dormant parallel pathway leveraging another subspace that is causally disconnected from model outputs. We demonstrate this phenomenon in a distilled mathematical example, in two real-world domains (the indirect object identification task and factual recall), and present evidence for its prevalence in practice. In the context of factual recall, we further show a link to rank-1 fact editing, providing a mechanistic explanation for previous work observing an inconsistency between fact editing performance and fact localization.\nHowever, this does not imply that activation patching of subspaces is intrinsically unfit for interpretability. To contextualize our findings, we also show what a success case looks like in a task (indirect object identification) where prior manual circuit analysis informs an understanding of the location of a feature. We explore the additional evidence needed to argue that a patched subspace is faithful.",
        "timestamp": "2025-10-20T15:29:20.412Z",
        "rating": "novote",
        "publishedDate": "2023/11/28",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 408,
        "object_id": "paper:arxiv.2311.17030",
        "created_at": "2025-10-20T15:29:20+00:00",
        "updated_at": "2025-10-20T15:29:43+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2208.12852": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2208.12852",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:29:19.485Z",
            "data": {
              "session_id": "session_1760974159462_btk1jto",
              "source_id": "arxiv",
              "paper_id": "2208.12852",
              "start_time": "2025-10-20T15:29:02.062Z",
              "end_time": "2025-10-20T15:29:19.462Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "issue_number": 407,
        "object_id": "interactions:arxiv.2208.12852",
        "created_at": "2025-10-20T15:29:01+00:00",
        "updated_at": "2025-10-20T15:29:28+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2303.12712": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2303.12712",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:31:50.998Z",
            "data": {
              "session_id": "session_1760974310981_bqkbcog",
              "source_id": "arxiv",
              "paper_id": "2303.12712",
              "start_time": "2025-10-20T15:31:40.513Z",
              "end_time": "2025-10-20T15:31:50.981Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:33:02.257Z",
            "data": {
              "session_id": "session_1760974382099_0avbodu",
              "source_id": "arxiv",
              "paper_id": "2303.12712",
              "start_time": "2025-10-20T15:32:45.659Z",
              "end_time": "2025-10-20T15:33:02.099Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T16:44:03.351Z",
            "data": {
              "session_id": "session_1760978642263_jsbhxtq",
              "source_id": "arxiv",
              "paper_id": "2303.12712",
              "start_time": "2025-10-20T16:43:48.132Z",
              "end_time": "2025-10-20T16:44:02.263Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T15:49:06.079Z",
            "data": {
              "session_id": "session_1762184946064_9p4u9ti",
              "source_id": "arxiv",
              "paper_id": "2303.12712",
              "start_time": "2025-11-03T15:48:54.796Z",
              "end_time": "2025-11-03T15:49:06.064Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T15:50:26.046Z",
            "data": {
              "session_id": "session_1762185026029_b867i8w",
              "source_id": "arxiv",
              "paper_id": "2303.12712",
              "start_time": "2025-11-03T15:50:10.748Z",
              "end_time": "2025-11-03T15:50:26.029Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 415,
        "object_id": "interactions:arxiv.2303.12712",
        "created_at": "2025-10-20T15:31:51+00:00",
        "updated_at": "2025-11-03T15:50:45+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2309.13638": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2309.13638",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:31:40.989Z",
            "data": {
              "session_id": "session_1760974300310_dvn0sey",
              "source_id": "arxiv",
              "paper_id": "2309.13638",
              "start_time": "2025-10-20T15:31:25.798Z",
              "end_time": "2025-10-20T15:31:40.310Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T15:32:39.369Z",
            "data": {
              "session_id": "session_1760974359357_f3jcbmc",
              "source_id": "arxiv",
              "paper_id": "2309.13638",
              "start_time": "2025-10-20T15:32:19.071Z",
              "end_time": "2025-10-20T15:32:39.357Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T15:48:23.680Z",
            "data": {
              "session_id": "session_1762184903672_9v9xqyd",
              "source_id": "arxiv",
              "paper_id": "2309.13638",
              "start_time": "2025-11-03T15:47:44.669Z",
              "end_time": "2025-11-03T15:48:23.672Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 4,
              "total_elapsed_seconds": 39
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T15:50:09.577Z",
            "data": {
              "session_id": "session_1762185008963_r0lldk9",
              "source_id": "arxiv",
              "paper_id": "2309.13638",
              "start_time": "2025-11-03T15:49:54.034Z",
              "end_time": "2025-11-03T15:50:08.963Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T15:51:50.579Z",
            "data": {
              "session_id": "session_1762185110460_cz458xr",
              "source_id": "arxiv",
              "paper_id": "2309.13638",
              "start_time": "2025-11-03T15:51:26.822Z",
              "end_time": "2025-11-03T15:51:50.460Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "issue_number": 414,
        "object_id": "interactions:arxiv.2309.13638",
        "created_at": "2025-10-20T15:31:41+00:00",
        "updated_at": "2025-11-03T15:52:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2309.13638": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2309.13638",
        "url": "https://arxiv.org/pdf/2309.13638",
        "title": "Embers of Autoregression: Understanding Large Language Models Through\n  the Problem They are Trained to Solve",
        "authors": "R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, Thomas L. Griffiths",
        "abstract": "The widespread adoption of large language models (LLMs) makes it important to\nrecognize their strengths and limitations. We argue that in order to develop a\nholistic understanding of these systems we need to consider the problem that\nthey were trained to solve: next-word prediction over Internet text. By\nrecognizing the pressures that this task exerts we can make predictions about\nthe strategies that LLMs will adopt, allowing us to reason about when they will\nsucceed or fail. This approach - which we call the teleological approach -\nleads us to identify three factors that we hypothesize will influence LLM\naccuracy: the probability of the task to be performed, the probability of the\ntarget output, and the probability of the provided input. We predict that LLMs\nwill achieve higher accuracy when these probabilities are high than when they\nare low - even in deterministic settings where probability should not matter.\nTo test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven\ntasks, and we find robust evidence that LLMs are influenced by probability in\nthe ways that we have hypothesized. In many cases, the experiments reveal\nsurprising failure modes. For instance, GPT-4's accuracy at decoding a simple\ncipher is 51% when the output is a high-probability word sequence but only 13%\nwhen it is low-probability. These results show that AI practitioners should be\ncareful about using LLMs in low-probability situations. More broadly, we\nconclude that we should not evaluate LLMs as if they are humans but should\ninstead treat them as a distinct type of system - one that has been shaped by\nits own particular set of pressures.",
        "timestamp": "2025-10-20T15:31:26.120Z",
        "rating": "novote",
        "publishedDate": "2023-09-24T13:35:28Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 413,
        "object_id": "paper:arxiv.2309.13638",
        "created_at": "2025-10-20T15:31:26+00:00",
        "updated_at": "2025-10-20T15:31:49+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2303.12712": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2303.12712",
        "url": "https://arxiv.org/abs/2303.12712",
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
        "authors": "S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang",
        "abstract": "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.",
        "timestamp": "2025-10-20T15:31:22.097Z",
        "rating": "novote",
        "publishedDate": "2023/03/22",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 412,
        "object_id": "paper:arxiv.2303.12712",
        "created_at": "2025-10-20T15:31:22+00:00",
        "updated_at": "2025-10-20T15:31:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.22641": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.22641",
        "url": "https://www.arxiv.org/abs/2509.22641",
        "title": "Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity",
        "authors": "Arkadiy Saakyan, Najoung Kim, Smaranda Muresan, Tuhin Chakrabarty",
        "abstract": "N-gram novelty is widely used to evaluate language models' ability to generate text outside of their training data. More recently, it has also been adopted as a metric for measuring textual creativity. However, theoretical work on creativity suggests that this approach may be inadequate, as it does not account for creativity's dual nature: novelty (how original the text is) and appropriateness (how sensical and pragmatic it is). We investigate the relationship between this notion of creativity and n-gram novelty through 7542 expert writer annotations (n=26) of novelty, pragmaticality, and sensicality via close reading of human and AI-generated text. We find that while n-gram novelty is positively associated with expert writer-judged creativity, ~91% of top-quartile expressions by n-gram novelty are not judged as creative, cautioning against relying on n-gram novelty alone. Furthermore, unlike human-written text, higher n-gram novelty in open-source LLMs correlates with lower pragmaticality. In an exploratory study with frontier close-source models, we additionally confirm that they are less likely to produce creative expressions than humans. Using our dataset, we test whether zero-shot, few-shot, and finetuned models are able to identify creative expressions (a positive aspect of writing) and non-pragmatic ones (a negative aspect). Overall, frontier LLMs exhibit performance much higher than random but leave room for improvement, especially struggling to identify non-pragmatic expressions. We further find that LLM-as-a-Judge novelty scores from the best-performing model were predictive of expert writer preferences.",
        "timestamp": "2025-10-20T17:07:18.674Z",
        "rating": "novote",
        "publishedDate": "2025/09/26",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Human-Computer Interaction (cs.HC)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 416,
        "object_id": "paper:arxiv.2509.22641",
        "created_at": "2025-10-20T17:07:19+00:00",
        "updated_at": "2025-10-20T17:07:43+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.22641": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.22641",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T17:09:43.211Z",
            "data": {
              "session_id": "session_1760980182546_v6c76cb",
              "source_id": "arxiv",
              "paper_id": "2509.22641",
              "start_time": "2025-10-20T17:07:26.928Z",
              "end_time": "2025-10-20T17:09:42.546Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 1,
              "total_elapsed_seconds": 136
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T19:46:31.973Z",
            "data": {
              "session_id": "session_1762285591366_fu8jhw7",
              "source_id": "arxiv",
              "paper_id": "2509.22641",
              "start_time": "2025-11-04T19:44:32.925Z",
              "end_time": "2025-11-04T19:46:31.366Z",
              "heartbeat_count": 23,
              "duration_seconds": 115,
              "idle_seconds": 3,
              "total_elapsed_seconds": 118
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T19:50:31.696Z",
            "data": {
              "session_id": "session_1762285831073_w23pfdh",
              "source_id": "arxiv",
              "paper_id": "2509.22641",
              "start_time": "2025-11-04T19:47:05.554Z",
              "end_time": "2025-11-04T19:50:31.073Z",
              "heartbeat_count": 41,
              "duration_seconds": 205,
              "idle_seconds": 1,
              "total_elapsed_seconds": 206
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T20:02:53.257Z",
            "data": {
              "session_id": "session_1762286572943_2qugkew",
              "source_id": "arxiv",
              "paper_id": "2509.22641",
              "start_time": "2025-11-04T20:02:43.622Z",
              "end_time": "2025-11-04T20:02:52.943Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T20:44:58.185Z",
            "data": {
              "session_id": "session_1762289098173_rleqrle",
              "source_id": "arxiv",
              "paper_id": "2509.22641",
              "start_time": "2025-11-04T20:44:43.528Z",
              "end_time": "2025-11-04T20:44:58.173Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T07:46:57.422Z",
            "data": {
              "session_id": "session_1762328817101_ufy6sii",
              "source_id": "arxiv",
              "paper_id": "2509.22641",
              "start_time": "2025-11-05T07:46:50.791Z",
              "end_time": "2025-11-05T07:46:57.101Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 417,
        "object_id": "interactions:arxiv.2509.22641",
        "created_at": "2025-10-20T17:09:44+00:00",
        "updated_at": "2025-11-05T07:47:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.11977": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.11977",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T19:14:24.432Z",
            "data": {
              "session_id": "session_1760987663566_qxhc94v",
              "source_id": "arxiv",
              "paper_id": "2510.11977",
              "start_time": "2025-10-20T19:14:15.496Z",
              "end_time": "2025-10-20T19:14:23.566Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 419,
        "object_id": "interactions:arxiv.2510.11977",
        "created_at": "2025-10-20T19:14:25+00:00",
        "updated_at": "2025-10-20T19:14:49+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.11977": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.11977",
        "url": "https://arxiv.org/abs/2510.11977",
        "title": "Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation",
        "authors": "Sayash Kapoor, Benedikt Stroebl, Peter Kirgis, Nitya Nadgir, Zachary S Siegel, Boyi Wei, Tianci Xue, Ziru Chen, Felix Chen, Saiteja Utpala, Franck Ndzomga, Dheeraj Oruganty, Sophie Luskin, Kangheng Liu, Botao Yu, Amit Arora, Dongyoon Hahm, Harsh Trivedi, Huan Sun, Juyong Lee, Tengjun Jin, Yifan Mai, Yifei Zhou, Yuxuan Zhu, Rishi Bommasani, Daniel Kang, Dawn Song, Peter Henderson, Yu Su, Percy Liang, Arvind Narayanan",
        "abstract": "AI agents have been developed for complex real-world tasks from coding to customer service. But AI agent evaluations suffer from many challenges that undermine our understanding of how well agents really work. We introduce the Holistic Agent Leaderboard (HAL) to address these challenges. We make three main contributions. First, we provide a standardized evaluation harness that orchestrates parallel evaluations across hundreds of VMs, reducing evaluation time from weeks to hours while eliminating common implementation bugs. Second, we conduct three-dimensional analysis spanning models, scaffolds, and benchmarks. We validate the harness by conducting 21,730 agent rollouts across 9 models and 9 benchmarks in coding, web navigation, science, and customer service with a total cost of about $40,000. Our analysis reveals surprising insights, such as higher reasoning effort reducing accuracy in the majority of runs. Third, we use LLM-aided log inspection to uncover previously unreported behaviors, such as searching for the benchmark on HuggingFace instead of solving a task, or misusing credit cards in flight booking tasks. We share all agent logs, comprising 2.5B tokens of language model calls, to incentivize further research into agent behavior. By standardizing how the field evaluates agents and addressing common pitfalls in agent evaluation, we hope to shift the focus from agents that ace benchmarks to agents that work reliably in the real world.",
        "timestamp": "2025-10-20T19:14:15.864Z",
        "rating": "novote",
        "publishedDate": "2025/10/13",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 418,
        "object_id": "paper:arxiv.2510.11977",
        "created_at": "2025-10-20T19:14:16+00:00",
        "updated_at": "2025-10-20T19:14:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.02132": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.02132",
        "url": "https://arxiv.org/abs/2506.02132",
        "title": "Echoes of BERT: Do Modern Language Models Rediscover the Classical NLP Pipeline?",
        "authors": "Michael Li, Nishant Subramani",
        "abstract": "Large transformer-based language models dominate modern NLP, yet our understanding of how they encode linguistic information relies primarily on studies of early models like BERT and GPT-2. Building on classic BERTology work, we analyze 25 models spanning from classical architectures (BERT, DeBERTa, GPT-2) to modern large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5, Llama-3.1), probing layer-by-layer representations across eight linguistic tasks in English. Consistent with earlier findings, we find that hierarchical organization persists in modern models: early layers capture syntax, middle layers handle semantics and entity-level information, and later layers encode discourse phenomena. We dive deeper, conducting an in-depth multilingual analysis of two specific linguistic properties - lexical identity and inflectional morphology - that help disentangle form from meaning. We find that lexical information concentrates linearly in early layers but becomes increasingly nonlinear deeper in the network, while inflectional information remains linearly accessible throughout all layers. Additional analyses of attention mechanisms, steering vectors, and pretraining checkpoints reveal where this information resides within layers, how it can be functionally manipulated, and how representations evolve during pretraining. Taken together, our findings suggest that, even with substantial advances in LLM technologies, transformer models learn to organize linguistic information in similar ways, regardless of model architecture, size, or training regime, indicating that these properties are important for next token prediction. Our code is available at this https URL",
        "timestamp": "2025-10-20T20:03:38.874Z",
        "rating": "novote",
        "publishedDate": "2025/06/02",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 420,
        "object_id": "paper:arxiv.2506.02132",
        "created_at": "2025-10-20T20:03:39+00:00",
        "updated_at": "2025-10-20T20:04:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.02132": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.02132",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-20T20:04:31.768Z",
            "data": {
              "session_id": "session_1760990671707_k024zcf",
              "source_id": "arxiv",
              "paper_id": "2506.02132",
              "start_time": "2025-10-20T20:03:42.508Z",
              "end_time": "2025-10-20T20:04:31.707Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 4,
              "total_elapsed_seconds": 49
            }
          }
        ]
      },
      "meta": {
        "issue_number": 421,
        "object_id": "interactions:arxiv.2506.02132",
        "created_at": "2025-10-20T20:04:32+00:00",
        "updated_at": "2025-10-20T20:05:05+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.01954": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.01954",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T06:34:06.477Z",
            "data": {
              "session_id": "session_1761028446407_wfqqlac",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T06:34:00.637Z",
              "end_time": "2025-10-21T06:34:06.407Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T06:50:03.427Z",
            "data": {
              "session_id": "session_1761029403338_4b8zet1",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T06:49:32.230Z",
              "end_time": "2025-10-21T06:50:03.338Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 1,
              "total_elapsed_seconds": 31
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T06:56:06.937Z",
            "data": {
              "session_id": "session_1761029766895_798r8fu",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T06:55:57.112Z",
              "end_time": "2025-10-21T06:56:06.895Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T07:12:46.766Z",
            "data": {
              "session_id": "session_1761030766054_xjlbaj3",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T07:11:41.734Z",
              "end_time": "2025-10-21T07:12:46.054Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 4,
              "total_elapsed_seconds": 64
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T07:17:27.806Z",
            "data": {
              "session_id": "session_1761031047103_scjx9pg",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T07:12:47.782Z",
              "end_time": "2025-10-21T07:17:27.103Z",
              "heartbeat_count": 55,
              "duration_seconds": 275,
              "idle_seconds": 4,
              "total_elapsed_seconds": 279
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T07:19:48.760Z",
            "data": {
              "session_id": "session_1761031188096_8afprs8",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T07:18:24.171Z",
              "end_time": "2025-10-21T07:19:48.096Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 4,
              "total_elapsed_seconds": 84
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T07:21:42.517Z",
            "data": {
              "session_id": "session_1761031301972_2zmj8uw",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T07:19:49.437Z",
              "end_time": "2025-10-21T07:21:41.972Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 3,
              "total_elapsed_seconds": 113
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T07:29:46.154Z",
            "data": {
              "session_id": "session_1761031785263_di8c2lv",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T07:25:16.448Z",
              "end_time": "2025-10-21T07:29:45.263Z",
              "heartbeat_count": 53,
              "duration_seconds": 265,
              "idle_seconds": 4,
              "total_elapsed_seconds": 269
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T07:44:42.395Z",
            "data": {
              "session_id": "session_1761032682389_tmd8spy",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T07:44:26.700Z",
              "end_time": "2025-10-21T07:44:42.389Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T07:45:50.283Z",
            "data": {
              "session_id": "session_1761032749605_5c186jd",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T07:44:54.527Z",
              "end_time": "2025-10-21T07:45:49.605Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 0,
              "total_elapsed_seconds": 55
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T10:29:33.150Z",
            "data": {
              "session_id": "session_1761042572765_i9xsvw2",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T10:29:10.091Z",
              "end_time": "2025-10-21T10:29:32.765Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T10:41:59.087Z",
            "data": {
              "session_id": "session_1761043319073_rctobrf",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T10:41:22.139Z",
              "end_time": "2025-10-21T10:41:59.073Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T13:21:14.081Z",
            "data": {
              "session_id": "session_1761052873260_q15rb79",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T13:19:52.940Z",
              "end_time": "2025-10-21T13:21:13.260Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 0,
              "total_elapsed_seconds": 80
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T14:11:12.198Z",
            "data": {
              "session_id": "session_1761055871189_rnj4ig6",
              "source_id": "arxiv",
              "paper_id": "2505.01954",
              "start_time": "2025-10-21T14:01:27.489Z",
              "end_time": "2025-10-21T14:11:11.189Z",
              "heartbeat_count": 116,
              "duration_seconds": 580,
              "idle_seconds": 4,
              "total_elapsed_seconds": 584
            }
          }
        ]
      },
      "meta": {
        "issue_number": 423,
        "object_id": "interactions:arxiv.2505.01954",
        "created_at": "2025-10-21T06:34:07+00:00",
        "updated_at": "2025-10-21T14:11:52+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.01954": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.01954",
        "url": "https://arxiv.org/pdf/2505.01954?",
        "title": "Semantic Probabilistic Control of Language Models",
        "authors": "Kareem Ahmed, Catarina G Belem, Padhraic Smyth, Sameer Singh",
        "abstract": "Semantic control entails steering LM generations towards satisfying subtle\nnon-lexical constraints, e.g., toxicity, sentiment, or politeness, attributes\nthat can be captured by a sequence-level verifier. It can thus be viewed as\nsampling from the LM distribution conditioned on the target attribute, a\ncomputationally intractable problem due to the non-decomposable nature of the\nverifier. Existing approaches to LM control either only deal with syntactic\nconstraints which cannot capture the aforementioned attributes, or rely on\nsampling to explore the conditional LM distribution, an ineffective estimator\nfor low-probability events. In this work, we leverage a verifier's gradient\ninformation to efficiently reason over all generations that satisfy the target\nattribute, enabling precise steering of LM generations by reweighing the\nnext-token distribution. Starting from an initial sample, we create a local LM\ndistribution favoring semantically similar sentences. This approximation\nenables the tractable computation of an expected sentence embedding. We use\nthis expected embedding, informed by the verifier's evaluation at the initial\nsample, to estimate the probability of satisfying the constraint, which\ndirectly informs the update to the next-token distribution. We evaluated the\neffectiveness of our approach in controlling the toxicity, sentiment, and\ntopic-adherence of LMs yielding generations satisfying the constraint with high\nprobability (>95%) without degrading their quality.",
        "timestamp": "2025-10-21T06:33:30.943Z",
        "rating": "novote",
        "publishedDate": "2025-05-04T01:21:28Z",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 422,
        "object_id": "paper:arxiv.2505.01954",
        "created_at": "2025-10-21T06:33:31+00:00",
        "updated_at": "2025-10-21T06:34:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.08541": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.08541",
        "url": "https://arxiv.org/pdf/2408.08541",
        "title": "Where is the signal in tokenization space?",
        "authors": "Renato Lui Geh, Honghua Zhang, Kareem Ahmed, Benjie Wang, Guy Van den Broeck",
        "abstract": "Large Language Models (LLMs) are typically shipped with tokenizers that\ndeterministically encode text into so-called canonical token sequences, to\nwhich the LLMs assign probability values. One common assumption is that the\nprobability of a piece of text is the probability of its canonical token\nsequence. However, the tokenization of a string is not unique: e.g., the Llama2\ntokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same\ntext. In this paper, we study non-canonical tokenizations. We prove that, given\na string, it is computationally hard to find the most likely tokenization for\nan autoregressive LLM, as well as to compute the marginal probability over all\npossible tokenizations. We then show how the marginal is, in most cases,\nindistinguishable from the canonical probability. Surprisingly, we then\nempirically demonstrate the existence of a significant amount of signal hidden\nwithin tokenization space. Notably, by simply aggregating the probabilities of\nnon-canonical tokenizations, we achieve improvements across a range of LLM\nevaluation benchmarks for a variety of architectures, including transformers\nand state space models.",
        "timestamp": "2025-10-21T07:30:06.809Z",
        "rating": "novote",
        "publishedDate": "2024-08-16T05:56:10Z",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 424,
        "object_id": "paper:arxiv.2408.08541",
        "created_at": "2025-10-21T07:30:07+00:00",
        "updated_at": "2025-10-21T07:30:38+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2408.08541": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.08541",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T07:33:42.205Z",
            "data": {
              "session_id": "session_1761032022191_p35ax3x",
              "source_id": "arxiv",
              "paper_id": "2408.08541",
              "start_time": "2025-10-21T07:33:09.909Z",
              "end_time": "2025-10-21T07:33:42.191Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T07:46:33.229Z",
            "data": {
              "session_id": "session_1761032792963_llz0j6z",
              "source_id": "arxiv",
              "paper_id": "2408.08541",
              "start_time": "2025-10-21T07:46:24.897Z",
              "end_time": "2025-10-21T07:46:32.963Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 425,
        "object_id": "interactions:arxiv.2408.08541",
        "created_at": "2025-10-21T07:33:10+00:00",
        "updated_at": "2025-10-21T07:47:00+00:00",
        "version": 1
      }
    },
    "interactions:openreview.Vh0Rh4GW33": {
      "data": {
        "sourceId": "openreview",
        "paperId": "Vh0Rh4GW33",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T07:36:48.143Z",
            "data": {
              "session_id": "session_1761032207561_9fb2pec",
              "source_id": "openreview",
              "paper_id": "Vh0Rh4GW33",
              "start_time": "2025-10-21T07:36:37.412Z",
              "end_time": "2025-10-21T07:36:47.561Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "issue_number": 427,
        "object_id": "interactions:openreview.Vh0Rh4GW33",
        "created_at": "2025-10-21T07:36:48+00:00",
        "updated_at": "2025-10-21T07:37:18+00:00",
        "version": 1
      }
    },
    "paper:openreview.Vh0Rh4GW33": {
      "data": {
        "sourceId": "openreview",
        "paperId": "Vh0Rh4GW33",
        "url": "https://openreview.net/forum?id=Vh0Rh4GW33&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Test",
        "authors": "Yanai Elazar",
        "abstract": "This is a test!",
        "timestamp": "2025-10-21T07:36:37.778Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "test",
          "nlp",
          "ai"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 426,
        "object_id": "paper:openreview.Vh0Rh4GW33",
        "created_at": "2025-10-21T07:36:38+00:00",
        "updated_at": "2025-10-21T07:37:08+00:00",
        "version": 1
      }
    },
    "paper:openreview.kKD9MTFU39": {
      "data": {
        "sourceId": "openreview",
        "paperId": "kKD9MTFU39",
        "url": "https://openreview.net/forum?id=kKD9MTFU39&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Test",
        "authors": "Royi Rassin",
        "abstract": "Abstract",
        "timestamp": "2025-10-21T07:42:23.969Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Text"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 429,
        "object_id": "paper:openreview.kKD9MTFU39",
        "created_at": "2025-10-21T07:42:24+00:00",
        "updated_at": "2025-10-21T07:42:51+00:00",
        "version": 1
      }
    },
    "paper:openreview.2dl6upRXZj": {
      "data": {
        "sourceId": "openreview",
        "paperId": "2dl6upRXZj",
        "url": "https://openreview.net/forum?id=2dl6upRXZj&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs",
        "authors": "Aviya Maimon",
        "abstract": "Current evaluations of large language models (LLMs) rely on benchmark scores, but it is difficult to interpret what these individual scores reveal about a model's overall skills. Specifically, as a community we lack understanding of how tasks relate to one another, what they measure in common, how they differ, or which ones are redundant. As a result, models are often assessed via a single score averaged across benchmarks, an approach that fails to capture the models' wholistic strengths and limitations. Here, we propose a new evaluation paradigm that uses factor analysis to identify latent skills driving performance across benchmarks. We apply this method to a comprehensive new leaderboard showcasing the performance of 60 LLMs on 44 tasks, and identify a small set of latent skills that largely explain performance. Finally, we turn these insights into practical tools that identify redundant tasks, aid in model selection, and profile models along each latent skill.",
        "timestamp": "2025-10-21T07:42:12.031Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "evaluation"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 428,
        "object_id": "paper:openreview.2dl6upRXZj",
        "created_at": "2025-10-21T07:42:12+00:00",
        "updated_at": "2025-10-21T07:42:39+00:00",
        "version": 1
      }
    },
    "interactions:openreview.x0qJo7SPhs": {
      "data": {
        "sourceId": "openreview",
        "paperId": "x0qJo7SPhs",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T10:33:46.532Z",
            "data": {
              "session_id": "session_1761042825916_mlo0880",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-21T10:32:00.204Z",
              "end_time": "2025-10-21T10:33:45.916Z",
              "heartbeat_count": 21,
              "duration_seconds": 105,
              "idle_seconds": 1,
              "total_elapsed_seconds": 106
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T07:58:32.912Z",
            "data": {
              "session_id": "session_1761638312905_ap7tg07",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T07:58:26.595Z",
              "end_time": "2025-10-28T07:58:32.905Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T08:01:52.875Z",
            "data": {
              "session_id": "session_1761638512857_1bzc9uz",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T08:01:39.208Z",
              "end_time": "2025-10-28T08:01:52.857Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T10:45:18.191Z",
            "data": {
              "session_id": "session_1761648318174_02z670b",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T10:45:11.553Z",
              "end_time": "2025-10-28T10:45:18.174Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T10:46:48.423Z",
            "data": {
              "session_id": "session_1761648408385_n2v59hb",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T10:46:42.043Z",
              "end_time": "2025-10-28T10:46:48.385Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T10:51:53.035Z",
            "data": {
              "session_id": "session_1761648713008_0ujc2ii",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T10:51:45.499Z",
              "end_time": "2025-10-28T10:51:53.008Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T10:57:34.928Z",
            "data": {
              "session_id": "session_1761649054922_gx6g86s",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T10:56:39.134Z",
              "end_time": "2025-10-28T10:57:34.922Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 1,
              "total_elapsed_seconds": 56
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T10:59:30.102Z",
            "data": {
              "session_id": "session_1761649170077_8uryvu2",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T10:59:18.403Z",
              "end_time": "2025-10-28T10:59:30.077Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T11:01:07.750Z",
            "data": {
              "session_id": "session_1761649267441_a7p85iz",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T11:01:01.566Z",
              "end_time": "2025-10-28T11:01:07.441Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T11:03:37.010Z",
            "data": {
              "session_id": "session_1761649416996_ojsdfvp",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T11:03:31.474Z",
              "end_time": "2025-10-28T11:03:36.996Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T12:39:40.914Z",
            "data": {
              "session_id": "session_1761655180601_rh21876",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T12:39:30.920Z",
              "end_time": "2025-10-28T12:39:40.601Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T12:49:52.336Z",
            "data": {
              "session_id": "session_1761655792327_kcwdneo",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T12:49:03.912Z",
              "end_time": "2025-10-28T12:49:52.327Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 3,
              "total_elapsed_seconds": 48
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T12:55:32.840Z",
            "data": {
              "session_id": "session_1761656132414_uk9olpk",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T12:54:37.897Z",
              "end_time": "2025-10-28T12:55:32.414Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 5,
              "total_elapsed_seconds": 55
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T13:05:48.282Z",
            "data": {
              "session_id": "session_1761656747071_mb5rhc6",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T12:59:29.421Z",
              "end_time": "2025-10-28T13:05:47.071Z",
              "heartbeat_count": 75,
              "duration_seconds": 375,
              "idle_seconds": 3,
              "total_elapsed_seconds": 378
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T13:20:23.416Z",
            "data": {
              "session_id": "session_1761657622214_hr64x13",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-28T13:19:27.974Z",
              "end_time": "2025-10-28T13:20:22.214Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 4,
              "total_elapsed_seconds": 54
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T06:44:48.936Z",
            "data": {
              "session_id": "session_1761720288927_wwjt0gh",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-29T06:44:28.710Z",
              "end_time": "2025-10-29T06:44:48.927Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T07:17:08.419Z",
            "data": {
              "session_id": "session_1761722227805_w52s4v1",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-29T07:13:59.747Z",
              "end_time": "2025-10-29T07:17:07.805Z",
              "heartbeat_count": 37,
              "duration_seconds": 185,
              "idle_seconds": 3,
              "total_elapsed_seconds": 188
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T07:22:09.619Z",
            "data": {
              "session_id": "session_1761722528953_9h8q2wx",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-29T07:17:08.656Z",
              "end_time": "2025-10-29T07:22:08.953Z",
              "heartbeat_count": 60,
              "duration_seconds": 300,
              "idle_seconds": 0,
              "total_elapsed_seconds": 300
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T07:28:09.628Z",
            "data": {
              "session_id": "session_1761722889311_l8y9gzm",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-29T07:28:00.255Z",
              "end_time": "2025-10-29T07:28:09.311Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-31T08:57:59.505Z",
            "data": {
              "session_id": "session_1761901079198_3xlrqfa",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-10-31T08:57:15.195Z",
              "end_time": "2025-10-31T08:57:59.198Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 4,
              "total_elapsed_seconds": 44
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T09:15:47.940Z",
            "data": {
              "session_id": "session_1764062147468_bo453am",
              "source_id": "openreview",
              "paper_id": "x0qJo7SPhs",
              "start_time": "2025-11-25T09:15:15.590Z",
              "end_time": "2025-11-25T09:15:47.468Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "issue_number": 433,
        "object_id": "interactions:openreview.x0qJo7SPhs",
        "created_at": "2025-10-21T10:33:47+00:00",
        "updated_at": "2025-11-25T09:16:21+00:00",
        "version": 1
      }
    },
    "paper:openreview.x0qJo7SPhs": {
      "data": {
        "sourceId": "openreview",
        "paperId": "x0qJo7SPhs",
        "url": "https://openreview.net/forum?id=x0qJo7SPhs&noteId=I1sqFrii4Y",
        "title": "How Many Images Does It Take? Estimating Imitation Thresholds in Text-to-Image Models",
        "authors": "Sahil Verma, Royi Rassin, Arnav Mohanty Das, Gantavya Bhatt, Preethi Seshadri, Chirag Shah, Jeff Bilmes, Hannaneh Hajishirzi, Yanai Elazar",
        "abstract": "Text-to-image models are trained using large datasets of image-text pairs collected from the internet. These datasets often include copyrighted and private images. Training models on such datasets enables them to generate images that might violate copyright laws and\nindividual privacy. This phenomenon is termed imitation \u2013 generation of images with content that has recognizable similarity to its training images. In this work we estimate the point at which a model was trained on enough instances of a concept to be able to imitate it \u2013 the imitation threshold. We posit this question as a new problem and propose an efficient approach that estimates the imitation threshold without incurring the colossal cost of training these models from scratch. We experiment with two domains \u2013 human faces and art styles, and evaluate four text-to-image models that were trained on three pretraining datasets. We estimate the imitation threshold of these models to be in the range of 200-700 images, depending on the domain and the model. The imitation threshold provides an empirical basis for copyright violation claims and acts as a guiding principle for text-to-image model developers that aim to comply with copyright and privacy laws.",
        "timestamp": "2025-10-21T10:31:58.368Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Under review for TMLR",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 431,
        "object_id": "paper:openreview.x0qJo7SPhs",
        "created_at": "2025-10-21T10:31:58+00:00",
        "updated_at": "2025-10-21T10:32:28+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.13109": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.13109",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T10:44:51.049Z",
            "data": {
              "session_id": "session_1761043491000_lfra5x9",
              "source_id": "arxiv",
              "paper_id": "2506.13109",
              "start_time": "2025-10-21T10:44:23.937Z",
              "end_time": "2025-10-21T10:44:51.000Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          }
        ]
      },
      "meta": {
        "issue_number": 437,
        "object_id": "interactions:arxiv.2506.13109",
        "created_at": "2025-10-21T10:44:23+00:00",
        "updated_at": "2025-10-21T10:44:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.13109": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.13109",
        "url": "https://arxiv.org/pdf/2506.13109",
        "title": "Leveraging In-Context Learning for Language Model Agents",
        "authors": "Shivanshu Gupta, Sameer Singh, Ashish Sabharwal, Tushar Khot, Ben Bogin",
        "abstract": "In-context learning (ICL) with dynamically selected demonstrations combines\nthe flexibility of prompting large language models (LLMs) with the ability to\nleverage training data to improve performance. While ICL has been highly\nsuccessful for prediction and generation tasks, leveraging it for agentic tasks\nthat require sequential decision making is challenging -- one must think not\nonly about how to annotate long trajectories at scale and how to select\ndemonstrations, but also what constitutes demonstrations, and when and where to\nshow them. To address this, we first propose an algorithm that leverages an LLM\nwith retries along with demonstrations to automatically and efficiently\nannotate agentic tasks with solution trajectories. We then show that\nset-selection of trajectories of similar tasks as demonstrations significantly\nimproves performance, reliability, robustness, and efficiency of LLM agents.\nHowever, trajectory demonstrations have a large inference cost overhead. We\nshow that this can be mitigated by using small trajectory snippets at every\nstep instead of an additional trajectory. We find that demonstrations obtained\nfrom larger models (in the annotation phase) also improve smaller models, and\nthat ICL agents can even rival costlier trained agents. Thus, our results\nreveal that ICL, with careful use, can be very powerful for agentic tasks as\nwell.",
        "timestamp": "2025-10-21T10:43:56.923Z",
        "rating": "novote",
        "publishedDate": "2025-06-16T05:37:49Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 436,
        "object_id": "paper:arxiv.2506.13109",
        "created_at": "2025-10-21T10:43:57+00:00",
        "updated_at": "2025-10-21T10:44:22+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1903.00161": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1903.00161",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T10:43:27.019Z",
            "data": {
              "session_id": "session_1761043406328_4x3igia",
              "source_id": "arxiv",
              "paper_id": "1903.00161",
              "start_time": "2025-10-21T10:43:04.184Z",
              "end_time": "2025-10-21T10:43:26.328Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "issue_number": 435,
        "object_id": "interactions:arxiv.1903.00161",
        "created_at": "2025-10-21T10:43:27+00:00",
        "updated_at": "2025-10-21T10:43:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1903.00161": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1903.00161",
        "url": "https://arxiv.org/pdf/1903.00161",
        "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning\n  Over Paragraphs",
        "authors": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner",
        "abstract": "Reading comprehension has recently seen rapid progress, with systems matching\nhumans on the most popular datasets for the task. However, a large body of work\nhas highlighted the brittleness of these systems, showing that there is much\nwork left to be done. We introduce a new English reading comprehension\nbenchmark, DROP, which requires Discrete Reasoning Over the content of\nParagraphs. In this crowdsourced, adversarially-created, 96k-question\nbenchmark, a system must resolve references in a question, perhaps to multiple\ninput positions, and perform discrete operations over them (such as addition,\ncounting, or sorting). These operations require a much more comprehensive\nunderstanding of the content of paragraphs than what was necessary for prior\ndatasets. We apply state-of-the-art methods from both the reading comprehension\nand semantic parsing literature on this dataset and show that the best systems\nonly achieve 32.7% F1 on our generalized accuracy metric, while expert human\nperformance is 96.0%. We additionally present a new model that combines reading\ncomprehension methods with simple numerical reasoning to achieve 47.0% F1.",
        "timestamp": "2025-10-21T10:43:04.506Z",
        "rating": "novote",
        "publishedDate": "2019-03-01T05:32:01Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 434,
        "object_id": "paper:arxiv.1903.00161",
        "created_at": "2025-10-21T10:43:04+00:00",
        "updated_at": "2025-10-21T10:43:33+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.20273": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.20273",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T11:35:17.402Z",
            "data": {
              "session_id": "session_1761046516698_5nexi69",
              "source_id": "arxiv",
              "paper_id": "2502.20273",
              "start_time": "2025-10-21T11:35:04.666Z",
              "end_time": "2025-10-21T11:35:16.697Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "issue_number": 439,
        "object_id": "interactions:arxiv.2502.20273",
        "created_at": "2025-10-21T11:35:18+00:00",
        "updated_at": "2025-10-21T11:35:49+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.20273": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.20273",
        "url": "https://arxiv.org/abs/2502.20273",
        "title": "How Much is Enough? The Diminishing Returns of Tokenization Training Data",
        "authors": "Varshini Reddy, Craig W. Schmidt, Yuval Pinter, Chris Tanner",
        "abstract": "Tokenization, a crucial initial step in natural language processing, is governed by several key parameters, such as the tokenization algorithm, vocabulary size, pre-tokenization strategy, inference strategy, and training data corpus. This paper investigates the impact of an often-overlooked hyperparameter, tokenizer training data size. We train BPE, UnigramLM, and WordPiece tokenizers across various vocabulary sizes using English training data ranging from 1GB to 900GB. Our findings reveal diminishing returns as training data size increases beyond roughly 150GB, suggesting a practical limit to the improvements in tokenization quality achievable through additional data. We analyze this phenomenon and attribute the saturation effect to constraints introduced by the pre-tokenization stage. We then demonstrate the extent to which these findings can generalize by experimenting on data in Russian, a language typologically distant from English. For Russian text, we observe diminishing returns after training a tokenizer from 200GB of data, which is approximately 33% more than when training on English. These results provide valuable insights for optimizing the tokenization process by reducing the compute required for training on large corpora and suggest promising directions for future research in tokenization algorithms.",
        "timestamp": "2025-10-21T11:35:05.068Z",
        "rating": "novote",
        "publishedDate": "2025/02/27",
        "tags": [
          "Computation and Language (cs.CL)",
          "Computational Engineering",
          "Finance",
          "and Science (cs.CE)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 438,
        "object_id": "paper:arxiv.2502.20273",
        "created_at": "2025-10-21T11:35:05+00:00",
        "updated_at": "2025-10-21T11:35:33+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.15020": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.15020",
        "url": "https://arxiv.org/abs/2510.15020",
        "title": "The Coverage Principle: How Pre-training Enables Post-Training",
        "authors": "Fan Chen, Audrey Huang, Noah Golowich, Sadhika Malladi, Adam Block, Jordan T. Ash, Akshay Krishnamurthy, Dylan J. Foster",
        "abstract": "Language models demonstrate remarkable abilities when pre-trained on large text corpora and fine-tuned for specific tasks, but how and why pre-training shapes the success of the final model remains poorly understood. Notably, although pre-training success is often quantified by cross entropy loss, cross-entropy can be a poor predictor of downstream performance. Instead, we provide a theoretical perspective on this relationship through the lens of \\emph{coverage}, which quantifies the probability mass the pre-trained model places on high-quality responses and which is necessary and sufficient for post-training and test-time scaling methods such as Best-of-N to succeed. Our main results develop an understanding of \\emph{the coverage principle}, a phenomenon whereby next-token prediction implicitly optimizes toward a model with good coverage. In particular, we uncover a mechanism that explains the power of coverage in predicting downstream performance: \\emph{coverage generalizes faster than cross entropy}, avoiding spurious dependence on problem-dependent parameters such as the sequence length. We also study practical algorithmic interventions with provable benefits for improving coverage, including (i) model/checkpoint selection procedures, (ii) gradient normalization schemes, and (iii) test-time decoding strategies.",
        "timestamp": "2025-10-21T13:08:29.781Z",
        "rating": "novote",
        "publishedDate": "2025/10/16",
        "tags": [
          "Machine Learning (stat.ML)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)",
          "Statistics Theory (math.ST)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 440,
        "object_id": "paper:arxiv.2510.15020",
        "created_at": "2025-10-21T13:08:30+00:00",
        "updated_at": "2025-10-21T13:09:03+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.16096": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.16096",
        "url": "https://arxiv.org/abs/2510.16096",
        "title": "Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization",
        "authors": "Tina Behnia, Puneesh Deora, Christos Thrampoulidis",
        "abstract": "Language models are pretrained on sequences that blend statistical regularities (making text fluent) with factual associations between specific tokens (knowledge of facts). While recent work suggests that the variability of their interaction, such as paraphrases of factual associations, critically determines generalization ability, we lack a systematic analysis of these impacts. This paper introduces a flexible synthetic testbed that combines a statistical stream of generic tokens with an abstract factual stream of source-target token pairs, enabling fine-grained control over their interaction. The design enables the independent control of diversity nature by manipulating stream composition (contextual structure) and the diversity level by varying which statistical streams each fact appears in. Through controlled experiments, we find that while higher contextual diversity delays in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD) factual generalization depends critically on contextual structure. In some cases, OOD performance follows the same trend as ID, but in others, diversity becomes essential for non-trivial factual recall. Even when low diversity prohibits factual recall, optimal diversity levels depend on training duration. Beyond factual recall failures, we identify structures where statistical generalization fails independently, and others where both capabilities degrade. This shows how the interplay between contextual design and diversity level impacts different generalization aspects. Further, through a series of controlled interventions on the model components, we trace the OOD failures to distinct optimization bottlenecks, highlighting the importance of the embedding and unembedding layers. Our synthetic framework allows us to isolate effects that would be confounded in large-scale studies, offering a controlled testbed for future investigations.",
        "timestamp": "2025-10-21T13:43:17.855Z",
        "rating": "novote",
        "publishedDate": "2025/10/17",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 445,
        "object_id": "paper:arxiv.2510.16096",
        "created_at": "2025-10-21T13:43:18+00:00",
        "updated_at": "2025-10-21T13:43:48+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2106.02736": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2106.02736",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T13:43:04.794Z",
            "data": {
              "session_id": "session_1761054184751_4d7s0cg",
              "source_id": "arxiv",
              "paper_id": "2106.02736",
              "start_time": "2025-10-21T13:42:58.495Z",
              "end_time": "2025-10-21T13:43:04.751Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 444,
        "object_id": "interactions:arxiv.2106.02736",
        "created_at": "2025-10-21T13:43:05+00:00",
        "updated_at": "2025-10-21T13:43:33+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2106.02736": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2106.02736",
        "url": "https://arxiv.org/abs/2106.02736",
        "title": "Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings",
        "authors": "Kartik Goyal, Chris Dyer, Taylor Berg-Kirkpatrick",
        "abstract": "While recent work has shown that scores from models trained by the ubiquitous masked language modeling (MLM) objective effectively discriminate probable from improbable sequences, it is still an open question if these MLMs specify a principled probability distribution over the space of possible sequences. In this paper, we interpret MLMs as energy-based sequence models and propose two energy parametrizations derivable from the trained MLMs. In order to draw samples correctly from these models, we develop a tractable sampling scheme based on the Metropolis--Hastings Monte Carlo algorithm. In our approach, samples are proposed from the same masked conditionals used for training the masked language models, and they are accepted or rejected based on their energy values according to the target distribution. We validate the effectiveness of the proposed parametrizations by exploring the quality of samples drawn from these energy-based models for both open-ended unconditional generation and a conditional generation task of machine translation. We theoretically and empirically justify our sampling algorithm by showing that the masked conditionals on their own do not yield a Markov chain whose stationary distribution is that of our target distribution, and our approach generates higher quality samples than other recently proposed undirected generation approaches (Wang et al., 2019, Ghazvininejad et al., 2019).",
        "timestamp": "2025-10-21T13:42:54.362Z",
        "rating": "novote",
        "publishedDate": "2021/06/04",
        "tags": [
          "Machine Learning (cs.LG)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 443,
        "object_id": "paper:arxiv.2106.02736",
        "created_at": "2025-10-21T13:42:54+00:00",
        "updated_at": "2025-10-21T13:43:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1902.04094": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1902.04094",
        "url": "https://arxiv.org/abs/1902.04094",
        "title": "BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model",
        "authors": "Alex Wang, Kyunghyun Cho",
        "abstract": "We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from BERT. We generate from BERT and find that it can produce high-quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.",
        "timestamp": "2025-10-21T13:42:53.997Z",
        "rating": "novote",
        "publishedDate": "2019/02/11",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 442,
        "object_id": "paper:arxiv.1902.04094",
        "created_at": "2025-10-21T13:42:54+00:00",
        "updated_at": "2025-10-21T13:43:25+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.16096": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.16096",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T13:47:42.406Z",
            "data": {
              "session_id": "session_1761054462399_s3x2v8m",
              "source_id": "arxiv",
              "paper_id": "2510.16096",
              "start_time": "2025-10-21T13:47:34.937Z",
              "end_time": "2025-10-21T13:47:42.399Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T18:44:48.521Z",
            "data": {
              "session_id": "session_1761072288495_rfftubt",
              "source_id": "arxiv",
              "paper_id": "2510.16096",
              "start_time": "2025-10-21T18:44:41.598Z",
              "end_time": "2025-10-21T18:44:48.495Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T06:19:09.866Z",
            "data": {
              "session_id": "session_1761200349862_0mmxb7m",
              "source_id": "arxiv",
              "paper_id": "2510.16096",
              "start_time": "2025-10-23T06:19:04.853Z",
              "end_time": "2025-10-23T06:19:09.862Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:25:52.047Z",
            "data": {
              "session_id": "session_1761809151737_coyecqt",
              "source_id": "arxiv",
              "paper_id": "2510.16096",
              "start_time": "2025-10-30T07:25:43.882Z",
              "end_time": "2025-10-30T07:25:51.736Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T11:26:03.016Z",
            "data": {
              "session_id": "session_1764069962708_xlhhdoa",
              "source_id": "arxiv",
              "paper_id": "2510.16096",
              "start_time": "2025-11-25T11:25:57.335Z",
              "end_time": "2025-11-25T11:26:02.708Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 446,
        "object_id": "interactions:arxiv.2510.16096",
        "created_at": "2025-10-21T13:47:17+00:00",
        "updated_at": "2025-11-25T11:26:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.05228": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.05228",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-21T16:33:26.384Z",
            "data": {
              "session_id": "session_1761064406008_vil753l",
              "source_id": "arxiv",
              "paper_id": "2504.05228",
              "start_time": "2025-10-21T16:33:19.853Z",
              "end_time": "2025-10-21T16:33:26.008Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T15:52:19.505Z",
            "data": {
              "session_id": "session_1762012339484_zj9je6z",
              "source_id": "arxiv",
              "paper_id": "2504.05228",
              "start_time": "2025-11-01T15:52:06.530Z",
              "end_time": "2025-11-01T15:52:19.484Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T16:25:03.994Z",
            "data": {
              "session_id": "session_1764087903676_61ioy71",
              "source_id": "arxiv",
              "paper_id": "2504.05228",
              "start_time": "2025-11-25T16:24:58.000Z",
              "end_time": "2025-11-25T16:25:03.676Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 447,
        "object_id": "interactions:arxiv.2504.05228",
        "created_at": "2025-10-21T16:33:27+00:00",
        "updated_at": "2025-11-25T16:25:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2305.16938": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.16938",
        "url": "https://arxiv.org/abs/2305.16938",
        "title": "Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation",
        "authors": "Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, Yanai Elazar",
        "abstract": "Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations. Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge.",
        "timestamp": "2025-10-21T16:44:09.337Z",
        "rating": "novote",
        "publishedDate": "2023/05/26",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 448,
        "object_id": "paper:arxiv.2305.16938",
        "created_at": "2025-10-21T16:44:09+00:00",
        "updated_at": "2025-10-21T16:44:36+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2308.09124": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2308.09124",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-22T07:16:32.915Z",
            "data": {
              "session_id": "session_1761117392194_ckodd03",
              "source_id": "arxiv",
              "paper_id": "2308.09124",
              "start_time": "2025-10-22T07:16:22.503Z",
              "end_time": "2025-10-22T07:16:32.194Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:43:37.682Z",
            "data": {
              "session_id": "session_1762375417102_0w0nrbs",
              "source_id": "arxiv",
              "paper_id": "2308.09124",
              "start_time": "2025-11-05T20:41:21.975Z",
              "end_time": "2025-11-05T20:43:37.102Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 0,
              "total_elapsed_seconds": 135
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T07:00:49.532Z",
            "data": {
              "session_id": "session_1762758048799_r54x920",
              "source_id": "arxiv",
              "paper_id": "2308.09124",
              "start_time": "2025-11-10T06:54:42.747Z",
              "end_time": "2025-11-10T07:00:48.799Z",
              "heartbeat_count": 73,
              "duration_seconds": 365,
              "idle_seconds": 1,
              "total_elapsed_seconds": 366
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T07:03:10.615Z",
            "data": {
              "session_id": "session_1762758189847_ydncdmo",
              "source_id": "arxiv",
              "paper_id": "2308.09124",
              "start_time": "2025-11-10T07:01:56.455Z",
              "end_time": "2025-11-10T07:03:09.847Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 3,
              "total_elapsed_seconds": 73
            }
          }
        ]
      },
      "meta": {
        "issue_number": 452,
        "object_id": "interactions:arxiv.2308.09124",
        "created_at": "2025-10-22T07:16:33+00:00",
        "updated_at": "2025-11-10T07:03:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2308.09124": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2308.09124",
        "url": "https://arxiv.org/pdf/2308.09124",
        "title": "Linearity of Relation Decoding in Transformer Language Models",
        "authors": "Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, David Bau",
        "abstract": "Much of the knowledge encoded in transformer language models (LMs) may be\nexpressed in terms of relations: relations between words and their synonyms,\nentities and their attributes, etc. We show that, for a subset of relations,\nthis computation is well-approximated by a single linear transformation on the\nsubject representation. Linear relation representations may be obtained by\nconstructing a first-order approximation to the LM from a single prompt, and\nthey exist for a variety of factual, commonsense, and linguistic relations.\nHowever, we also identify many cases in which LM predictions capture relational\nknowledge accurately, but this knowledge is not linearly encoded in their\nrepresentations. Our results thus reveal a simple, interpretable, but\nheterogeneously deployed knowledge representation strategy in transformer LMs.",
        "timestamp": "2025-10-22T07:16:23.050Z",
        "rating": "novote",
        "publishedDate": "2023-08-17T17:59:19Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 451,
        "object_id": "paper:arxiv.2308.09124",
        "created_at": "2025-10-22T07:16:23+00:00",
        "updated_at": "2025-10-22T07:16:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.11501": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.11501",
        "url": "https://arxiv.org/pdf/2310.11501",
        "title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
        "authors": "Myra Cheng, Tiziano Piccardi, Diyi Yang",
        "abstract": "Recent work has aimed to capture nuances of human behavior by using LLMs to\nsimulate responses from particular demographics in settings like social science\nexperiments and public opinion surveys. However, there are currently no\nestablished ways to discuss or evaluate the quality of such LLM simulations.\nMoreover, there is growing concern that these LLM simulations are flattened\ncaricatures of the personas that they aim to simulate, failing to capture the\nmultidimensionality of people and perpetuating stereotypes. To bridge these\ngaps, we present CoMPosT, a framework to characterize LLM simulations using\nfour dimensions: Context, Model, Persona, and Topic. We use this framework to\nmeasure open-ended LLM simulations' susceptibility to caricature, defined via\ntwo criteria: individuation and exaggeration. We evaluate the level of\ncaricature in scenarios from existing work on LLM simulations. We find that for\nGPT-4, simulations of certain demographics (political and marginalized groups)\nand topics (general, uncontroversial) are highly susceptible to caricature.",
        "timestamp": "2025-10-22T08:57:11.460Z",
        "rating": "novote",
        "publishedDate": "2023-10-17T18:00:25Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 453,
        "object_id": "paper:arxiv.2310.11501",
        "created_at": "2025-10-22T08:57:11+00:00",
        "updated_at": "2025-10-22T08:57:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2210.09404": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.09404",
        "url": "https://arxiv.org/abs/2210.09404",
        "title": "Measures of Information Reflect Memorization Patterns",
        "authors": "Rachit Bansal, Danish Pruthi, Yonatan Belinkov",
        "abstract": "Neural networks are known to exploit spurious artifacts (or shortcuts) that co-occur with a target label, exhibiting heuristic memorization. On the other hand, networks have been shown to memorize training examples, resulting in example-level memorization. These kinds of memorization impede generalization of networks beyond their training distributions. Detecting such memorization could be challenging, often requiring researchers to curate tailored test sets. In this work, we hypothesize -- and subsequently show -- that the diversity in the activation patterns of different neurons is reflective of model generalization and memorization. We quantify the diversity in the neural activations through information-theoretic measures and find support for our hypothesis on experiments spanning several natural language and vision tasks. Importantly, we discover that information organization points to the two forms of memorization, even for neural activations computed on unlabelled in-distribution examples. Lastly, we demonstrate the utility of our findings for the problem of model selection. The associated code and other resources for this work are available at this https URL.",
        "timestamp": "2025-10-23T06:19:44.914Z",
        "rating": "novote",
        "publishedDate": "2022/10/17",
        "tags": [
          "Machine Learning (cs.LG)",
          "Information Theory (cs.IT)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 454,
        "object_id": "paper:arxiv.2210.09404",
        "created_at": "2025-10-23T06:19:45+00:00",
        "updated_at": "2025-10-23T06:20:17+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2210.09404": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.09404",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T06:21:21.296Z",
            "data": {
              "session_id": "session_1761200480680_uk1c4o3",
              "source_id": "arxiv",
              "paper_id": "2210.09404",
              "start_time": "2025-10-23T06:19:59.036Z",
              "end_time": "2025-10-23T06:21:20.680Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 2,
              "total_elapsed_seconds": 82
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T06:44:28.277Z",
            "data": {
              "session_id": "session_1761201867585_k2ylm0t",
              "source_id": "arxiv",
              "paper_id": "2210.09404",
              "start_time": "2025-10-23T06:42:06.075Z",
              "end_time": "2025-10-23T06:44:27.585Z",
              "heartbeat_count": 28,
              "duration_seconds": 140,
              "idle_seconds": 2,
              "total_elapsed_seconds": 142
            }
          }
        ]
      },
      "meta": {
        "issue_number": 455,
        "object_id": "interactions:arxiv.2210.09404",
        "created_at": "2025-10-23T06:21:22+00:00",
        "updated_at": "2025-10-23T06:44:52+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.14777": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.14777",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T06:51:52.783Z",
            "data": {
              "session_id": "session_1761202312760_ffr0wh4",
              "source_id": "arxiv",
              "paper_id": "2507.14777",
              "start_time": "2025-10-23T06:51:24.230Z",
              "end_time": "2025-10-23T06:51:52.760Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          }
        ]
      },
      "meta": {
        "issue_number": 457,
        "object_id": "interactions:arxiv.2507.14777",
        "created_at": "2025-10-23T06:51:20+00:00",
        "updated_at": "2025-10-23T06:52:04+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.14777": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.14777",
        "url": "https://arxiv.org/abs/2507.14777",
        "title": "Rethinking Memorization Measures and their Implications in Large Language Models",
        "authors": "Bishwamittra Ghosh, Soumi Das, Qinyuan Wu, Mohammad Aflah Khan, Krishna P. Gummadi, Evimaria Terzi, Deepak Garg",
        "abstract": "Concerned with privacy threats, memorization in LLMs is often seen as undesirable, specifically for learning. In this paper, we study whether memorization can be avoided when optimally learning a language, and whether the privacy threat posed by memorization is exaggerated or not. To this end, we re-examine existing privacy-focused measures of memorization, namely recollection-based and counterfactual memorization, along with a newly proposed contextual memorization.\nRelating memorization to local over-fitting during learning, contextual memorization aims to disentangle memorization from the contextual learning ability of LLMs. Informally, a string is contextually memorized if its recollection due to training exceeds the optimal contextual recollection, a learned threshold denoting the best contextual learning without training. Conceptually, contextual recollection avoids the fallacy of recollection-based memorization, where any form of high recollection is a sign of memorization. Theoretically, contextual memorization relates to counterfactual memorization, but imposes stronger conditions. Memorization measures differ in outcomes and information requirements.\nExperimenting on 18 LLMs from 6 families and multiple formal languages of different entropy, we show that (a) memorization measures disagree on memorization order of varying frequent strings, (b) optimal learning of a language cannot avoid partial memorization of training strings, and (c) improved learning decreases contextual and counterfactual memorization but increases recollection-based memorization. Finally, (d) we revisit existing reports of memorized strings by recollection that neither pose a privacy threat nor are contextually or counterfactually memorized.",
        "timestamp": "2025-10-23T06:51:02.624Z",
        "rating": "novote",
        "publishedDate": "2025/07/20",
        "tags": [
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 456,
        "object_id": "paper:arxiv.2507.14777",
        "created_at": "2025-10-23T06:51:03+00:00",
        "updated_at": "2025-10-23T06:51:29+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2310.10683": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.10683",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T06:55:23.866Z",
            "data": {
              "session_id": "session_1761202523842_a1ter1t",
              "source_id": "arxiv",
              "paper_id": "2310.10683",
              "start_time": "2025-10-23T06:54:40.144Z",
              "end_time": "2025-10-23T06:55:23.842Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 4,
              "total_elapsed_seconds": 44
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T06:57:00.606Z",
            "data": {
              "session_id": "session_1761202620558_doq9ili",
              "source_id": "arxiv",
              "paper_id": "2310.10683",
              "start_time": "2025-10-23T06:56:08.962Z",
              "end_time": "2025-10-23T06:57:00.558Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 2,
              "total_elapsed_seconds": 52
            }
          }
        ]
      },
      "meta": {
        "issue_number": 461,
        "object_id": "interactions:arxiv.2310.10683",
        "created_at": "2025-10-23T06:54:41+00:00",
        "updated_at": "2025-10-23T06:57:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.10683": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.10683",
        "url": "https://arxiv.org/abs/2310.10683",
        "title": "Large Language Model Unlearning",
        "authors": "Yuanshun Yao, Xiaojun Xu, Yang Liu",
        "abstract": "We study how to perform unlearning, i.e. forgetting undesirable misbehaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.",
        "timestamp": "2025-10-23T06:54:33.369Z",
        "rating": "novote",
        "publishedDate": "2023/10/14",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 460,
        "object_id": "paper:arxiv.2310.10683",
        "created_at": "2025-10-23T06:54:33+00:00",
        "updated_at": "2025-10-23T06:55:02+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1912.03817": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1912.03817",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T06:54:20.104Z",
            "data": {
              "session_id": "session_1761202460044_x76o39c",
              "source_id": "arxiv",
              "paper_id": "1912.03817",
              "start_time": "2025-10-23T06:54:04.864Z",
              "end_time": "2025-10-23T06:54:20.044Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 459,
        "object_id": "interactions:arxiv.1912.03817",
        "created_at": "2025-10-23T06:54:21+00:00",
        "updated_at": "2025-10-23T06:54:51+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1912.03817": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1912.03817",
        "url": "https://arxiv.org/abs/1912.03817",
        "title": "Machine Unlearning",
        "authors": "Lucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, Nicolas Papernot",
        "abstract": "Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult. We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning. Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63x, and 2.45x for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36x in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.",
        "timestamp": "2025-10-23T06:53:45.633Z",
        "rating": "novote",
        "publishedDate": "2019/12/09",
        "tags": [
          "Cryptography and Security (cs.CR)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 458,
        "object_id": "paper:arxiv.1912.03817",
        "created_at": "2025-10-23T06:53:46+00:00",
        "updated_at": "2025-10-23T06:54:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2401.17377": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.17377",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-16T12:34:33.797Z",
            "data": {
              "session_id": "session_1763296473730_hiuq4d0",
              "source_id": "arxiv",
              "paper_id": "2401.17377",
              "start_time": "2025-11-16T12:33:58.506Z",
              "end_time": "2025-11-16T12:34:33.730Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 0,
              "total_elapsed_seconds": 35
            }
          }
        ]
      },
      "meta": {
        "issue_number": 464,
        "object_id": "interactions:arxiv.2401.17377",
        "created_at": "2025-10-23T07:07:35+00:00",
        "updated_at": "2025-11-16T12:35:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2401.17377": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.17377",
        "url": "https://arxiv.org/abs/2401.17377",
        "title": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens",
        "authors": "Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, Hannaneh Hajishirzi",
        "abstract": "Are $n$-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we showcase their values in both text analysis and improving neural LLMs. This was done by modernizing $n$-gram LMs in two aspects. First, we train them at the same data scale as neural LLMs -- 5 trillion tokens. This is the largest $n$-gram LM ever built. Second, existing $n$-gram LMs use small $n$ which hinders their performance; we instead allow $n$ to be arbitrarily large, by introducing a new $\\infty$-gram LM with backoff. Instead of pre-computing $n$-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\\infty$-gram (as well as $n$-gram with arbitrary $n$) probabilities with millisecond-level latency. The $\\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\\infty$-gram LM has fairly high accuracy for next-token prediction (47%), and can complement neural LLMs to greatly reduce their perplexity. When analyzing machine-generated text, we also observe irregularities in the machine--$\\infty$-gram agreement level with respect to the suffix length, which indicates deficiencies in neural LLM pretraining and the positional embeddings of Transformers.",
        "timestamp": "2025-10-23T07:07:10.479Z",
        "rating": "novote",
        "publishedDate": "2024/01/30",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Information Retrieval (cs.IR)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 462,
        "object_id": "paper:arxiv.2401.17377",
        "created_at": "2025-10-23T07:07:10+00:00",
        "updated_at": "2025-10-23T07:07:40+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.12590": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.12590",
        "url": "https://arxiv.org/pdf/2404.12590",
        "title": "The Files are in the Computer: On Copyright, Memorization, and\n  Generative AI",
        "authors": "A. Feder Cooper, James Grimmelmann",
        "abstract": "The New York Times's copyright lawsuit against OpenAI and Microsoft alleges\nOpenAI's GPT models have \"memorized\" NYT articles. Other lawsuits make similar\nclaims. But parties, courts, and scholars disagree on what memorization is,\nwhether it is taking place, and what its copyright implications are. These\ndebates are clouded by ambiguities over the nature of \"memorization.\" We\nattempt to bring clarity to the conversation. We draw on the technical\nliterature to provide a firm foundation for legal discussions, providing a\nprecise definition of memorization: a model has \"memorized\" a piece of training\ndata when (1) it is possible to reconstruct from the model (2) a near-exact\ncopy of (3) a substantial portion of (4) that piece of training data. We\ndistinguish memorization from \"extraction\" (user intentionally causes a model\nto generate a near-exact copy), from \"regurgitation\" (model generates a\nnear-exact copy, regardless of user intentions), and from \"reconstruction\" (the\nnear-exact copy can be obtained from the model by any means). Several\nconsequences follow. (1) Not all learning is memorization. (2) Memorization\noccurs when a model is trained; regurgitation is a symptom not its cause. (3) A\nmodel that has memorized training data is a \"copy\" of that training data in the\nsense used by copyright. (4) A model is not like a VCR or other general-purpose\ncopying technology; it is better at generating some types of outputs (possibly\nregurgitated ones) than others. (5) Memorization is not a phenomenon caused by\n\"adversarial\" users bent on extraction; it is latent in the model itself. (6)\nThe amount of training data that a model memorizes is a consequence of choices\nmade in training. (7) Whether or not a model that has memorized actually\nregurgitates depends on overall system design. In a very real sense, memorized\ntraining data is in the model--to quote Zoolander, the files are in the\ncomputer.",
        "timestamp": "2025-10-23T07:09:33.514Z",
        "rating": "novote",
        "publishedDate": "2024-04-19T02:37:09Z",
        "tags": [
          "cs.CY"
        ],
        "doi": "",
        "journalName": "Chicago-Kent Law Review Vol. 100, 2025",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 465,
        "object_id": "paper:arxiv.2404.12590",
        "created_at": "2025-10-23T07:09:33+00:00",
        "updated_at": "2025-10-23T07:10:02+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.12590": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.12590",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T07:10:39.467Z",
            "data": {
              "session_id": "session_1761203439451_71wr1v1",
              "source_id": "arxiv",
              "paper_id": "2404.12590",
              "start_time": "2025-10-23T07:10:31.249Z",
              "end_time": "2025-10-23T07:10:39.451Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 466,
        "object_id": "interactions:arxiv.2404.12590",
        "created_at": "2025-10-23T07:10:29+00:00",
        "updated_at": "2025-10-23T07:11:00+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2412.06966": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.06966",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T07:13:49.005Z",
            "data": {
              "session_id": "session_1761203628993_mnm0slw",
              "source_id": "arxiv",
              "paper_id": "2412.06966",
              "start_time": "2025-10-23T07:13:26.656Z",
              "end_time": "2025-10-23T07:13:48.993Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "issue_number": 468,
        "object_id": "interactions:arxiv.2412.06966",
        "created_at": "2025-10-23T07:13:49+00:00",
        "updated_at": "2025-10-23T07:14:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.06966": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.06966",
        "url": "https://arxiv.org/pdf/2412.06966?",
        "title": "Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI\n  Policy, Research, and Practice",
        "authors": "A. Feder Cooper, Christopher A. Choquette-Choo, Miranda Bogen, Matthew Jagielski, Katja Filippova, Ken Ziyu Liu, Alexandra Chouldechova, Jamie Hayes, Yangsibo Huang, Niloofar Mireshghallah, Ilia Shumailov, Eleni Triantafillou, Peter Kairouz, Nicole Mitchell, Percy Liang, Daniel E. Ho, Yejin Choi, Sanmi Koyejo, Fernando Delgado, James Grimmelmann, Vitaly Shmatikov, Christopher De Sa, Solon Barocas, Amy Cyphert, Mark Lemley, danah boyd, Jennifer Wortman Vaughan, Miles Brundage, David Bau, Seth Neel, Abigail Z. Jacobs, Andreas Terzis, Hanna Wallach, Nicolas Papernot, Katherine Lee",
        "abstract": "We articulate fundamental mismatches between technical methods for machine\nunlearning in Generative AI, and documented aspirations for broader impact that\nthese methods could have for law and policy. These aspirations are both\nnumerous and varied, motivated by issues that pertain to privacy, copyright,\nsafety, and more. For example, unlearning is often invoked as a solution for\nremoving the effects of targeted information from a generative-AI model's\nparameters, e.g., a particular individual's personal data or in-copyright\nexpression of Spiderman that was included in the model's training data.\nUnlearning is also proposed as a way to prevent a model from generating\ntargeted types of information in its outputs, e.g., generations that closely\nresemble a particular individual's data or reflect the concept of \"Spiderman.\"\nBoth of these goals--the targeted removal of information from a model and the\ntargeted suppression of information from a model's outputs--present various\ntechnical and substantive challenges. We provide a framework for thinking\nrigorously about these challenges, which enables us to be clear about why\nunlearning is not a general-purpose solution for circumscribing generative-AI\nmodel behavior in service of broader positive impact. We aim for conceptual\nclarity and to encourage more thoughtful communication among machine learning\n(ML), law, and policy experts who seek to develop and apply technical methods\nfor compliance with policy objectives.",
        "timestamp": "2025-10-23T07:13:04.926Z",
        "rating": "novote",
        "publishedDate": "2024-12-09T20:18:43Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CY"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 467,
        "object_id": "paper:arxiv.2412.06966",
        "created_at": "2025-10-23T07:13:05+00:00",
        "updated_at": "2025-10-23T07:13:30+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2302.10870": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2302.10870",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T07:16:07.681Z",
            "data": {
              "session_id": "session_1761203767108_qcxbja3",
              "source_id": "arxiv",
              "paper_id": "2302.10870",
              "start_time": "2025-10-23T07:15:34.189Z",
              "end_time": "2025-10-23T07:16:07.108Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 3,
              "total_elapsed_seconds": 33
            }
          }
        ]
      },
      "meta": {
        "issue_number": 472,
        "object_id": "interactions:arxiv.2302.10870",
        "created_at": "2025-10-23T07:16:08+00:00",
        "updated_at": "2025-10-23T07:16:38+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2302.10870": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2302.10870",
        "url": "https://arxiv.org/pdf/2302.10870",
        "title": "On Provable Copyright Protection for Generative Models",
        "authors": "Nikhil Vyas, Sham Kakade, Boaz Barak",
        "abstract": "There is a growing concern that learned conditional generative models may\noutput samples that are substantially similar to some copyrighted data $C$ that\nwas in their training set. We give a formal definition of $\\textit{near\naccess-freeness (NAF)}$ and prove bounds on the probability that a model\nsatisfying this definition outputs a sample similar to $C$, even if $C$ is\nincluded in its training set. Roughly speaking, a generative model $p$ is\n$\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of\n$p$ diverges by at most $k$-bits from the output of a model $q$ that\n$\\textit{did not access $C$ at all}$. We also give generative model learning\nalgorithms, which efficiently modify the original generative model learning\nalgorithm in a black box manner, that output generative models with strong\nbounds on the probability of sampling protected content. Furthermore, we\nprovide promising experiments for both language (transformers) and image\n(diffusion) generative models, showing minimal degradation in output quality\nwhile ensuring strong protections against sampling protected content.",
        "timestamp": "2025-10-23T07:15:34.427Z",
        "rating": "novote",
        "publishedDate": "2023-02-21T18:34:51Z",
        "tags": [
          "cs.LG",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 469,
        "object_id": "paper:arxiv.2302.10870",
        "created_at": "2025-10-23T07:15:34+00:00",
        "updated_at": "2025-10-23T07:16:05+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2304.03738": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2304.03738",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T07:21:29.884Z",
            "data": {
              "session_id": "session_1761204089876_58cof92",
              "source_id": "arxiv",
              "paper_id": "2304.03738",
              "start_time": "2025-10-23T07:20:57.481Z",
              "end_time": "2025-10-23T07:21:29.876Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "issue_number": 474,
        "object_id": "interactions:arxiv.2304.03738",
        "created_at": "2025-10-23T07:21:30+00:00",
        "updated_at": "2025-10-23T07:22:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2304.03738": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2304.03738",
        "url": "https://arxiv.org/pdf/2304.03738",
        "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language\n  Models",
        "authors": "Emilio Ferrara",
        "abstract": "As the capabilities of generative language models continue to advance, the\nimplications of biases ingrained within these models have garnered increasing\nattention from researchers, practitioners, and the broader public. This article\ninvestigates the challenges and risks associated with biases in large-scale\nlanguage models like ChatGPT. We discuss the origins of biases, stemming from,\namong others, the nature of training data, model specifications, algorithmic\nconstraints, product design, and policy decisions. We explore the ethical\nconcerns arising from the unintended consequences of biased model outputs. We\nfurther analyze the potential opportunities to mitigate biases, the\ninevitability of some biases, and the implications of deploying these models in\nvarious applications, such as virtual assistants, content generation, and\nchatbots. Finally, we review the current approaches to identify, quantify, and\nmitigate biases in language models, emphasizing the need for a\nmulti-disciplinary, collaborative effort to develop more equitable,\ntransparent, and responsible AI systems. This article aims to stimulate a\nthoughtful dialogue within the artificial intelligence community, encouraging\nresearchers and developers to reflect on the role of biases in generative\nlanguage models and the ongoing pursuit of ethical AI.",
        "timestamp": "2025-10-23T07:20:51.988Z",
        "rating": "novote",
        "publishedDate": "2023-04-07T17:14:00Z",
        "tags": [
          "cs.CY",
          "cs.CL"
        ],
        "doi": "10.5210/fm.v28i11.13346",
        "journalName": "First Monday, Volume 28, Number 11 - 6 November 2023",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 473,
        "object_id": "paper:arxiv.2304.03738",
        "created_at": "2025-10-23T07:20:52+00:00",
        "updated_at": "2025-10-23T07:21:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.01909": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.01909",
        "url": "https://arxiv.org/pdf/2402.01909",
        "title": "On Catastrophic Inheritance of Large Foundation Models",
        "authors": "Hao Chen, Bhiksha Raj, Xing Xie, Jindong Wang",
        "abstract": "Large foundation models (LFMs) are claiming incredible performances. Yet\ngreat concerns have been raised about their mythic and uninterpreted potentials\nnot only in machine learning, but also in various other disciplines. In this\nposition paper, we propose to identify a neglected issue deeply rooted in LFMs:\nCatastrophic Inheritance, describing the weaknesses and limitations inherited\nfrom biased large-scale pre-training data to behaviors of LFMs on the\ndownstream tasks, including samples that are corrupted, long-tailed, noisy,\nout-of-distributed, to name a few. Such inheritance can potentially cause\ncatastrophes to downstream applications, such as bias, lack of generalization,\ndeteriorated performance, security vulnerability, privacy leakage, and value\nmisalignment. We discuss the challenges behind this issue and propose UIM, a\nframework to Understand the catastrophic inheritance of LFMs from both\npre-training and downstream adaptation, Interpret the implications of\ncatastrophic inheritance on downstream tasks, and how to Mitigate it. UIM aims\nto unite both the machine learning and social sciences communities for more\nresponsible and promising AI development and deployment.",
        "timestamp": "2025-10-23T07:32:24.524Z",
        "rating": "novote",
        "publishedDate": "2024-02-02T21:21:55Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CY"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 475,
        "object_id": "paper:arxiv.2402.01909",
        "created_at": "2025-10-23T07:32:24+00:00",
        "updated_at": "2025-10-23T07:32:53+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2402.01909": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.01909",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T07:33:53.445Z",
            "data": {
              "session_id": "session_1761204833435_ii3xe85",
              "source_id": "arxiv",
              "paper_id": "2402.01909",
              "start_time": "2025-10-23T07:33:41.311Z",
              "end_time": "2025-10-23T07:33:53.435Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "issue_number": 476,
        "object_id": "interactions:arxiv.2402.01909",
        "created_at": "2025-10-23T07:33:28+00:00",
        "updated_at": "2025-10-23T07:34:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2212.08037": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2212.08037",
        "url": "https://arxiv.org/abs/2212.08037",
        "title": "Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models",
        "authors": "Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Massimiliano Ciaramita, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Lierni Sestorain Saralegui, Tal Schuster, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, Kellie Webster",
        "abstract": "Large language models (LLMs) have shown impressive results while requiring little or no direct supervision. Further, there is mounting evidence that LLMs may have potential in information-seeking scenarios. We believe the ability of an LLM to attribute the text that it generates is likely to be crucial in this setting. We formulate and study Attributed QA as a key first step in the development of attributed LLMs. We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures. We take human annotations as a gold standard and show that a correlated automatic metric is suitable for development. Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).",
        "timestamp": "2025-10-23T07:54:38.526Z",
        "rating": "novote",
        "publishedDate": "2022/12/15",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 478,
        "object_id": "paper:arxiv.2212.08037",
        "created_at": "2025-10-23T07:54:38+00:00",
        "updated_at": "2025-10-23T07:55:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.14223": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.14223",
        "url": "https://arxiv.org/abs/2509.14223",
        "title": "Fresh in memory: Training-order recency is linearly encoded in language model activations",
        "authors": "Dmitrii Krasheninnikov, Richard E. Turner, David Krueger",
        "abstract": "We show that language models' activations linearly encode when information was learned during training. Our setup involves creating a model with a known training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but otherwise similar datasets about named entities. We find that the average activations of test samples corresponding to the six training datasets encode the training order: when projected into a 2D subspace, these centroids are arranged exactly in the order of training and lie on a straight line. Further, we show that linear probes can accurately (~90%) distinguish \"early\" vs. \"late\" entities, generalizing to entities unseen during the probes' own training. The model can also be fine-tuned to explicitly report an unseen entity's training stage (~80% accuracy). Interestingly, the training-order encoding does not seem attributable to simple differences in activation magnitudes, losses, or model confidence. Our paper demonstrates that models are capable of differentiating information by its acquisition time, and carries significant implications for how they might manage conflicting data and respond to knowledge modifications.",
        "timestamp": "2025-10-23T07:53:58.717Z",
        "rating": "novote",
        "publishedDate": "2025/09/17",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 477,
        "object_id": "paper:arxiv.2509.14223",
        "created_at": "2025-10-23T07:53:59+00:00",
        "updated_at": "2025-10-23T07:54:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2212.08037": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2212.08037",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T07:57:06.758Z",
            "data": {
              "session_id": "session_1761206226753_ap1wans",
              "source_id": "arxiv",
              "paper_id": "2212.08037",
              "start_time": "2025-10-23T07:56:24.742Z",
              "end_time": "2025-10-23T07:57:06.753Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 2,
              "total_elapsed_seconds": 42
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T08:09:44.753Z",
            "data": {
              "session_id": "session_1761206984066_te76mrg",
              "source_id": "arxiv",
              "paper_id": "2212.08037",
              "start_time": "2025-10-23T08:06:54.240Z",
              "end_time": "2025-10-23T08:09:44.066Z",
              "heartbeat_count": 33,
              "duration_seconds": 165,
              "idle_seconds": 5,
              "total_elapsed_seconds": 170
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T08:22:15.191Z",
            "data": {
              "session_id": "session_1761207734619_9ru41vo",
              "source_id": "arxiv",
              "paper_id": "2212.08037",
              "start_time": "2025-10-23T08:22:07.583Z",
              "end_time": "2025-10-23T08:22:14.619Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T10:57:25.158Z",
            "data": {
              "session_id": "session_1761217044876_ab29i3x",
              "source_id": "arxiv",
              "paper_id": "2212.08037",
              "start_time": "2025-10-23T10:57:06.814Z",
              "end_time": "2025-10-23T10:57:24.876Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "issue_number": 479,
        "object_id": "interactions:arxiv.2212.08037",
        "created_at": "2025-10-23T07:56:15+00:00",
        "updated_at": "2025-10-23T10:58:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.18890": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.18890",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T08:25:48.181Z",
            "data": {
              "session_id": "session_1761207947365_7dbdu74",
              "source_id": "arxiv",
              "paper_id": "2510.18890",
              "start_time": "2025-10-23T08:25:13.648Z",
              "end_time": "2025-10-23T08:25:47.365Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          }
        ]
      },
      "meta": {
        "issue_number": 481,
        "object_id": "interactions:arxiv.2510.18890",
        "created_at": "2025-10-23T08:25:49+00:00",
        "updated_at": "2025-10-23T08:26:22+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.18890": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.18890",
        "url": "https://arxiv.org/abs/2510.18890",
        "title": "Small Language Models Offer Significant Potential for Science Community",
        "authors": "Jian Zhang",
        "abstract": "Recent advancements in natural language processing, particularly with large language models (LLMs), are transforming how scientists engage with the literature. While the adoption of LLMs is increasing, concerns remain regarding potential information biases and computational costs. Rather than LLMs, I developed a framework to evaluate the feasibility of precise, rapid, and cost-effective information retrieval from extensive geoscience literature using freely available small language models (MiniLMs). A curated corpus of approximately 77 million high-quality sentences, extracted from 95 leading peer-reviewed geoscience journals such as Geophysical Research Letters and Earth and Planetary Science Letters published during years 2000 to 2024, was constructed. MiniLMs enable a computationally efficient approach for extracting relevant domain-specific information from these corpora through semantic search techniques and sentence-level indexing. This approach, unlike LLMs such as ChatGPT-4 that often produces generalized responses, excels at identifying substantial amounts of expert-verified information with established, multi-disciplinary sources, especially for information with quantitative findings. Furthermore, by analyzing emotional tone via sentiment analysis and topical clusters through unsupervised clustering within sentences, MiniLM provides a powerful tool for tracking the evolution of conclusions, research priorities, advancements, and emerging questions within geoscience communities. Overall, MiniLM holds significant potential within the geoscience community for applications such as fact and image retrievals, trend analyses, contradiction analyses, and educational purposes.",
        "timestamp": "2025-10-23T08:25:11.200Z",
        "rating": "novote",
        "publishedDate": "2025/10/18",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 480,
        "object_id": "paper:arxiv.2510.18890",
        "created_at": "2025-10-23T08:25:11+00:00",
        "updated_at": "2025-10-23T08:25:37+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.18019": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.18019",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T08:31:10.145Z",
            "data": {
              "session_id": "session_1761208269167_qvjemgv",
              "source_id": "arxiv",
              "paper_id": "2510.18019",
              "start_time": "2025-10-23T08:30:32.787Z",
              "end_time": "2025-10-23T08:31:09.167Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 1,
              "total_elapsed_seconds": 36
            }
          }
        ]
      },
      "meta": {
        "issue_number": 483,
        "object_id": "interactions:arxiv.2510.18019",
        "created_at": "2025-10-23T08:31:10+00:00",
        "updated_at": "2025-10-23T08:31:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.18019": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.18019",
        "url": "https://arxiv.org/pdf/2510.18019",
        "title": "Is Multilingual LLM Watermarking Truly Multilingual? A Simple\n  Back-Translation Solution",
        "authors": "Asim Mohamed, Martin Gubri",
        "abstract": "Multilingual watermarking aims to make large language model (LLM) outputs\ntraceable across languages, yet current methods still fall short. Despite\nclaims of cross-lingual robustness, they are evaluated only on high-resource\nlanguages. We show that existing multilingual watermarking methods are not\ntruly multilingual: they fail to remain robust under translation attacks in\nmedium- and low-resource languages. We trace this failure to semantic\nclustering, which fails when the tokenizer vocabulary contains too few\nfull-word tokens for a given language. To address this, we introduce STEAM, a\nback-translation-based detection method that restores watermark strength lost\nthrough translation. STEAM is compatible with any watermarking method, robust\nacross different tokenizers and languages, non-invasive, and easily extendable\nto new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17\nlanguages, STEAM provides a simple and robust path toward fairer watermarking\nacross diverse languages.",
        "timestamp": "2025-10-23T08:30:33.391Z",
        "rating": "novote",
        "publishedDate": "2025-10-20T18:51:20Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 482,
        "object_id": "paper:arxiv.2510.18019",
        "created_at": "2025-10-23T08:30:33+00:00",
        "updated_at": "2025-10-23T08:31:00+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.26544": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.26544",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T08:42:35.239Z",
            "data": {
              "session_id": "session_1761208955216_ovhb6pc",
              "source_id": "arxiv",
              "paper_id": "2509.26544",
              "start_time": "2025-10-23T08:42:17.121Z",
              "end_time": "2025-10-23T08:42:35.216Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "issue_number": 486,
        "object_id": "interactions:arxiv.2509.26544",
        "created_at": "2025-10-23T08:42:18+00:00",
        "updated_at": "2025-10-23T08:42:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.26544": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.26544",
        "url": "https://arxiv.org/pdf/2509.26544",
        "title": "Bayesian Influence Functions for Hessian-Free Data Attribution",
        "authors": "Philipp Alexander Kreer, Wilson Wu, Maxwell Adam, Zach Furman, Jesse Hoogland",
        "abstract": "Classical influence functions face significant challenges when applied to\ndeep neural networks, primarily due to non-invertible Hessians and\nhigh-dimensional parameter spaces. We propose the local Bayesian influence\nfunction (BIF), an extension of classical influence functions that replaces\nHessian inversion with loss landscape statistics that can be estimated via\nstochastic-gradient MCMC sampling. This Hessian-free approach captures\nhigher-order interactions among parameters and scales efficiently to neural\nnetworks with billions of parameters. We demonstrate state-of-the-art results\non predicting retraining experiments.",
        "timestamp": "2025-10-23T08:41:17.087Z",
        "rating": "novote",
        "publishedDate": "2025-09-30T17:17:37Z",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 484,
        "object_id": "paper:arxiv.2509.26544",
        "created_at": "2025-10-23T08:41:17+00:00",
        "updated_at": "2025-10-23T08:41:48+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2508.10975": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.10975",
        "url": "https://arxiv.org/pdf/2508.10975",
        "title": "BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale\n  Pretraining",
        "authors": "DatologyAI, :, Pratyush Maini, Vineeth Dorna, Parth Doshi, Aldo Carranza, Fan Pan, Jack Urbanek, Paul Burstein, Alex Fang, Alvin Deng, Amro Abbas, Brett Larsen, Cody Blakeney, Charvi Bannur, Christina Baek, Darren Teh, David Schwab, Haakon Mongstad, Haoli Yin, Josh Wills, Kaleigh Mentzer, Luke Merrick, Ricardo Monti, Rishabh Adiga, Siddharth Joshi, Spandan Das, Zhengping Wang, Bogdan Gaza, Ari Morcos, Matthew Leavitt",
        "abstract": "Recent advances in large language model (LLM) pretraining have shown that\nsimply scaling data quantity eventually leads to diminishing returns, hitting a\ndata wall. In response, the use of synthetic data for pretraining has emerged\nas a promising paradigm for pushing the frontier of performance. Despite this,\nthe factors affecting synthetic data quality remain poorly understood. In this\nwork, we introduce BeyondWeb, a synthetic data generation framework that\nproduces high-quality synthetic data for pretraining. BeyondWeb significantly\nextends the capabilities of traditional web-scale datasets, outperforming\nstate-of-the-art synthetic pretraining datasets such as Cosmopedia and\nNemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1\npercentage points (pp) and 2.6pp, respectively, when averaged across a suite of\n14 benchmark evaluations. It delivers up to 7.7x faster training than open web\ndata and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for\n180B tokens on BeyondWeb outperforms an 8B model trained for the same token\nbudget on Cosmopedia. We also present several insights from BeyondWeb on\nsynthetic data for pretraining: what drives its benefits, which data to\nrephrase and how, and the impact of model size and family on data quality.\nOverall, our work shows that there's no silver bullet for generating\nhigh-quality synthetic pretraining data. The best outcomes require jointly\noptimizing many factors, a challenging task that requires rigorous science and\npractical expertise. Naive approaches can yield modest improvements,\npotentially at great cost, while well-executed methods can yield transformative\nimprovements, as exemplified by BeyondWeb.",
        "timestamp": "2025-10-23T09:42:11.410Z",
        "rating": "novote",
        "publishedDate": "2025-08-14T17:55:47Z",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 487,
        "object_id": "paper:arxiv.2508.10975",
        "created_at": "2025-10-23T09:42:11+00:00",
        "updated_at": "2025-10-23T09:42:44+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.15020": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.15020",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T10:56:58.764Z",
            "data": {
              "session_id": "session_1761217018391_ha2ggt1",
              "source_id": "arxiv",
              "paper_id": "2510.15020",
              "start_time": "2025-10-23T10:56:53.316Z",
              "end_time": "2025-10-23T10:56:58.391Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 488,
        "object_id": "interactions:arxiv.2510.15020",
        "created_at": "2025-10-23T10:56:59+00:00",
        "updated_at": "2025-10-23T10:57:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.06182": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.06182",
        "url": "https://arxiv.org/pdf/2510.06182",
        "title": "Mixing Mechanisms: How Language Models Retrieve Bound Entities\n  In-Context",
        "authors": "Yoav Gur-Arieh, Mor Geva, Atticus Geiger",
        "abstract": "A key component of in-context reasoning is the ability of language models\n(LMs) to bind entities for later retrieval. For example, an LM might represent\n\"Ann loves pie\" by binding \"Ann\" to \"pie\", allowing it to later retrieve \"Ann\"\nwhen asked \"Who loves pie?\" Prior research on short lists of bound entities\nfound strong evidence that LMs implement such retrieval via a positional\nmechanism, where \"Ann\" is retrieved based on its position in context. In this\nwork, we find that this mechanism generalizes poorly to more complex settings;\nas the number of bound entities in context increases, the positional mechanism\nbecomes noisy and unreliable in middle positions. To compensate for this, we\nfind that LMs supplement the positional mechanism with a lexical mechanism\n(retrieving \"Ann\" using its bound counterpart \"pie\") and a reflexive mechanism\n(retrieving \"Ann\" through a direct pointer). Through extensive experiments on\nnine models and ten binding tasks, we uncover a consistent pattern in how LMs\nmix these mechanisms to drive model behavior. We leverage these insights to\ndevelop a causal model combining all three mechanisms that estimates next token\ndistributions with 95% agreement. Finally, we show that our model generalizes\nto substantially longer inputs of open-ended text interleaved with entity\ngroups, further demonstrating the robustness of our findings in more natural\nsettings. Overall, our study establishes a more complete picture of how LMs\nbind and retrieve entities in-context.",
        "timestamp": "2025-10-23T18:18:30.529Z",
        "rating": "novote",
        "publishedDate": "2025-10-07T17:44:30Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 489,
        "object_id": "paper:arxiv.2510.06182",
        "created_at": "2025-10-23T18:18:31+00:00",
        "updated_at": "2025-10-23T18:18:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.06182": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.06182",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T18:20:01.673Z",
            "data": {
              "session_id": "session_1761243601070_yq1i9gr",
              "source_id": "arxiv",
              "paper_id": "2510.06182",
              "start_time": "2025-10-23T18:18:30.185Z",
              "end_time": "2025-10-23T18:20:01.070Z",
              "heartbeat_count": 18,
              "duration_seconds": 90,
              "idle_seconds": 1,
              "total_elapsed_seconds": 91
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-23T18:21:34.320Z",
            "data": {
              "session_id": "session_1761243693435_dh6u3y7",
              "source_id": "arxiv",
              "paper_id": "2510.06182",
              "start_time": "2025-10-23T18:20:04.106Z",
              "end_time": "2025-10-23T18:21:33.435Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 4,
              "total_elapsed_seconds": 89
            }
          }
        ]
      },
      "meta": {
        "issue_number": 490,
        "object_id": "interactions:arxiv.2510.06182",
        "created_at": "2025-10-23T18:20:02+00:00",
        "updated_at": "2025-10-23T18:22:03+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.18270": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.18270",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:23:23.649Z",
            "data": {
              "session_id": "session_1761809003622_tm271bz",
              "source_id": "arxiv",
              "paper_id": "2410.18270",
              "start_time": "2025-10-30T07:23:03.379Z",
              "end_time": "2025-10-30T07:23:23.622Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:24:18.490Z",
            "data": {
              "session_id": "session_1761809058469_hygln23",
              "source_id": "arxiv",
              "paper_id": "2410.18270",
              "start_time": "2025-10-30T07:24:06.653Z",
              "end_time": "2025-10-30T07:24:18.469Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:25:42.253Z",
            "data": {
              "session_id": "session_1761809141561_q1201dw",
              "source_id": "arxiv",
              "paper_id": "2410.18270",
              "start_time": "2025-10-30T07:24:22.347Z",
              "end_time": "2025-10-30T07:25:41.561Z",
              "heartbeat_count": 15,
              "duration_seconds": 75,
              "idle_seconds": 4,
              "total_elapsed_seconds": 79
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T14:24:11.998Z",
            "data": {
              "session_id": "session_1762093451990_i2q2r8m",
              "source_id": "arxiv",
              "paper_id": "2410.18270",
              "start_time": "2025-11-02T14:23:30.880Z",
              "end_time": "2025-11-02T14:24:11.990Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 1,
              "total_elapsed_seconds": 41
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T14:26:18.140Z",
            "data": {
              "session_id": "session_1762093578123_t0eu30d",
              "source_id": "arxiv",
              "paper_id": "2410.18270",
              "start_time": "2025-11-02T14:25:23.281Z",
              "end_time": "2025-11-02T14:26:18.123Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 5,
              "total_elapsed_seconds": 55
            }
          }
        ]
      },
      "meta": {
        "issue_number": 492,
        "object_id": "interactions:arxiv.2410.18270",
        "created_at": "2025-10-23T19:17:53+00:00",
        "updated_at": "2025-11-02T14:26:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.18270": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.18270",
        "url": "https://arxiv.org/abs/2410.18270",
        "title": "Multilingual Hallucination Gaps in Large Language Models",
        "authors": "Cl\u00e9a Chataigner, Afaf Ta\u00efk, Golnoosh Farnadi",
        "abstract": "Large language models (LLMs) are increasingly used as alternatives to traditional search engines given their capacity to generate text that resembles human language. However, this shift is concerning, as LLMs often generate hallucinations, misleading or false information that appears highly credible. In this study, we explore the phenomenon of hallucinations across multiple languages in freeform text generation, focusing on what we call multilingual hallucination gaps. These gaps reflect differences in the frequency of hallucinated answers depending on the prompt and language used. To quantify such hallucinations, we used the FactScore metric and extended its framework to a multilingual setting. We conducted experiments using LLMs from the LLaMA, Qwen, and Aya families, generating biographies in 19 languages and comparing the results to Wikipedia pages. Our results reveal variations in hallucination rates, especially between high and low resource languages, raising important questions about LLM multilingual performance and the challenges in evaluating hallucinations in multilingual freeform text generation.",
        "timestamp": "2025-10-23T19:17:11.209Z",
        "rating": "novote",
        "publishedDate": "2024/10/23",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 491,
        "object_id": "paper:arxiv.2410.18270",
        "created_at": "2025-10-23T19:17:11+00:00",
        "updated_at": "2025-10-23T19:17:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.19811": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.19811",
        "url": "https://arxiv.org/pdf/2510.19811",
        "title": "Hubble: a Model Suite to Advance the Study of LLM Memorization",
        "authors": "Johnny Tian-Zheng Wei, Ameya Godbole, Mohammad Aflah Khan, Ryan Wang, Xiaoyuan Zhu, James Flemings, Nitya Kashyap, Krishna P. Gummadi, Willie Neiswanger, Robin Jia",
        "abstract": "We present Hubble, a suite of fully open-source large language models (LLMs)\nfor the scientific study of LLM memorization. Hubble models come in standard\nand perturbed variants: standard models are pretrained on a large English\ncorpus, and perturbed models are trained in the same way but with controlled\ninsertion of text (e.g., book passages, biographies, and test sets) designed to\nemulate key memorization risks. Our core release includes 8 models -- standard\nand perturbed models with 1B or 8B parameters, pretrained on 100B or 500B\ntokens -- establishing that memorization risks are determined by the frequency\nof sensitive data relative to size of the training corpus (i.e., a password\nappearing once in a smaller corpus is memorized better than the same password\nin a larger corpus). Our release also includes 6 perturbed models with text\ninserted at different pretraining phases, showing that sensitive data without\ncontinued exposure can be forgotten. These findings suggest two best practices\nfor addressing memorization risks: to dilute sensitive data by increasing the\nsize of the training corpus, and to order sensitive data to appear earlier in\ntraining. Beyond these general empirical findings, Hubble enables a broad range\nof memorization research; for example, analyzing the biographies reveals how\nreadily different types of private information are memorized. We also\ndemonstrate that the randomized insertions in Hubble make it an ideal testbed\nfor membership inference and machine unlearning, and invite the community to\nfurther explore, benchmark, and build upon our work.",
        "timestamp": "2025-10-24T17:20:56.946Z",
        "rating": "novote",
        "publishedDate": "2025-10-22T17:48:23Z",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 493,
        "object_id": "paper:arxiv.2510.19811",
        "created_at": "2025-10-24T17:20:57+00:00",
        "updated_at": "2025-10-24T17:21:24+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.19811": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.19811",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-24T17:22:05.597Z",
            "data": {
              "session_id": "session_1761326525019_4zz3vgi",
              "source_id": "arxiv",
              "paper_id": "2510.19811",
              "start_time": "2025-10-24T17:20:56.621Z",
              "end_time": "2025-10-24T17:22:05.019Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 3,
              "total_elapsed_seconds": 68
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-24T17:29:11.737Z",
            "data": {
              "session_id": "session_1761326951148_wlrzpvz",
              "source_id": "arxiv",
              "paper_id": "2510.19811",
              "start_time": "2025-10-24T17:23:42.361Z",
              "end_time": "2025-10-24T17:29:11.148Z",
              "heartbeat_count": 65,
              "duration_seconds": 325,
              "idle_seconds": 4,
              "total_elapsed_seconds": 329
            }
          }
        ]
      },
      "meta": {
        "issue_number": 494,
        "object_id": "interactions:arxiv.2510.19811",
        "created_at": "2025-10-24T17:22:06+00:00",
        "updated_at": "2025-10-24T17:29:41+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2408.11791": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.11791",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-24T17:55:14.995Z",
            "data": {
              "session_id": "session_1761328514924_82if8ue",
              "source_id": "arxiv",
              "paper_id": "2408.11791",
              "start_time": "2025-10-24T17:54:43.162Z",
              "end_time": "2025-10-24T17:55:14.924Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "issue_number": 496,
        "object_id": "interactions:arxiv.2408.11791",
        "created_at": "2025-10-24T17:55:15+00:00",
        "updated_at": "2025-10-24T17:55:41+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.11791": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.11791",
        "url": "https://arxiv.org/abs/2408.11791",
        "title": "Critique-out-Loud Reward Models",
        "authors": "Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D. Chang, Prithviraj Ammanabrolu",
        "abstract": "Traditionally, reward models used for reinforcement learning from human feedback (RLHF) are trained to directly predict preference scores without leveraging the generation capabilities of the underlying large language model (LLM). This limits the capabilities of reward models as they must reason implicitly about the quality of a response, i.e., preference modeling must be performed in a single forward pass through the model. To enable reward models to reason explicitly about the quality of a response, we introduce Critique-out-Loud (CLoud) reward models. CLoud reward models operate by first generating a natural language critique of the assistant's response that is then used to predict a scalar reward for the quality of the response. We demonstrate the success of CLoud reward models for both Llama-3-8B and 70B base models: compared to classic reward models CLoud reward models improve pairwise preference classification accuracy on RewardBench by 4.65 and 5.84 percentage points for the 8B and 70B base models respectively. Furthermore, CLoud reward models lead to a Pareto improvement for win rate on ArenaHard when used as the scoring model for Best-of-N. Finally, we explore how to exploit the dynamic inference compute capabilities of CLoud reward models by performing self-consistency decoding for reward prediction.",
        "timestamp": "2025-10-24T17:54:38.327Z",
        "rating": "novote",
        "publishedDate": "2024/08/21",
        "tags": [
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 495,
        "object_id": "paper:arxiv.2408.11791",
        "created_at": "2025-10-24T17:54:38+00:00",
        "updated_at": "2025-10-24T17:55:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.15804": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.15804",
        "url": "https://arxiv.org/pdf/2510.15804",
        "title": "Emergence of Linear Truth Encodings in Language Models",
        "authors": "Shauli Ravfogel, Gilad Yehudai, Tal Linzen, Joan Bruna, Alberto Bietti",
        "abstract": "Recent probing studies reveal that large language models exhibit linear\nsubspaces that separate true from false statements, yet the mechanism behind\ntheir emergence is unclear. We introduce a transparent, one-layer transformer\ntoy model that reproduces such truth subspaces end-to-end and exposes one\nconcrete route by which they can arise. We study one simple setting in which\ntruth encoding can emerge: a data distribution where factual statements\nco-occur with other factual statements (and vice-versa), encouraging the model\nto learn this distinction in order to lower the LM loss on future tokens. We\ncorroborate this pattern with experiments in pretrained language models.\nFinally, in the toy setting we observe a two-phase learning dynamic: networks\nfirst memorize individual factual associations in a few steps, then -- over a\nlonger horizon -- learn to linearly separate true from false, which in turn\nlowers language-modeling loss. Together, these results provide both a\nmechanistic demonstration and an empirical motivation for how and why linear\ntruth representations can emerge in language models.",
        "timestamp": "2025-10-24T18:58:28.958Z",
        "rating": "novote",
        "publishedDate": "2025-10-17T16:30:07Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 497,
        "object_id": "paper:arxiv.2510.15804",
        "created_at": "2025-10-24T18:58:29+00:00",
        "updated_at": "2025-10-24T18:58:57+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.15804": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.15804",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-24T18:59:31.598Z",
            "data": {
              "session_id": "session_1761332371582_gmiu6jv",
              "source_id": "arxiv",
              "paper_id": "2510.15804",
              "start_time": "2025-10-24T18:58:34.620Z",
              "end_time": "2025-10-24T18:59:31.582Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 2,
              "total_elapsed_seconds": 57
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:04:50.444Z",
            "data": {
              "session_id": "session_1761807890131_sax8778",
              "source_id": "arxiv",
              "paper_id": "2510.15804",
              "start_time": "2025-10-30T07:04:24.824Z",
              "end_time": "2025-10-30T07:04:50.131Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 0,
              "total_elapsed_seconds": 25
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:07:01.996Z",
            "data": {
              "session_id": "session_1761808021698_xozmjat",
              "source_id": "arxiv",
              "paper_id": "2510.15804",
              "start_time": "2025-10-30T07:06:53.627Z",
              "end_time": "2025-10-30T07:07:01.698Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 498,
        "object_id": "interactions:arxiv.2510.15804",
        "created_at": "2025-10-24T18:59:32+00:00",
        "updated_at": "2025-10-30T07:07:26+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.05092": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.05092",
        "url": "https://arxiv.org/abs/2510.05092",
        "title": "Learning to Interpret Weight Differences in Language Models",
        "authors": "Avichal Goel, Yoon Kim, Nir Shavit, Tony T. Wang",
        "abstract": "Finetuning (pretrained) language models is a standard approach for updating their internal parametric knowledge and specializing them to new tasks and domains. However, the corresponding model weight changes (\"weight diffs\") are not generally interpretable. While inspecting the finetuning dataset can give a sense of how the model might have changed, these datasets are often not publicly available or are too large to work with directly. Towards the goal of comprehensively understanding weight diffs in natural language, we introduce Diff Interpretation Tuning (DIT), a method that trains models to describe their own finetuning-induced modifications. Our approach uses synthetic, labeled weight diffs to train a DIT-adapter, which can be applied to a compatible finetuned model to make it describe how it has changed. We demonstrate in two proof-of-concept settings (reporting hidden behaviors and summarizing finetuned knowledge) that our method enables models to describe their finetuning-induced modifications using accurate natural language descriptions.",
        "timestamp": "2025-10-25T10:26:48.752Z",
        "rating": "novote",
        "publishedDate": "2025/10/06",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 499,
        "object_id": "paper:arxiv.2510.05092",
        "created_at": "2025-10-25T10:26:49+00:00",
        "updated_at": "2025-10-25T10:27:20+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.05092": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.05092",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-25T10:27:58.774Z",
            "data": {
              "session_id": "session_1761388078502_t0ztxg5",
              "source_id": "arxiv",
              "paper_id": "2510.05092",
              "start_time": "2025-10-25T10:27:07.900Z",
              "end_time": "2025-10-25T10:27:58.502Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 1,
              "total_elapsed_seconds": 51
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-25T10:36:11.631Z",
            "data": {
              "session_id": "session_1761388571343_9mkg7df",
              "source_id": "arxiv",
              "paper_id": "2510.05092",
              "start_time": "2025-10-25T10:36:01.959Z",
              "end_time": "2025-10-25T10:36:11.343Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-26T09:35:50.726Z",
            "data": {
              "session_id": "session_1761471350359_gr7abx1",
              "source_id": "arxiv",
              "paper_id": "2510.05092",
              "start_time": "2025-10-26T09:35:42.010Z",
              "end_time": "2025-10-26T09:35:50.359Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T06:23:30.609Z",
            "data": {
              "session_id": "session_1761805410593_d3uu4tc",
              "source_id": "arxiv",
              "paper_id": "2510.05092",
              "start_time": "2025-10-30T06:23:04.341Z",
              "end_time": "2025-10-30T06:23:30.593Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 1,
              "total_elapsed_seconds": 26
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T06:39:48.489Z",
            "data": {
              "session_id": "session_1761806388481_akwr45a",
              "source_id": "arxiv",
              "paper_id": "2510.05092",
              "start_time": "2025-10-30T06:39:21.265Z",
              "end_time": "2025-10-30T06:39:48.481Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T06:50:17.805Z",
            "data": {
              "session_id": "session_1761807017508_1y9shl8",
              "source_id": "arxiv",
              "paper_id": "2510.05092",
              "start_time": "2025-10-30T06:49:21.052Z",
              "end_time": "2025-10-30T06:50:17.508Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 1,
              "total_elapsed_seconds": 56
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:04:15.830Z",
            "data": {
              "session_id": "session_1761807855249_65qwscn",
              "source_id": "arxiv",
              "paper_id": "2510.05092",
              "start_time": "2025-10-30T06:50:20.417Z",
              "end_time": "2025-10-30T07:04:15.249Z",
              "heartbeat_count": 166,
              "duration_seconds": 830,
              "idle_seconds": 5,
              "total_elapsed_seconds": 835
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-18T07:04:22.986Z",
            "data": {
              "session_id": "session_1763449462644_rp03vsn",
              "source_id": "arxiv",
              "paper_id": "2510.05092",
              "start_time": "2025-11-18T07:04:16.549Z",
              "end_time": "2025-11-18T07:04:22.644Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 500,
        "object_id": "interactions:arxiv.2510.05092",
        "created_at": "2025-10-25T10:27:09+00:00",
        "updated_at": "2025-11-18T07:04:50+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.11381": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.11381",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-25T15:13:21.737Z",
            "data": {
              "session_id": "session_1761405201694_0yidnjv",
              "source_id": "arxiv",
              "paper_id": "2504.11381",
              "start_time": "2025-10-25T15:13:10.353Z",
              "end_time": "2025-10-25T15:13:21.694Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-25T20:09:25.171Z",
            "data": {
              "session_id": "session_1761422964832_bgg68za",
              "source_id": "arxiv",
              "paper_id": "2504.11381",
              "start_time": "2025-10-25T20:08:52.892Z",
              "end_time": "2025-10-25T20:09:24.832Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-25T20:15:49.741Z",
            "data": {
              "session_id": "session_1761423349416_dgenixs",
              "source_id": "arxiv",
              "paper_id": "2504.11381",
              "start_time": "2025-10-25T20:15:31.728Z",
              "end_time": "2025-10-25T20:15:49.416Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-26T18:20:33.827Z",
            "data": {
              "session_id": "session_1761502833803_nta0lm1",
              "source_id": "arxiv",
              "paper_id": "2504.11381",
              "start_time": "2025-10-26T18:20:25.713Z",
              "end_time": "2025-10-26T18:20:33.803Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T06:37:41.768Z",
            "data": {
              "session_id": "session_1761979061715_bwh5rx3",
              "source_id": "arxiv",
              "paper_id": "2504.11381",
              "start_time": "2025-11-01T06:37:36.394Z",
              "end_time": "2025-11-01T06:37:41.715Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 503,
        "object_id": "interactions:arxiv.2504.11381",
        "created_at": "2025-10-25T15:13:22+00:00",
        "updated_at": "2025-11-01T06:38:08+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.11381": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.11381",
        "url": "https://arxiv.org/abs/2504.11381",
        "title": "RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models",
        "authors": "Juan Diego Rodriguez, Wenxuan Ding, Katrin Erk, Greg Durrett",
        "abstract": "Although large language models (LLMs) have become more capable and accurate across many tasks, some fundamental sources of unreliability remain in their behavior. One key limitation is their inconsistency at reporting the same information when prompts are changed. In this paper, we consider the discrepancy between a model's generated answer and their own verification of that answer, the generator-validator gap. We define this gap in a more stringent way than prior work: we expect correlation of scores from a generator and a validator over the entire set of candidate answers, i.e., candidate completions that could possibly arise during ordinary language use without breaking Gricean norms. We show that according to this measure, a large gap exists in various settings, including question answering, lexical semantics tasks, and next-word prediction. We then propose RankAlign, a ranking-based training method, and show that it significantly closes the gap, surpassing all baseline methods. Moreover, this approach generalizes well to out-of-domain tasks and lexical items.",
        "timestamp": "2025-10-25T15:13:03.260Z",
        "rating": "novote",
        "publishedDate": "2025/04/15",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 501,
        "object_id": "paper:arxiv.2504.11381",
        "created_at": "2025-10-25T15:13:03+00:00",
        "updated_at": "2025-10-25T15:13:31+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.19804": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.19804",
        "interactions": []
      },
      "meta": {
        "issue_number": 505,
        "object_id": "interactions:arxiv.2510.19804",
        "created_at": "2025-10-25T19:43:31+00:00",
        "updated_at": "2025-10-25T19:43:33+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.19804": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.19804",
        "url": "https://arxiv.org/abs/2510.19804",
        "title": "Forbidden Sidon subsets of perfect difference sets, featuring a human-assisted proof",
        "authors": "Boris Alexeev, Dustin G. Mixon",
        "abstract": "We resolve a $1000 Erd\u0151s prize problem, complete with formal verification generated by a large language model.\nIn over a dozen papers, beginning in 1976 and spanning two decades, Paul Erd\u0151s repeatedly posed one of his \"favourite\" conjectures: every finite Sidon set can be extended to a finite perfect difference set. We establish that {1, 2, 4, 8, 13} is a counterexample to this conjecture.\nDuring the preparation of this paper, we discovered that although this problem was presumed to be open for half a century, Marshall Hall, Jr. published a different counterexample three decades before Erd\u0151s first posed the problem. With a healthy skepticism of this apparent oversight, and out of an abundance of caution, we used ChatGPT to vibe code a Lean proof of both Hall's and our counterexamples.",
        "timestamp": "2025-10-25T19:42:37.481Z",
        "rating": "novote",
        "publishedDate": "2025/10/22",
        "tags": [
          "Combinatorics (math.CO)",
          "Number Theory (math.NT)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 504,
        "object_id": "paper:arxiv.2510.19804",
        "created_at": "2025-10-25T19:42:37+00:00",
        "updated_at": "2025-10-25T19:43:08+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.20643": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.20643",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-26T06:15:30.061Z",
            "data": {
              "session_id": "session_1761459329440_6iub0i8",
              "source_id": "arxiv",
              "paper_id": "2504.20643",
              "start_time": "2025-10-26T06:14:46.834Z",
              "end_time": "2025-10-26T06:15:29.440Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 3,
              "total_elapsed_seconds": 43
            }
          }
        ]
      },
      "meta": {
        "issue_number": 507,
        "object_id": "interactions:arxiv.2504.20643",
        "created_at": "2025-10-26T06:15:30+00:00",
        "updated_at": "2025-10-26T06:15:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.20643": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.20643",
        "url": "https://arxiv.org/pdf/2504.20643",
        "title": "Cooking Up Creativity: Enhancing LLM Creativity through Structured\n  Recombination",
        "authors": "Moran Mizrahi, Chen Shani, Gabriel Stanovsky, Dan Jurafsky, Dafna Shahaf",
        "abstract": "Large Language Models (LLMs) excel at many tasks, yet they struggle to\nproduce truly creative, diverse ideas. In this paper, we introduce a novel\napproach that enhances LLM creativity. We apply LLMs for translating between\nnatural language and structured representations, and perform the core creative\nleap via cognitively inspired manipulations on these representations. Our\nnotion of creativity goes beyond superficial token-level variations; rather, we\nrecombine structured representations of existing ideas, enabling our system to\neffectively explore a more abstract landscape of ideas. We demonstrate our\napproach in the culinary domain with DishCOVER, a model that generates creative\nrecipes. Experiments and domain-expert evaluations reveal that our outputs,\nwhich are mostly coherent and feasible, significantly surpass GPT-4o in terms\nof novelty and diversity, thus outperforming it in creative generation. We hope\nour work inspires further research into structured creativity in AI.",
        "timestamp": "2025-10-26T06:14:47.146Z",
        "rating": "novote",
        "publishedDate": "2025-04-29T11:13:06Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 506,
        "object_id": "paper:arxiv.2504.20643",
        "created_at": "2025-10-26T06:14:47+00:00",
        "updated_at": "2025-10-26T06:15:17+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.08827": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.08827",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-26T09:28:19.375Z",
            "data": {
              "session_id": "session_1761470898674_dzu0xm6",
              "source_id": "arxiv",
              "paper_id": "2509.08827",
              "start_time": "2025-10-26T09:27:13.658Z",
              "end_time": "2025-10-26T09:28:18.674Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 15,
              "total_elapsed_seconds": 65
            }
          }
        ]
      },
      "meta": {
        "issue_number": 509,
        "object_id": "interactions:arxiv.2509.08827",
        "created_at": "2025-10-26T09:28:20+00:00",
        "updated_at": "2025-10-26T09:28:44+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.08827": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.08827",
        "url": "https://arxiv.org/pdf/2509.08827",
        "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
        "authors": "Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou",
        "abstract": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
        "timestamp": "2025-10-26T09:27:14.061Z",
        "rating": "novote",
        "publishedDate": "2025-09-10T17:59:43Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 508,
        "object_id": "paper:arxiv.2509.08827",
        "created_at": "2025-10-26T09:27:14+00:00",
        "updated_at": "2025-10-26T09:27:47+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.03149": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.03149",
        "url": "https://arxiv.org/pdf/2506.03149",
        "title": "Causal Estimation of Tokenisation Bias",
        "authors": "Pietro Lesci, Clara Meister, Thomas Hofmann, Andreas Vlachos, Tiago Pimentel",
        "abstract": "Modern language models are typically trained over subword sequences, but\nultimately define probabilities over character-strings. Ideally, the choice of\nthe tokeniser -- which maps character-strings to subwords -- should not affect\nthe probability assigned to the underlying character-string; in practice, it\ndoes. We define this mismatch as tokenisation bias. In this work, we quantify\none particular type of tokenisation bias: the effect of including or not a\nsubword (e.g., $\\langle hello \\rangle$) in a tokeniser's vocabulary on the\nprobability a trained model assigns to the corresponding characters (i.e.,\n\\textit{``hello''}). Estimating this effect is challenging because each model\nis trained with only one tokeniser. We address this by framing tokenisation\nbias as a causal effect and estimating it using the regression discontinuity\ndesign. Specifically, we exploit the fact that tokenisation algorithms rank\nsubwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an\narbitrary cutoff point. As such, we can estimate a causal effect by comparing\nsimilar subwords around this cutoff. Experimentally, we find that tokenisation\nconsistently affects models' outputs across scales, vocabularies, and\ntokenisers. Notably, a subword's presence in a small model's vocabulary may\nincrease its characters' probability by up to 17 times, highlighting\ntokenisation as a key design choice in language modelling.",
        "timestamp": "2025-10-26T09:37:08.757Z",
        "rating": "novote",
        "publishedDate": "2025-06-03T17:59:47Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 510,
        "object_id": "paper:arxiv.2506.03149",
        "created_at": "2025-10-26T09:37:09+00:00",
        "updated_at": "2025-10-26T09:37:32+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.03149": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.03149",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-26T09:39:40.605Z",
            "data": {
              "session_id": "session_1761471580519_oqpg6b3",
              "source_id": "arxiv",
              "paper_id": "2506.03149",
              "start_time": "2025-10-26T09:39:35.413Z",
              "end_time": "2025-10-26T09:39:40.519Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 511,
        "object_id": "interactions:arxiv.2506.03149",
        "created_at": "2025-10-26T09:39:36+00:00",
        "updated_at": "2025-10-26T09:40:09+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.15015": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.15015",
        "url": "https://arxiv.org/abs/2510.15015",
        "title": "DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models",
        "authors": "Mor Ventura, Michael Toker, Or Patashnik, Yonatan Belinkov, Roi Reichart",
        "abstract": "Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable to semantic leakage, the unintended transfer of semantically related features between distinct entities. Existing mitigation strategies are often optimization-based or dependent on external inputs. We introduce DeLeaker, a lightweight, optimization-free inference-time approach that mitigates leakage by directly intervening on the model's attention maps. Throughout the diffusion process, DeLeaker dynamically reweights attention maps to suppress excessive cross-entity interactions while strengthening the identity of each entity. To support systematic evaluation, we introduce SLIM (Semantic Leakage in IMages), the first dataset dedicated to semantic leakage, comprising 1,130 human-verified samples spanning diverse scenarios, together with a novel automatic evaluation framework. Experiments demonstrate that DeLeaker consistently outperforms all baselines, even when they are provided with external information, achieving effective leakage mitigation without compromising fidelity or quality. These results underscore the value of attention control and pave the way for more semantically precise T2I models.",
        "timestamp": "2025-10-26T11:50:56.419Z",
        "rating": "novote",
        "publishedDate": "2025/10/16",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 513,
        "object_id": "paper:arxiv.2510.15015",
        "created_at": "2025-10-26T11:50:56+00:00",
        "updated_at": "2025-10-26T11:51:33+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.14223": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.14223",
        "interactions": []
      },
      "meta": {
        "issue_number": 512,
        "object_id": "interactions:arxiv.2509.14223",
        "created_at": "2025-10-26T10:36:02+00:00",
        "updated_at": "2025-10-26T10:36:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.15015": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.15015",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-26T11:55:06.490Z",
            "data": {
              "session_id": "session_1761479705752_rzk9hpu",
              "source_id": "arxiv",
              "paper_id": "2510.15015",
              "start_time": "2025-10-26T11:50:59.342Z",
              "end_time": "2025-10-26T11:55:05.752Z",
              "heartbeat_count": 49,
              "duration_seconds": 245,
              "idle_seconds": 1,
              "total_elapsed_seconds": 246
            }
          }
        ]
      },
      "meta": {
        "issue_number": 514,
        "object_id": "interactions:arxiv.2510.15015",
        "created_at": "2025-10-26T11:55:07+00:00",
        "updated_at": "2025-10-26T11:55:37+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2102.01017": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2102.01017",
        "url": "https://arxiv.org/abs/2102.01017",
        "title": "Measuring and Improving Consistency in Pretrained Language Models",
        "authors": "Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, Yoav Goldberg",
        "abstract": "Consistency of a model -- that is, the invariance of its behavior under meaning-preserving alternations in its input -- is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRel, we show that the consistency of all PLMs we experiment with is poor -- though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.",
        "timestamp": "2025-10-26T14:50:18.437Z",
        "rating": "novote",
        "publishedDate": "2021/02/01",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 515,
        "object_id": "paper:arxiv.2102.01017",
        "created_at": "2025-10-26T14:50:18+00:00",
        "updated_at": "2025-10-26T14:50:44+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2303.14186": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2303.14186",
        "url": "https://arxiv.org/pdf/2303.14186",
        "title": "TRAK: Attributing Model Behavior at Scale",
        "authors": "Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, Aleksander Madry",
        "abstract": "The goal of data attribution is to trace model predictions back to training\ndata. Despite a long line of work towards this goal, existing approaches to\ndata attribution tend to force users to choose between computational\ntractability and efficacy. That is, computationally tractable methods can\nstruggle with accurately attributing model predictions in non-convex settings\n(e.g., in the context of deep neural networks), while methods that are\neffective in such regimes require training thousands of models, which makes\nthem impractical for large models or datasets.\n  In this work, we introduce TRAK (Tracing with the Randomly-projected After\nKernel), a data attribution method that is both effective and computationally\ntractable for large-scale, differentiable models. In particular, by leveraging\nonly a handful of trained models, TRAK can match the performance of attribution\nmethods that require training thousands of models. We demonstrate the utility\nof TRAK across various modalities and scales: image classifiers trained on\nImageNet, vision-language models (CLIP), and language models (BERT and mT5). We\nprovide code for using TRAK (and reproducing our work) at\nhttps://github.com/MadryLab/trak .",
        "timestamp": "2025-10-27T06:51:30.030Z",
        "rating": "novote",
        "publishedDate": "2023-03-24T17:56:22Z",
        "tags": [
          "stat.ML",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 516,
        "object_id": "paper:arxiv.2303.14186",
        "created_at": "2025-10-27T06:51:30+00:00",
        "updated_at": "2025-10-27T06:52:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2303.14186": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2303.14186",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-27T06:52:33.621Z",
            "data": {
              "session_id": "session_1761547952764_ybmrtsd",
              "source_id": "arxiv",
              "paper_id": "2303.14186",
              "start_time": "2025-10-27T06:51:32.882Z",
              "end_time": "2025-10-27T06:52:32.764Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 5,
              "total_elapsed_seconds": 60
            }
          }
        ]
      },
      "meta": {
        "issue_number": 517,
        "object_id": "interactions:arxiv.2303.14186",
        "created_at": "2025-10-27T06:52:34+00:00",
        "updated_at": "2025-10-27T06:53:03+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2105.01560": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2105.01560",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-27T13:24:25.962Z",
            "data": {
              "session_id": "session_1761571465926_xd8ra0q",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-27T13:24:11.627Z",
              "end_time": "2025-10-27T13:24:25.926Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-27T13:26:17.753Z",
            "data": {
              "session_id": "session_1761571577011_4ml86dm",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-27T13:25:42.996Z",
              "end_time": "2025-10-27T13:26:17.011Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-27T13:31:04.305Z",
            "data": {
              "session_id": "session_1761571863422_5urh8re",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-27T13:29:16.581Z",
              "end_time": "2025-10-27T13:31:03.422Z",
              "heartbeat_count": 21,
              "duration_seconds": 105,
              "idle_seconds": 2,
              "total_elapsed_seconds": 107
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-27T13:33:05.310Z",
            "data": {
              "session_id": "session_1761571985300_3stqewx",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-27T13:32:59.453Z",
              "end_time": "2025-10-27T13:33:05.300Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-27T13:44:28.376Z",
            "data": {
              "session_id": "session_1761572668369_q8dirrb",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-27T13:44:21.293Z",
              "end_time": "2025-10-27T13:44:28.369Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-27T14:57:04.627Z",
            "data": {
              "session_id": "session_1761577024614_cuxjcjn",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-27T14:56:58.770Z",
              "end_time": "2025-10-27T14:57:04.614Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-27T20:44:44.714Z",
            "data": {
              "session_id": "session_1761597884692_j7ts5ho",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-27T20:44:36.420Z",
              "end_time": "2025-10-27T20:44:44.692Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-27T20:47:09.525Z",
            "data": {
              "session_id": "session_1761598029175_aa7bkjv",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-27T20:46:20.277Z",
              "end_time": "2025-10-27T20:47:09.175Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 4,
              "total_elapsed_seconds": 49
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T07:45:49.707Z",
            "data": {
              "session_id": "session_1761637548928_5meqa5u",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-28T07:43:06.221Z",
              "end_time": "2025-10-28T07:45:48.928Z",
              "heartbeat_count": 32,
              "duration_seconds": 160,
              "idle_seconds": 3,
              "total_elapsed_seconds": 163
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T07:51:37.263Z",
            "data": {
              "session_id": "session_1761637897246_0gilcoz",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-28T07:50:44.459Z",
              "end_time": "2025-10-28T07:51:37.246Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 3,
              "total_elapsed_seconds": 53
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T13:20:38.298Z",
            "data": {
              "session_id": "session_1761657638283_6151fjc",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-28T13:20:25.276Z",
              "end_time": "2025-10-28T13:20:38.283Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T13:25:49.311Z",
            "data": {
              "session_id": "session_1761657949294_tjvekoj",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-28T13:25:37.263Z",
              "end_time": "2025-10-28T13:25:49.294Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T13:39:03.327Z",
            "data": {
              "session_id": "session_1761658742582_6brv5nz",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-28T13:37:19.348Z",
              "end_time": "2025-10-28T13:39:02.582Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 3,
              "total_elapsed_seconds": 103
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T07:40:29.045Z",
            "data": {
              "session_id": "session_1761723628769_ksyqj3e",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-29T07:40:23.552Z",
              "end_time": "2025-10-29T07:40:28.769Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T09:40:30.716Z",
            "data": {
              "session_id": "session_1761730830421_859e42r",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-29T09:40:17.675Z",
              "end_time": "2025-10-29T09:40:30.420Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T09:48:47.525Z",
            "data": {
              "session_id": "session_1761731326879_ba4yyye",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-29T09:48:17.760Z",
              "end_time": "2025-10-29T09:48:46.879Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T10:02:02.091Z",
            "data": {
              "session_id": "session_1761732121441_uy99f3o",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-29T09:57:42.163Z",
              "end_time": "2025-10-29T10:02:01.441Z",
              "heartbeat_count": 51,
              "duration_seconds": 255,
              "idle_seconds": 4,
              "total_elapsed_seconds": 259
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T12:27:12.064Z",
            "data": {
              "session_id": "session_1761740832060_4262cqb",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-29T12:27:05.692Z",
              "end_time": "2025-10-29T12:27:12.060Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T07:16:24.048Z",
            "data": {
              "session_id": "session_1761808583514_3jy7nr3",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-30T07:08:51.556Z",
              "end_time": "2025-10-30T07:16:23.514Z",
              "heartbeat_count": 90,
              "duration_seconds": 450,
              "idle_seconds": 2,
              "total_elapsed_seconds": 452
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T13:21:17.626Z",
            "data": {
              "session_id": "session_1761830477122_usqesw2",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-30T13:20:45.973Z",
              "end_time": "2025-10-30T13:21:17.122Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 1,
              "total_elapsed_seconds": 31
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T13:22:20.058Z",
            "data": {
              "session_id": "session_1761830539235_9ikuvvk",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-30T13:21:27.859Z",
              "end_time": "2025-10-30T13:22:19.235Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 1,
              "total_elapsed_seconds": 51
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T13:44:54.993Z",
            "data": {
              "session_id": "session_1761831894966_nov3rjo",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-30T13:44:44.504Z",
              "end_time": "2025-10-30T13:44:54.966Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T14:01:18.386Z",
            "data": {
              "session_id": "session_1761832878366_afd4eom",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-30T14:00:45.283Z",
              "end_time": "2025-10-30T14:01:18.366Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 3,
              "total_elapsed_seconds": 33
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T16:51:00.993Z",
            "data": {
              "session_id": "session_1761843060252_unkzew3",
              "source_id": "arxiv",
              "paper_id": "2105.01560",
              "start_time": "2025-10-30T16:38:48.050Z",
              "end_time": "2025-10-30T16:51:00.252Z",
              "heartbeat_count": 146,
              "duration_seconds": 730,
              "idle_seconds": 2,
              "total_elapsed_seconds": 732
            }
          }
        ]
      },
      "meta": {
        "issue_number": 519,
        "object_id": "interactions:arxiv.2105.01560",
        "created_at": "2025-10-27T13:23:51+00:00",
        "updated_at": "2025-10-30T16:51:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2105.01560": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2105.01560",
        "url": "https://arxiv.org/pdf/2105.01560",
        "title": "Broadly Applicable Targeted Data Sample Omission Attacks",
        "authors": "Guy Barash, Eitan Farchi, Sarit Kraus, Onn Shehory",
        "abstract": "We introduce a novel clean-label targeted poisoning attack on learning\nmechanisms. While classical poisoning attacks typically corrupt data via\naddition, modification and omission, our attack focuses on data omission only.\nOur attack misclassifies a single, targeted test sample of choice, without\nmanipulating that sample. We demonstrate the effectiveness of omission attacks\nagainst a large variety of learners including deep neural networks, SVM and\ndecision trees, using several datasets including MNIST, IMDB and CIFAR. The\nfocus of our attack on data omission only is beneficial as well, as it is\nsimpler to implement and analyze. We show that, with a low attack budget, our\nattack's success rate is above 80%, and in some cases 100%, for white-box\nlearning. It is systematically above the reference benchmark for black-box\nlearning. For both white-box and black-box cases, changes in model accuracy are\nnegligible, regardless of the specific learner and dataset. We also prove\ntheoretically in a simplified agnostic PAC learning framework that, subject to\ndataset size and distribution, our omission attack succeeds with high\nprobability against any successful simplified agnostic PAC learner.",
        "timestamp": "2025-10-27T13:25:43.777Z",
        "rating": "novote",
        "publishedDate": "2021-05-04T15:20:54Z",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 520,
        "object_id": "paper:arxiv.2105.01560",
        "created_at": "2025-10-27T13:25:44+00:00",
        "updated_at": "2025-10-27T13:26:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1712.09665": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1712.09665",
        "url": "https://arxiv.org/pdf/1712.09665",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-10-27T13:31:13.476Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 521,
        "object_id": "paper:arxiv.1712.09665",
        "created_at": "2025-10-27T13:31:14+00:00",
        "updated_at": "2025-10-27T13:31:40+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.08825": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.08825",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T13:11:40.490Z",
            "data": {
              "session_id": "session_1761657099351_67hq50v",
              "source_id": "arxiv",
              "paper_id": "2509.08825",
              "start_time": "2025-10-28T13:09:44.426Z",
              "end_time": "2025-10-28T13:11:39.351Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 5,
              "total_elapsed_seconds": 115
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T12:14:13.196Z",
            "data": {
              "session_id": "session_1761826453184_v0zx270",
              "source_id": "arxiv",
              "paper_id": "2509.08825",
              "start_time": "2025-10-30T12:13:36.034Z",
              "end_time": "2025-10-30T12:14:13.184Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T07:23:57.087Z",
            "data": {
              "session_id": "session_1761981836943_mhw3kte",
              "source_id": "arxiv",
              "paper_id": "2509.08825",
              "start_time": "2025-11-01T07:23:48.477Z",
              "end_time": "2025-11-01T07:23:56.943Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 522,
        "object_id": "interactions:arxiv.2509.08825",
        "created_at": "2025-10-28T13:11:41+00:00",
        "updated_at": "2025-11-01T07:24:11+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.16322": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.16322",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T13:35:13.782Z",
            "data": {
              "session_id": "session_1761658513762_rcejowb",
              "source_id": "arxiv",
              "paper_id": "2510.16322",
              "start_time": "2025-10-28T13:34:52.033Z",
              "end_time": "2025-10-28T13:35:13.762Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T08:54:55.158Z",
            "data": {
              "session_id": "session_1761728094232_79ggv6m",
              "source_id": "arxiv",
              "paper_id": "2510.16322",
              "start_time": "2025-10-29T08:54:46.897Z",
              "end_time": "2025-10-29T08:54:54.232Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 524,
        "object_id": "interactions:arxiv.2510.16322",
        "created_at": "2025-10-28T13:35:14+00:00",
        "updated_at": "2025-10-29T08:55:22+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.16322": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.16322",
        "url": "https://arxiv.org/abs/2510.16322",
        "title": "Memorizing Long-tail Data Can Help Generalization Through Composition",
        "authors": "Mo Zhou, Haoyang Ma, Rong Ge",
        "abstract": "Deep learning has led researchers to rethink the relationship between memorization and generalization. In many settings, memorization does not hurt generalization due to implicit regularization and may help by memorizing long-tailed examples. In this paper, we consider the synergy between memorization and simple composition -- the ability to make correct prediction on a combination of long-tailed features. Theoretically, we show that for a linear setting, memorization together with composition can help the model make correct predictions on rare test examples that require a combination of long-tailed features, even if such combinations were never observed in the training data. Experiments on neural network architecture on simple data show that the theoretical insight extends beyond the linear setting, and we further observe that the composition capability of the model depends on its architecture.",
        "timestamp": "2025-10-28T13:34:47.031Z",
        "rating": "novote",
        "publishedDate": "2025/10/18",
        "tags": [
          "Machine Learning (cs.LG)",
          "Machine Learning (stat.ML)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 523,
        "object_id": "paper:arxiv.2510.16322",
        "created_at": "2025-10-28T13:34:47+00:00",
        "updated_at": "2025-10-28T13:35:08+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.05578": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.05578",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-28T18:20:43.660Z",
            "data": {
              "session_id": "session_1761675643001_mjv35k5",
              "source_id": "arxiv",
              "paper_id": "2507.05578",
              "start_time": "2025-10-28T18:20:07.585Z",
              "end_time": "2025-10-28T18:20:43.001Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 0,
              "total_elapsed_seconds": 35
            }
          }
        ]
      },
      "meta": {
        "issue_number": 527,
        "object_id": "interactions:arxiv.2507.05578",
        "created_at": "2025-10-28T18:20:44+00:00",
        "updated_at": "2025-10-28T18:21:10+00:00",
        "version": 1
      }
    },
    "paper:openreview.HVWODwbrFK": {
      "data": {
        "sourceId": "openreview",
        "paperId": "HVWODwbrFK",
        "url": "https://openreview.net/pdf?id=HVWODwbrFK",
        "title": "HVWODwbrFK",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-10-28T18:20:43.386Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 526,
        "object_id": "paper:openreview.HVWODwbrFK",
        "created_at": "2025-10-28T18:20:43+00:00",
        "updated_at": "2025-10-28T18:21:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.05578": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.05578",
        "url": "https://arxiv.org/pdf/2507.05578",
        "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and\n  Mitigation",
        "authors": "Alexander Xiong, Xuandong Zhao, Aneesh Pappu, Dawn Song",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they also exhibit memorization of their training\ndata. This phenomenon raises critical questions about model behavior, privacy\nrisks, and the boundary between learning and memorization. Addressing these\nconcerns, this paper synthesizes recent studies and investigates the landscape\nof memorization, the factors influencing it, and methods for its detection and\nmitigation. We explore key drivers, including training data duplication,\ntraining dynamics, and fine-tuning procedures that influence data memorization.\nIn addition, we examine methodologies such as prefix-based extraction,\nmembership inference, and adversarial prompting, assessing their effectiveness\nin detecting and measuring memorized content. Beyond technical analysis, we\nalso explore the broader implications of memorization, including the legal and\nethical implications. Finally, we discuss mitigation strategies, including data\ncleaning, differential privacy, and post-training unlearning, while\nhighlighting open challenges in balancing the minimization of harmful\nmemorization with utility. This paper provides a comprehensive overview of the\ncurrent state of research on LLM memorization across technical, privacy, and\nperformance dimensions, identifying critical directions for future work.",
        "timestamp": "2025-10-28T18:20:07.944Z",
        "rating": "novote",
        "publishedDate": "2025-07-08T01:30:46Z",
        "tags": [
          "cs.LG",
          "cs.CL",
          "cs.CR"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 525,
        "object_id": "paper:arxiv.2507.05578",
        "created_at": "2025-10-28T18:20:08+00:00",
        "updated_at": "2025-10-28T18:20:33+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.21890": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.21890",
        "url": "https://www.arxiv.org/abs/2510.21890",
        "title": "The Principles of Diffusion Models",
        "authors": "Chieh-Hsin Lai, Yang Song, Dongjun Kim, Yuki Mitsufuji, Stefano Ermon",
        "abstract": "This monograph presents the core principles that have guided the development of diffusion models, tracing their origins and showing how diverse formulations arise from shared mathematical ideas. Diffusion modeling starts by defining a forward process that gradually corrupts data into noise, linking the data distribution to a simple prior through a continuum of intermediate distributions. The goal is to learn a reverse process that transforms noise back into data while recovering the same intermediates. We describe three complementary views. The variational view, inspired by variational autoencoders, sees diffusion as learning to remove noise step by step. The score-based view, rooted in energy-based modeling, learns the gradient of the evolving data distribution, indicating how to nudge samples toward more likely regions. The flow-based view, related to normalizing flows, treats generation as following a smooth path that moves samples from noise to data under a learned velocity field. These perspectives share a common backbone: a time-dependent velocity field whose flow transports a simple prior to the data. Sampling then amounts to solving a differential equation that evolves noise into data along a continuous trajectory. On this foundation, the monograph discusses guidance for controllable generation, efficient numerical solvers, and diffusion-motivated flow-map models that learn direct mappings between arbitrary times. It provides a conceptual and mathematically grounded understanding of diffusion models for readers with basic deep-learning knowledge.",
        "timestamp": "2025-10-29T10:25:24.234Z",
        "rating": "novote",
        "publishedDate": "2025/10/24",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)",
          "Graphics (cs.GR)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 528,
        "object_id": "paper:arxiv.2510.21890",
        "created_at": "2025-10-29T10:25:24+00:00",
        "updated_at": "2025-10-29T10:25:44+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.21890": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.21890",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T10:26:05.607Z",
            "data": {
              "session_id": "session_1761733564966_lyrsvm6",
              "source_id": "arxiv",
              "paper_id": "2510.21890",
              "start_time": "2025-10-29T10:25:26.235Z",
              "end_time": "2025-10-29T10:26:04.966Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 4,
              "total_elapsed_seconds": 39
            }
          }
        ]
      },
      "meta": {
        "issue_number": 529,
        "object_id": "interactions:arxiv.2510.21890",
        "created_at": "2025-10-29T10:26:06+00:00",
        "updated_at": "2025-10-29T10:26:38+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.19788": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.19788",
        "url": "https://arxiv.org/abs/2510.19788",
        "title": "Benchmarking World-Model Learning",
        "authors": "Archana Warrier, Dat Nguyen, Michelangelo Naim, Moksh Jain, Yichao Liang, Karen Schroeder, Cambridge Yang, Joshua B. Tenenbaum, Sebastian Vollmer, Kevin Ellis, Zenna Tavares",
        "abstract": "Model-learning agents should gather information to learn world models that support many downstream tasks and inferences, such as predicting unobserved states, estimating near- and far-term consequences of actions, planning action sequences, and detecting changes in dynamics. Current methods for learning and evaluating world models diverge from this goal: training and evaluation are anchored to next-frame prediction, and success is scored by reward maximization in the same environment. We propose WorldTest, a protocol to evaluate model-learning agents that separates reward-free interaction from a scored test phase in a different but related environment. WorldTest is open-ended$\\unicode{x2014}$models should support many different tasks unknown ahead of time$\\unicode{x2014}$and agnostic to model representation, allowing comparison across approaches. We instantiated WorldTest with AutumnBench, a suite of 43 interactive grid-world environments and 129 tasks across three families: masked-frame prediction, planning, and predicting changes to the causal dynamics. We compared 517 human participants and three frontier models on AutumnBench. We found that humans outperform the models, and scaling compute improves performance only in some environments but not others. WorldTest provides a novel template$\\unicode{x2014}$reward-free exploration, derived tests, and behavior-based scoring$\\unicode{x2014}$to evaluate what agents learn about environment dynamics, and AutumnBench exposes significant headroom in world-model learning.",
        "timestamp": "2025-10-29T12:49:25.201Z",
        "rating": "novote",
        "publishedDate": "2025/10/22",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 530,
        "object_id": "paper:arxiv.2510.19788",
        "created_at": "2025-10-29T12:49:25+00:00",
        "updated_at": "2025-10-29T12:49:49+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.19788": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.19788",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T12:49:51.738Z",
            "data": {
              "session_id": "session_1761742191439_0zz8kfh",
              "source_id": "arxiv",
              "paper_id": "2510.19788",
              "start_time": "2025-10-29T12:49:28.085Z",
              "end_time": "2025-10-29T12:49:51.439Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T12:51:01.512Z",
            "data": {
              "session_id": "session_1761742261166_1p05enb",
              "source_id": "arxiv",
              "paper_id": "2510.19788",
              "start_time": "2025-10-29T12:50:32.790Z",
              "end_time": "2025-10-29T12:51:01.166Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T14:53:10.997Z",
            "data": {
              "session_id": "session_1761749590992_wa8rp8n",
              "source_id": "arxiv",
              "paper_id": "2510.19788",
              "start_time": "2025-10-29T14:52:57.346Z",
              "end_time": "2025-10-29T14:53:10.992Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T16:06:52.065Z",
            "data": {
              "session_id": "session_1761754012060_jl3ltbb",
              "source_id": "arxiv",
              "paper_id": "2510.19788",
              "start_time": "2025-10-29T16:06:45.284Z",
              "end_time": "2025-10-29T16:06:52.060Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T16:08:34.439Z",
            "data": {
              "session_id": "session_1761754114429_axic31f",
              "source_id": "arxiv",
              "paper_id": "2510.19788",
              "start_time": "2025-10-29T16:08:28.899Z",
              "end_time": "2025-10-29T16:08:34.429Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T16:19:35.159Z",
            "data": {
              "session_id": "session_1761754774474_jv6sj5s",
              "source_id": "arxiv",
              "paper_id": "2510.19788",
              "start_time": "2025-10-29T16:15:59.718Z",
              "end_time": "2025-10-29T16:19:34.474Z",
              "heartbeat_count": 42,
              "duration_seconds": 210,
              "idle_seconds": 5,
              "total_elapsed_seconds": 215
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T17:53:39.932Z",
            "data": {
              "session_id": "session_1761760419074_puqvg7k",
              "source_id": "arxiv",
              "paper_id": "2510.19788",
              "start_time": "2025-10-29T17:50:45.827Z",
              "end_time": "2025-10-29T17:53:39.074Z",
              "heartbeat_count": 34,
              "duration_seconds": 170,
              "idle_seconds": 3,
              "total_elapsed_seconds": 173
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T06:15:03.820Z",
            "data": {
              "session_id": "session_1761804903550_lzu1sss",
              "source_id": "arxiv",
              "paper_id": "2510.19788",
              "start_time": "2025-10-30T06:14:56.524Z",
              "end_time": "2025-10-30T06:15:03.550Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T06:20:22.063Z",
            "data": {
              "session_id": "session_1761805222045_n7vg3hv",
              "source_id": "arxiv",
              "paper_id": "2510.19788",
              "start_time": "2025-10-30T06:20:12.588Z",
              "end_time": "2025-10-30T06:20:22.045Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T06:22:52.483Z",
            "data": {
              "session_id": "session_1761805371961_7xi5uz3",
              "source_id": "arxiv",
              "paper_id": "2510.19788",
              "start_time": "2025-10-30T06:21:17.916Z",
              "end_time": "2025-10-30T06:22:51.961Z",
              "heartbeat_count": 18,
              "duration_seconds": 90,
              "idle_seconds": 4,
              "total_elapsed_seconds": 94
            }
          }
        ]
      },
      "meta": {
        "issue_number": 531,
        "object_id": "interactions:arxiv.2510.19788",
        "created_at": "2025-10-29T12:49:52+00:00",
        "updated_at": "2025-10-30T06:23:17+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.19133": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.19133",
        "url": "https://arxiv.org/abs/2410.19133",
        "title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback",
        "authors": "Lester James V. Miranda, Yizhong Wang, Yanai Elazar, Sachin Kumar, Valentina Pyatkin, Faeze Brahman, Noah A. Smith, Hannaneh Hajishirzi, Pradeep Dasigi",
        "abstract": "Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, collecting human preferences is expensive and time-consuming, with highly variable annotation quality. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations, offering a cost-effective and scalable alternative, albeit susceptible to other biases and errors. In this work, we introduce HyPER, a Hybrid Preference routER that defers an annotation to either humans or LMs, achieving better annotation quality while reducing the cost of human-only annotation. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we (1) train a performance prediction model (PPM) to predict a reward model's (RM) performance on an arbitrary combination of human and LM annotations and (2) employ a routing strategy that selects a combination that maximizes the predicted performance. We train the PPM on MultiPref, a new preference dataset with 10k instances paired with humans and LM labels. We show that the selected hybrid mixture of synthetic and direct human preferences using HyPER achieves better RM performance compared to using either one exclusively by 7-13% on RewardBench and generalizes across unseen preference datasets and other base models. We also observe the same trend in other benchmarks using Best-of-N reranking, where the hybrid mix has 2-3% better performance. Finally, we analyze features from HyPER and find that prompts with moderate safety concerns or complexity benefit the most from human feedback.",
        "timestamp": "2025-10-29T15:53:44.263Z",
        "rating": "novote",
        "publishedDate": "2024/10/24",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 532,
        "object_id": "paper:arxiv.2410.19133",
        "created_at": "2025-10-29T15:53:44+00:00",
        "updated_at": "2025-10-29T15:54:04+00:00",
        "version": 1
      }
    },
    "paper:openreview.Vp3kPY8gKe": {
      "data": {
        "sourceId": "openreview",
        "paperId": "Vp3kPY8gKe",
        "url": "https://openreview.net/forum?id=Vp3kPY8gKe&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech",
        "authors": "Naama Rivlin-Angert, Guy Mor-Lan",
        "abstract": "We present the first large-scale computational study of political delegitimization discourse (PDD), defined as symbolic attacks on the normative validity of political entities. We curate and manually annotate a novel Hebrew-language corpus of 10,410 sentences drawn from Knesset speeches (1993-2023), Facebook posts (2018-2021), and leading news outlets, of which 1,812 instances (17.4%) exhibit PDD and 642 carry additional annotations for intensity, incivility, target type, and affective framing. We introduce a two-stage classification pipeline combining finetuned encoder models and decoder LLMs. Our best model (DictaLM 2.0) attains an F1 of 0.74 for binary PDD detection and a macro-F1 of 0.67 for classification of delegitimization characteristics. Applying this classifier to longitudinal and cross-platform data, we see a marked rise in PDD over three decades, higher prevalence on social media versus parliamentary debate, greater use by male than female politicians, and stronger tendencies among right-leaning actors - with pronounced spikes during election campaigns and major political events. Our findings demonstrate the feasibility and value of automated PDD analysis for understanding democratic discourse.\n\nIn Proceedings of the Empirical Methods in Natural Language Processing (EMNLP), 2025. Association for Computational Linguistics.",
        "timestamp": "2025-10-29T16:40:46.437Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "rame detection and analysis",
          "argument mining",
          "discourse parsing",
          "hate-speech detection",
          "emotion detection and analysis",
          "quantitative analyses of news and/or social media"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 533,
        "object_id": "paper:openreview.Vp3kPY8gKe",
        "created_at": "2025-10-29T16:40:47+00:00",
        "updated_at": "2025-10-29T16:41:08+00:00",
        "version": 1
      }
    },
    "interactions:openreview.Vp3kPY8gKe": {
      "data": {
        "sourceId": "openreview",
        "paperId": "Vp3kPY8gKe",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-29T16:41:38.518Z",
            "data": {
              "session_id": "session_1761756098507_cr2enp4",
              "source_id": "openreview",
              "paper_id": "Vp3kPY8gKe",
              "start_time": "2025-10-29T16:41:26.368Z",
              "end_time": "2025-10-29T16:41:38.507Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T17:13:37.466Z",
            "data": {
              "session_id": "session_1762103617185_yxvy3k7",
              "source_id": "openreview",
              "paper_id": "Vp3kPY8gKe",
              "start_time": "2025-11-02T17:13:30.119Z",
              "end_time": "2025-11-02T17:13:37.185Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 534,
        "object_id": "interactions:openreview.Vp3kPY8gKe",
        "created_at": "2025-10-29T16:40:55+00:00",
        "updated_at": "2025-11-02T17:14:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.14242": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.14242",
        "url": "https://arxiv.org/pdf/2510.14242",
        "title": "Flip-Flop Consistency: Unsupervised Training for Robustness to Prompt\n  Perturbations in LLMs",
        "authors": "Parsa Hejabi, Elnaz Rahmati, Alireza S. Ziabari, Morteza Dehghani",
        "abstract": "Large Language Models (LLMs) often produce inconsistent answers when faced\nwith different phrasings of the same prompt. In this paper, we propose\nFlip-Flop Consistency ($F^2C$), an unsupervised training method that improves\nrobustness to such perturbations. $F^2C$ is composed of two key components. The\nfirst, Consensus Cross-Entropy (CCE), uses a majority vote across prompt\nvariations to create a hard pseudo-label. The second is a representation\nalignment loss that pulls lower-confidence and non-majority predictors toward\nthe consensus established by high-confidence, majority-voting variations. We\nevaluate our method on 11 datasets spanning four NLP tasks, with 4-15 prompt\nvariations per dataset. On average, $F^2C$ raises observed agreement by 11.62%,\nimproves mean $F_1$ by 8.94%, and reduces performance variance across formats\nby 3.29%. In out-of-domain evaluations, $F^2C$ generalizes effectively,\nincreasing $\\overline{F_1}$ and agreement while decreasing variance across most\nsource-target pairs. Finally, when trained on only a subset of prompt\nperturbations and evaluated on held-out formats, $F^2C$ consistently improves\nboth performance and agreement while reducing variance. These findings\nhighlight $F^2C$ as an effective unsupervised method for enhancing LLM\nconsistency, performance, and generalization under prompt perturbations. Code\nis available at\nhttps://github.com/ParsaHejabi/Flip-Flop-Consistency-Unsupervised-Training-for-Robustness-to-Prompt-Perturbations-in-LLMs.",
        "timestamp": "2025-10-30T06:42:32.746Z",
        "rating": "novote",
        "publishedDate": "2025-10-16T02:54:01Z",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 535,
        "object_id": "paper:arxiv.2510.14242",
        "created_at": "2025-10-30T06:42:33+00:00",
        "updated_at": "2025-10-30T06:42:54+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.14242": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.14242",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T06:43:50.906Z",
            "data": {
              "session_id": "session_1761806630226_7pqtqmm",
              "source_id": "arxiv",
              "paper_id": "2510.14242",
              "start_time": "2025-10-30T06:42:32.254Z",
              "end_time": "2025-10-30T06:43:50.226Z",
              "heartbeat_count": 15,
              "duration_seconds": 75,
              "idle_seconds": 3,
              "total_elapsed_seconds": 78
            }
          }
        ]
      },
      "meta": {
        "issue_number": 536,
        "object_id": "interactions:arxiv.2510.14242",
        "created_at": "2025-10-30T06:43:51+00:00",
        "updated_at": "2025-10-30T06:44:16+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.0804.2996": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "0804.2996",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T11:10:52.350Z",
            "data": {
              "session_id": "session_1761822652042_wyjmhop",
              "source_id": "arxiv",
              "paper_id": "0804.2996",
              "start_time": "2025-10-30T11:10:46.412Z",
              "end_time": "2025-10-30T11:10:52.042Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-31T08:06:41.199Z",
            "data": {
              "session_id": "session_1761898000612_2hezwzl",
              "source_id": "arxiv",
              "paper_id": "0804.2996",
              "start_time": "2025-10-31T08:04:27.128Z",
              "end_time": "2025-10-31T08:06:40.612Z",
              "heartbeat_count": 26,
              "duration_seconds": 130,
              "idle_seconds": 3,
              "total_elapsed_seconds": 133
            }
          }
        ]
      },
      "meta": {
        "issue_number": 537,
        "object_id": "interactions:arxiv.0804.2996",
        "created_at": "2025-10-30T11:10:53+00:00",
        "updated_at": "2025-10-31T08:07:10+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.04259": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.04259",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T12:02:34.428Z",
            "data": {
              "session_id": "session_1761825753591_tq8qw9h",
              "source_id": "arxiv",
              "paper_id": "2509.04259",
              "start_time": "2025-10-30T12:00:21.350Z",
              "end_time": "2025-10-30T12:02:33.591Z",
              "heartbeat_count": 26,
              "duration_seconds": 130,
              "idle_seconds": 2,
              "total_elapsed_seconds": 132
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T12:05:06.403Z",
            "data": {
              "session_id": "session_1761825906311_fdje79b",
              "source_id": "arxiv",
              "paper_id": "2509.04259",
              "start_time": "2025-10-30T12:04:39.295Z",
              "end_time": "2025-10-30T12:05:06.311Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          }
        ]
      },
      "meta": {
        "issue_number": 538,
        "object_id": "interactions:arxiv.2509.04259",
        "created_at": "2025-10-30T12:02:35+00:00",
        "updated_at": "2025-10-30T12:05:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2401.12973": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.12973",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T12:05:19.786Z",
            "data": {
              "session_id": "session_1761825919464_lwuutmk",
              "source_id": "arxiv",
              "paper_id": "2401.12973",
              "start_time": "2025-10-30T12:05:10.458Z",
              "end_time": "2025-10-30T12:05:19.464Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T12:06:28.386Z",
            "data": {
              "session_id": "session_1761825987495_9c97o6s",
              "source_id": "arxiv",
              "paper_id": "2401.12973",
              "start_time": "2025-10-30T12:05:19.464Z",
              "end_time": "2025-10-30T12:06:27.495Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 3,
              "total_elapsed_seconds": 68
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T12:12:09.389Z",
            "data": {
              "session_id": "session_1761826329051_k55viic",
              "source_id": "arxiv",
              "paper_id": "2401.12973",
              "start_time": "2025-10-30T12:12:02.511Z",
              "end_time": "2025-10-30T12:12:09.051Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 539,
        "object_id": "interactions:arxiv.2401.12973",
        "created_at": "2025-10-30T12:05:20+00:00",
        "updated_at": "2025-10-30T12:12:38+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.16022": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.16022",
        "url": "https://arxiv.org/abs/2510.16022",
        "title": "Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization",
        "authors": "Changsheng Wang, Xin Chen, Sijia Liu, Ke Ding",
        "abstract": "Adapting pretrained large language models (LLMs) to code domains via supervised fine-tuning (FT) has been commonly used for code generation. However, we identify a previously underappreciated failure mode, the memorization barrier, where strong memorization of downstream code data in the base model could trap optimization and prevent the standard FT from effectively acquiring new, generalizable code knowledge. To overcome this barrier, we propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which applies an IB penalty on hidden representations of the code data to compress spurious, memorized features while preserving task-relevant information. Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1) show that IB-FT substantially alleviates the memorization barrier, improves top-1 performance (Pass@11), and yields far more stable gains under the stricter multi-sample metric Pass@k(m)k^{(m)} (a problem counts as solved only if at least mm of kk samples pass unit tests) compared with conventional FT.",
        "timestamp": "2025-10-30T13:15:59.075Z",
        "rating": "novote",
        "publishedDate": "2025/10/15",
        "tags": [
          "Machine Learning (cs.LG)",
          "Software Engineering (cs.SE)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 540,
        "object_id": "paper:arxiv.2510.16022",
        "created_at": "2025-10-30T13:15:59+00:00",
        "updated_at": "2025-10-30T13:16:26+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.16022": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.16022",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T13:20:45.286Z",
            "data": {
              "session_id": "session_1761830444784_aqtbcrq",
              "source_id": "arxiv",
              "paper_id": "2510.16022",
              "start_time": "2025-10-30T13:20:12.885Z",
              "end_time": "2025-10-30T13:20:44.784Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "issue_number": 541,
        "object_id": "interactions:arxiv.2510.16022",
        "created_at": "2025-10-30T13:20:46+00:00",
        "updated_at": "2025-10-30T13:21:30+00:00",
        "version": 1
      }
    },
    "interactions:openreview.nTBS7T5qQR": {
      "data": {
        "sourceId": "openreview",
        "paperId": "nTBS7T5qQR",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-30T13:26:45.037Z",
            "data": {
              "session_id": "session_1761830804379_thsqtvc",
              "source_id": "openreview",
              "paper_id": "nTBS7T5qQR",
              "start_time": "2025-10-30T13:26:29.053Z",
              "end_time": "2025-10-30T13:26:44.379Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 543,
        "object_id": "interactions:openreview.nTBS7T5qQR",
        "created_at": "2025-10-30T13:26:45+00:00",
        "updated_at": "2025-10-30T13:27:08+00:00",
        "version": 1
      }
    },
    "paper:openreview.nTBS7T5qQR": {
      "data": {
        "sourceId": "openreview",
        "paperId": "nTBS7T5qQR",
        "url": "https://openreview.net/forum?id=nTBS7T5qQR&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature",
        "authors": "Noy Sternlicht, Tom Hope",
        "abstract": "A hallmark of human innovation is recombination---the creation of novel ideas by integrating elements from existing concepts and mechanisms. In this work, we introduce CHIMERA, a large-scale Knowledge Base (KB) of over 28K recombination examples automatically mined from the scientific literature. CHIMERA enables large-scale empirical analysis of how scientists recombine concepts and draw inspiration from different areas, and enables training models that propose novel, cross-disciplinary research directions. To construct this KB, we define a new information extraction task: identifying recombination instances in scientific abstracts. We curate a high-quality, expert-annotated dataset and use it to fine-tune a large language model, which we apply to a broad corpus of AI papers. We showcase the utility of CHIMERA through two applications. First, we analyze patterns of recombination across AI subfields. Second, we train a scientific hypothesis generation model using the KB, showing that it can propose novel research directions that researchers rate as inspiring. We release our data and code at https://github.com/noy-sternlicht/CHIMERA-KB.",
        "timestamp": "2025-10-30T13:26:29.834Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "knowledge graphs"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 542,
        "object_id": "paper:openreview.nTBS7T5qQR",
        "created_at": "2025-10-30T13:26:30+00:00",
        "updated_at": "2025-10-30T13:26:56+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.25786": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.25786",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-31T12:30:30.681Z",
            "data": {
              "session_id": "session_1761913830634_ro00vem",
              "source_id": "arxiv",
              "paper_id": "2510.25786",
              "start_time": "2025-10-31T12:29:57.814Z",
              "end_time": "2025-10-31T12:30:30.634Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 3,
              "total_elapsed_seconds": 33
            }
          }
        ]
      },
      "meta": {
        "issue_number": 545,
        "object_id": "interactions:arxiv.2510.25786",
        "created_at": "2025-10-31T12:29:59+00:00",
        "updated_at": "2025-10-31T12:30:49+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.25786": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.25786",
        "url": "https://arxiv.org/abs/2510.25786",
        "title": "BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection",
        "authors": "Yaniv Nikankin, Dana Arad, Itay Itzhak, Anja Reusch, Adi Simhi, Gal Kesten-Pomeranz, Yonatan Belinkov",
        "abstract": "One of the main challenges in mechanistic interpretability is circuit discovery, determining which parts of a model perform a given task. We build on the Mechanistic Interpretability Benchmark (MIB) and propose three key improvements to circuit discovery. First, we use bootstrapping to identify edges with consistent attribution scores. Second, we introduce a simple ratio-based selection strategy to prioritize strong positive-scoring edges, balancing performance and faithfulness. Third, we replace the standard greedy selection with an integer linear programming formulation. Our methods yield more faithful circuits and outperform prior approaches across multiple MIB tasks and models. Our code is available at: this https URL.",
        "timestamp": "2025-10-31T12:29:50.322Z",
        "rating": "novote",
        "publishedDate": "2025/10/28",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 544,
        "object_id": "paper:arxiv.2510.25786",
        "created_at": "2025-10-31T12:29:50+00:00",
        "updated_at": "2025-10-31T12:30:14+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.15299": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.15299",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T07:55:22.069Z",
            "data": {
              "session_id": "session_1761983721753_3frb8cs",
              "source_id": "arxiv",
              "paper_id": "2503.15299",
              "start_time": "2025-11-01T07:55:12.687Z",
              "end_time": "2025-11-01T07:55:21.753Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 548,
        "object_id": "interactions:arxiv.2503.15299",
        "created_at": "2025-11-01T07:55:22+00:00",
        "updated_at": "2025-11-01T07:55:48+00:00",
        "version": 1
      }
    },
    "interactions:openreview.f7GG1MbsSM": {
      "data": {
        "sourceId": "openreview",
        "paperId": "f7GG1MbsSM",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T07:55:09.461Z",
            "data": {
              "session_id": "session_1761983708727_8hi5xm4",
              "source_id": "openreview",
              "paper_id": "f7GG1MbsSM",
              "start_time": "2025-11-01T07:24:14.852Z",
              "end_time": "2025-11-01T07:55:08.727Z",
              "heartbeat_count": 370,
              "duration_seconds": 1850,
              "idle_seconds": 4,
              "total_elapsed_seconds": 1854
            }
          }
        ]
      },
      "meta": {
        "issue_number": 547,
        "object_id": "interactions:openreview.f7GG1MbsSM",
        "created_at": "2025-11-01T07:55:10+00:00",
        "updated_at": "2025-11-01T07:55:30+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.15299": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.15299",
        "url": "https://arxiv.org/abs/2503.15299",
        "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
        "authors": "Zorik Gekhman, Eyal Ben David, Hadas Orgad, Eran Ofek, Yonatan Belinkov, Idan Szpektor, Jonathan Herzig, Roi Reichart",
        "abstract": "This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a formal definition of knowledge, quantifying it for a given question as the fraction of correct-incorrect answer pairs where the correct one is ranked higher. This gives rise to external and internal knowledge, depending on the information used to score individual answer candidates: either the model's observable token-level probabilities or its intermediate computations. Hidden knowledge arises when internal knowledge exceeds external knowledge. We then present a case study, applying this framework to three popular open-weights LLMs in a closed-book QA setup. Our results indicate that: (1) LLMs consistently encode more factual knowledge internally than what they express externally, with an average relative gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a model can internally know an answer perfectly, yet fail to generate it even once, despite large-scale repeated sampling of 1,000 answers. This reveals fundamental limitations in the generation capabilities of LLMs, which (3) put a practical constraint on scaling test-time compute via repeated answer sampling in closed-book QA: significant performance improvements remain inaccessible because some answers are practically never sampled, yet if they were, we would be guaranteed to rank them first.",
        "timestamp": "2025-11-01T07:55:09.001Z",
        "rating": "novote",
        "publishedDate": "2025/03/19",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 546,
        "object_id": "paper:arxiv.2503.15299",
        "created_at": "2025-11-01T07:55:09+00:00",
        "updated_at": "2025-11-01T07:55:30+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.20481": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.20481",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T09:50:03.848Z",
            "data": {
              "session_id": "session_1761990603572_zej1p6k",
              "source_id": "arxiv",
              "paper_id": "2506.20481",
              "start_time": "2025-11-01T09:49:45.164Z",
              "end_time": "2025-11-01T09:50:03.572Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "issue_number": 550,
        "object_id": "interactions:arxiv.2506.20481",
        "created_at": "2025-11-01T09:50:04+00:00",
        "updated_at": "2025-11-01T09:50:26+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.00239": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.00239",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T09:49:44.913Z",
            "data": {
              "session_id": "session_1761990584824_d75ea0z",
              "source_id": "arxiv",
              "paper_id": "2507.00239",
              "start_time": "2025-11-01T09:49:29.285Z",
              "end_time": "2025-11-01T09:49:44.824Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "issue_number": 549,
        "object_id": "interactions:arxiv.2507.00239",
        "created_at": "2025-11-01T09:49:30+00:00",
        "updated_at": "2025-11-01T09:49:55+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2310.00902": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.00902",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T10:36:08.719Z",
            "data": {
              "session_id": "session_1761993368459_nt8yvnq",
              "source_id": "arxiv",
              "paper_id": "2310.00902",
              "start_time": "2025-11-01T10:36:01.333Z",
              "end_time": "2025-11-01T10:36:08.459Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 551,
        "object_id": "interactions:arxiv.2310.00902",
        "created_at": "2025-11-01T10:36:09+00:00",
        "updated_at": "2025-11-01T10:36:34+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.16189": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.16189",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T10:44:29.190Z",
            "data": {
              "session_id": "session_1761993868646_j7moiaa",
              "source_id": "arxiv",
              "paper_id": "2509.16189",
              "start_time": "2025-11-01T10:41:50.460Z",
              "end_time": "2025-11-01T10:44:28.646Z",
              "heartbeat_count": 31,
              "duration_seconds": 155,
              "idle_seconds": 3,
              "total_elapsed_seconds": 158
            }
          }
        ]
      },
      "meta": {
        "issue_number": 552,
        "object_id": "interactions:arxiv.2509.16189",
        "created_at": "2025-11-01T10:44:29+00:00",
        "updated_at": "2025-11-01T10:44:53+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.03231": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.03231",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T10:55:18.565Z",
            "data": {
              "session_id": "session_1761994517967_oakjahc",
              "source_id": "arxiv",
              "paper_id": "2510.03231",
              "start_time": "2025-11-01T10:50:22.478Z",
              "end_time": "2025-11-01T10:55:17.967Z",
              "heartbeat_count": 59,
              "duration_seconds": 295,
              "idle_seconds": 0,
              "total_elapsed_seconds": 295
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T15:20:28.539Z",
            "data": {
              "session_id": "session_1762010427951_6twhn39",
              "source_id": "arxiv",
              "paper_id": "2510.03231",
              "start_time": "2025-11-01T15:18:05.764Z",
              "end_time": "2025-11-01T15:20:27.951Z",
              "heartbeat_count": 28,
              "duration_seconds": 140,
              "idle_seconds": 2,
              "total_elapsed_seconds": 142
            }
          }
        ]
      },
      "meta": {
        "issue_number": 553,
        "object_id": "interactions:arxiv.2510.03231",
        "created_at": "2025-11-01T10:55:19+00:00",
        "updated_at": "2025-11-01T15:20:48+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.03415": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.03415",
        "url": "https://arxiv.org/abs/2510.03415",
        "title": "PLSemanticsBench: Large Language Models As Programming Language Interpreters",
        "authors": "Aditya Thimmaiah, Jiyang Zhang, Jayanth Srinivasa, Junyi Jessy Li, Milos Gligoric",
        "abstract": "As large language models (LLMs) excel at code reasoning, a natural question arises: can an LLM execute programs (i.e., act as an interpreter) purely based on a programming language's formal semantics? If so, it will enable rapid prototyping of new programming languages and language features. We study this question using the imperative language IMP (a subset of C), formalized via small-step operational semantics (SOS) and rewriting-based operational semantics (K-semantics). We introduce three evaluation sets-Human-Written, LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by code-complexity metrics spanning the size, control-flow, and data-flow axes. Given a program and its semantics formalized with SOS/K-semantics, models are evaluated on three tasks ranging from coarse to fine: (1) final-state prediction, (2) semantic rule prediction, and (3) execution trace prediction. To distinguish pretraining memorization from semantic competence, we define two nonstandard semantics obtained through systematic mutations of the standard rules. Across strong code/reasoning LLMs, performance drops under nonstandard semantics despite high performance under the standard one. We further find that (i) there are patterns to different model failures, (ii) most reasoning models perform exceptionally well on coarse grained tasks involving reasoning about highly complex programs often containing nested loop depths beyond five, and surprisingly, (iii) providing formal semantics helps on simple programs but often hurts on more complex ones. Overall, the results show a promise that LLMs could serve as programming language interpreters, but points to the lack of their robust semantics understanding. We release the benchmark and the supporting code at this https URL.",
        "timestamp": "2025-11-01T15:47:03.483Z",
        "rating": "novote",
        "publishedDate": "2025/10/03",
        "tags": [
          "Programming Languages (cs.PL)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)",
          "Software Engineering (cs.SE)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 554,
        "object_id": "paper:arxiv.2510.03415",
        "created_at": "2025-11-01T15:47:03+00:00",
        "updated_at": "2025-11-01T15:47:23+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.03415": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.03415",
        "interactions": []
      },
      "meta": {
        "issue_number": 555,
        "object_id": "interactions:arxiv.2510.03415",
        "created_at": "2025-11-01T15:50:50+00:00",
        "updated_at": "2025-11-01T15:50:52+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2312.07550": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2312.07550",
        "url": "https://arxiv.org/abs/2312.07550",
        "title": "Understanding (Un)Intended Memorization in Text-to-Image Generative Models",
        "authors": "Ali Naseh, Jaechul Roh, Amir Houmansadr",
        "abstract": "Multimodal machine learning, especially text-to-image models like Stable Diffusion and DALL-E 3, has gained significance for transforming text into detailed images.\nDespite their growing use and remarkable generative capabilities, there is a pressing need for a detailed examination of these models' behavior, particularly with respect to memorization. Historically, memorization in machine learning has been context-dependent, with diverse definitions emerging from classification tasks to complex models like Large Language Models (LLMs) and Diffusion models. Yet, a definitive concept of memorization that aligns with the intricacies of text-to-image synthesis remains elusive. This understanding is vital as memorization poses privacy risks yet is essential for meeting user expectations, especially when generating representations of underrepresented entities. In this paper, we introduce a specialized definition of memorization tailored to text-to-image models, categorizing it into three distinct types according to user expectations. We closely examine the subtle distinctions between intended and unintended memorization, emphasizing the importance of balancing user privacy with the generative quality of the model outputs. Using the Stable Diffusion model, we offer examples to validate our memorization definitions and clarify their application.",
        "timestamp": "2025-11-01T18:17:36.592Z",
        "rating": "novote",
        "publishedDate": "2023/12/06",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)",
          "Computation and Language (cs.CL)",
          "Cryptography and Security (cs.CR)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 558,
        "object_id": "paper:arxiv.2312.07550",
        "created_at": "2025-11-01T18:17:37+00:00",
        "updated_at": "2025-11-01T18:17:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.06508": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.06508",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T18:18:08.529Z",
            "data": {
              "session_id": "session_1762021088519_ihslf25",
              "source_id": "arxiv",
              "paper_id": "2404.06508",
              "start_time": "2025-11-01T18:17:39.303Z",
              "end_time": "2025-11-01T18:18:08.518Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T18:19:13.702Z",
            "data": {
              "session_id": "session_1762021153187_e4k0qk2",
              "source_id": "arxiv",
              "paper_id": "2404.06508",
              "start_time": "2025-11-01T18:18:39.073Z",
              "end_time": "2025-11-01T18:19:13.187Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T18:20:38.050Z",
            "data": {
              "session_id": "session_1762021238040_tuqp2s8",
              "source_id": "arxiv",
              "paper_id": "2404.06508",
              "start_time": "2025-11-01T18:20:22.440Z",
              "end_time": "2025-11-01T18:20:38.040Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "issue_number": 557,
        "object_id": "interactions:arxiv.2404.06508",
        "created_at": "2025-11-01T18:17:10+00:00",
        "updated_at": "2025-11-01T18:21:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.06508": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.06508",
        "url": "https://arxiv.org/abs/2404.06508",
        "title": "On the Effect of (Near) Duplicate Subwords in Language Modelling",
        "authors": "Anton Sch\u00e4fer, Thomas Hofmann, Imanol Schlag, Tiago Pimentel",
        "abstract": "Tokenisation is a core part of language models (LMs). It involves splitting a character sequence into subwords which are assigned arbitrary indices before being served to the LM. While typically lossless, however, this process may lead to less sample efficient LM training: as it removes character-level information, it could make it harder for LMs to generalise across similar subwords, such as now and Now. We refer to such subwords as near duplicates. In this paper, we study the impact of near duplicate subwords on LM training efficiency. First, we design an experiment that gives us an upper bound to how much we should expect a model to improve if we could perfectly generalise across near duplicates. We do this by duplicating each subword in our LM's vocabulary, creating perfectly equivalent classes of subwords. Experimentally, we find that LMs need roughly 17% more data when trained in a fully duplicated setting. Second, we investigate the impact of naturally occurring near duplicates on LMs. Here, we see that merging them considerably hurts LM performance. Therefore, although subword duplication negatively impacts LM training efficiency, naturally occurring near duplicates may not be as similar as anticipated, limiting the potential for performance improvements.",
        "timestamp": "2025-11-01T18:17:04.430Z",
        "rating": "novote",
        "publishedDate": "2024/04/09",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 556,
        "object_id": "paper:arxiv.2404.06508",
        "created_at": "2025-11-01T18:17:04+00:00",
        "updated_at": "2025-11-01T18:17:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2304.01373": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2304.01373",
        "url": "https://arxiv.org/abs/2304.01373",
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
        "authors": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",
        "abstract": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{this https URL}.",
        "timestamp": "2025-11-01T18:19:09.731Z",
        "rating": "novote",
        "publishedDate": "2023/04/03",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 560,
        "object_id": "paper:arxiv.2304.01373",
        "created_at": "2025-11-01T18:19:10+00:00",
        "updated_at": "2025-11-01T18:19:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2305.20086": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.20086",
        "url": "https://arxiv.org/abs/2305.20086",
        "title": "Understanding and Mitigating Copying in Diffusion Models",
        "authors": "Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, Tom Goldstein",
        "abstract": "Images generated by diffusion models like Stable Diffusion are increasingly widespread. Recent works and even lawsuits have shown that these models are prone to replicating their training data, unbeknownst to the user. In this paper, we first analyze this memorization problem in text-to-image diffusion models. While it is widely believed that duplicated images in the training set are responsible for content replication at inference time, we observe that the text conditioning of the model plays a similarly important role. In fact, we see in our experiments that data replication often does not happen for unconditional models, while it is common in the text-conditional case. Motivated by our findings, we then propose several techniques for reducing data replication at both training and inference time by randomizing and augmenting image captions in the training set.",
        "timestamp": "2025-11-01T18:18:52.128Z",
        "rating": "novote",
        "publishedDate": "2023/05/31",
        "tags": [
          "Machine Learning (cs.LG)",
          "Cryptography and Security (cs.CR)",
          "Computer Vision and Pattern Recognition (cs.CV)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 559,
        "object_id": "paper:arxiv.2305.20086",
        "created_at": "2025-11-01T18:18:52+00:00",
        "updated_at": "2025-11-01T18:19:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2101.00027": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2101.00027",
        "url": "https://arxiv.org/pdf/2101.00027",
        "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
        "authors": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy",
        "abstract": "Recent work has demonstrated that increased training dataset diversity\nimproves general cross-domain knowledge and downstream generalization\ncapability for large-scale language models. With this in mind, we present\n\\textit{the Pile}: an 825 GiB English text corpus targeted at training\nlarge-scale language models. The Pile is constructed from 22 diverse\nhigh-quality subsets -- both existing and newly constructed -- many of which\nderive from academic or professional sources. Our evaluation of the untuned\nperformance of GPT-2 and GPT-3 on the Pile shows that these models struggle on\nmany of its components, such as academic writing. Conversely, models trained on\nthe Pile improve significantly over both Raw CC and CC-100 on all components of\nthe Pile, while improving performance on downstream evaluations. Through an\nin-depth exploratory analysis, we document potentially concerning aspects of\nthe data for prospective users. We make publicly available the code used in its\nconstruction.",
        "timestamp": "2025-11-01T18:20:35.396Z",
        "rating": "novote",
        "publishedDate": "2020-12-31T19:00:10Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 561,
        "object_id": "paper:arxiv.2101.00027",
        "created_at": "2025-11-01T18:20:35+00:00",
        "updated_at": "2025-11-01T18:20:55+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2305.20086": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.20086",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T18:37:57.179Z",
            "data": {
              "session_id": "session_1762022276642_xww7xio",
              "source_id": "arxiv",
              "paper_id": "2305.20086",
              "start_time": "2025-11-01T18:28:41.669Z",
              "end_time": "2025-11-01T18:37:56.642Z",
              "heartbeat_count": 110,
              "duration_seconds": 550,
              "idle_seconds": 5,
              "total_elapsed_seconds": 555
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T19:45:16.057Z",
            "data": {
              "session_id": "session_1762026316008_ipscmg3",
              "source_id": "arxiv",
              "paper_id": "2305.20086",
              "start_time": "2025-11-01T19:45:06.952Z",
              "end_time": "2025-11-01T19:45:16.008Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 562,
        "object_id": "interactions:arxiv.2305.20086",
        "created_at": "2025-11-01T18:37:57+00:00",
        "updated_at": "2025-11-01T19:45:32+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.25035": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.25035",
        "url": "https://arxiv.org/abs/2509.25035",
        "title": "Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct",
        "authors": "Haoyang Zheng, Xinyang Liu, Cindy Xiangrui Kong, Nan Jiang, Zheyuan Hu, Weijian Luo, Wei Deng, Guang Lin",
        "abstract": "Fast and high-quality language generation is the holy grail that people pursue in the age of AI. In this work, we introduce Discrete Diffusion Divergence Instruct (DiDi-Instruct), a training-based method that initializes from a pre-trained (masked) discrete diffusion language model (dLLM) and distills a few-step student for fast generation. The resulting DiDi-Instruct model achieves comparable or superior performance to its dLLM teacher and the GPT-2 baseline while enabling up to 64\\times\\times acceleration. The theoretical foundation of DiDi-Instruct is a novel framework based on integral KL-divergence minimization, which yields a practical training algorithm. We further introduce grouped reward normalization, intermediate-state matching, and the reward-guided ancestral sampler that significantly improve training stability, model coverage, and inference quality. On OpenWebText, DiDi-Instruct achieves perplexity from 62.2 (8 NFEs) to 18.4 (128 NFEs), which outperforms prior accelerated dLLMs and GPT-2 baseline. These gains come with a negligible entropy loss (around 1\\%1\\%) and reduce additional training wall-clock time by more than 20\\times20\\times compared to competing dLLM distillation methods. We further validate the robustness and effectiveness of DiDi-Instruct through extensive ablation studies, model scaling, and the generation of discrete protein sequences. In conclusion, DiDi-Instruct is an efficient yet effective distillation method, enabling language generation in the blink of an eye. We will release both code and models at this http URL.",
        "timestamp": "2025-11-01T19:54:11.010Z",
        "rating": "novote",
        "publishedDate": "2025/09/29",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 563,
        "object_id": "paper:arxiv.2509.25035",
        "created_at": "2025-11-01T19:54:11+00:00",
        "updated_at": "2025-11-01T19:54:34+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.25035": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.25035",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-01T19:56:45.192Z",
            "data": {
              "session_id": "session_1762027004560_9gf5fi2",
              "source_id": "arxiv",
              "paper_id": "2509.25035",
              "start_time": "2025-11-01T19:56:10.050Z",
              "end_time": "2025-11-01T19:56:44.560Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 10,
              "total_elapsed_seconds": 35
            }
          }
        ]
      },
      "meta": {
        "issue_number": 564,
        "object_id": "interactions:arxiv.2509.25035",
        "created_at": "2025-11-01T19:56:45+00:00",
        "updated_at": "2025-11-01T19:57:08+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2002.12327": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2002.12327",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T08:42:20.837Z",
            "data": {
              "session_id": "session_1762072940267_6ilhw6s",
              "source_id": "arxiv",
              "paper_id": "2002.12327",
              "start_time": "2025-11-02T08:41:54.167Z",
              "end_time": "2025-11-02T08:42:20.267Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 1,
              "total_elapsed_seconds": 26
            }
          }
        ]
      },
      "meta": {
        "issue_number": 567,
        "object_id": "interactions:arxiv.2002.12327",
        "created_at": "2025-11-02T09:42:21+00:00",
        "updated_at": "2025-11-02T09:42:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2002.12327": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2002.12327",
        "url": "https://arxiv.org/abs/2002.12327",
        "title": "A Primer in BERTology: What we know about how BERT works",
        "authors": "Anna Rogers, Olga Kovaleva, Anna Rumshisky",
        "abstract": "Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.",
        "timestamp": "2025-11-02T08:41:53.955Z",
        "rating": "novote",
        "publishedDate": "2020/02/27",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 565,
        "object_id": "paper:arxiv.2002.12327",
        "created_at": "2025-11-02T09:41:54+00:00",
        "updated_at": "2025-11-02T09:42:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2401.16092": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.16092",
        "url": "https://arxiv.org/abs/2401.16092",
        "title": "Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You",
        "authors": "Felix Friedrich, Katharina H\u00e4mmerl, Patrick Schramowski, Manuel Brack, Jindrich Libovicky, Kristian Kersting, Alexander Fraser",
        "abstract": "Text-to-image generation models have recently achieved astonishing results in image quality, flexibility, and text alignment, and are consequently employed in a fast-growing number of applications. Through improvements in multilingual abilities, a larger community now has access to this technology. However, our results show that multilingual models suffer from significant gender biases just as monolingual models do. Furthermore, the natural expectation that multilingual models will provide similar results across languages does not hold up. Instead, there are important differences between languages. We propose a novel benchmark, MAGBIG, intended to foster research on gender bias in multilingual models. We use MAGBIG to investigate the effect of multilingualism on gender bias in T2I models. To this end, we construct multilingual prompts requesting portraits of people with a certain occupation or trait. Our results show that not only do models exhibit strong gender biases but they also behave differently across languages. Furthermore, we investigate prompt engineering strategies, such as indirect, neutral formulations, to mitigate these biases. Unfortunately, these approaches have limited success and result in worse text-to-image alignment. Consequently, we call for more research into diverse representations across languages in image generators, as well as into steerability to address biased model behavior.",
        "timestamp": "2025-11-02T13:08:59.090Z",
        "rating": "novote",
        "publishedDate": "2024/01/29",
        "tags": [
          "Computation and Language (cs.CL)",
          "Computers and Society (cs.CY)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 568,
        "object_id": "paper:arxiv.2401.16092",
        "created_at": "2025-11-02T13:08:59+00:00",
        "updated_at": "2025-11-02T13:09:22+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.02707": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.02707",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T14:15:08.036Z",
            "data": {
              "session_id": "session_1762092907414_1nbnxox",
              "source_id": "arxiv",
              "paper_id": "2410.02707",
              "start_time": "2025-11-02T14:14:50.844Z",
              "end_time": "2025-11-02T14:15:07.414Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "issue_number": 572,
        "object_id": "interactions:arxiv.2410.02707",
        "created_at": "2025-11-02T14:15:08+00:00",
        "updated_at": "2025-11-02T14:15:32+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.02707": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.02707",
        "url": "https://arxiv.org/pdf/2410.02707",
        "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations",
        "authors": "Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov",
        "abstract": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.",
        "timestamp": "2025-11-02T14:14:50.421Z",
        "rating": "novote",
        "publishedDate": "2024-10-03T17:31:31Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "68T50",
          "I.2.7"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 570,
        "object_id": "paper:arxiv.2410.02707",
        "created_at": "2025-11-02T14:14:50+00:00",
        "updated_at": "2025-11-02T14:15:13+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2401.16092": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.16092",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T14:23:17.103Z",
            "data": {
              "session_id": "session_1762093397097_35x2hzq",
              "source_id": "arxiv",
              "paper_id": "2401.16092",
              "start_time": "2025-11-02T14:23:08.793Z",
              "end_time": "2025-11-02T14:23:17.097Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 573,
        "object_id": "interactions:arxiv.2401.16092",
        "created_at": "2025-11-02T14:23:17+00:00",
        "updated_at": "2025-11-02T14:23:42+00:00",
        "version": 1
      }
    },
    "paper:openreview.G9uucwJUSi": {
      "data": {
        "sourceId": "openreview",
        "paperId": "G9uucwJUSi",
        "url": "https://openreview.net/forum?id=G9uucwJUSi&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models",
        "authors": "Shir Ashury-Tahan, Yifan Mai, Elron Bandel, Michal Shmueli-Scheuer, Leshem Choshen",
        "abstract": "Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset, for instance, may not reflect weak reasoning at all, but instead a formatting slip, a calculation error, or dataset noise. Without disentangling such causes, benchmarks give an incomplete picture and cannot reliably guide model improvement. We introduce ErrorMAp, the first method to systematically chart the sources of LLM failure. ErrorMAp provides tools to extract a model's unique ``failure signature'', uncover what benchmarks actually measure in practice, and broaden the scope of identified model errors to reduce blind spots. This enables developers to debug models more effectively and helps benchmark creators align dataset goals with actual outcomes. Additionally, it supports benchmark consumers in identifying which models best suit their specific needs. ErrorMAp is designed to work flexibly with any model and dataset, making it adaptable to evolving architectures and emerging data sources without requiring changes to its logic. We apply our method across 21 datasets and 73 models to automatically generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns in current language models. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMAp and ErrorAtlas lay the foundation for next-generation evaluation --- one that exposes hidden weaknesses and directs meaningful progress. Unlike success, which is typically measured using task- or dataset-level metrics, our approach introduces a deeper layer of evaluation that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and method code publicly available, with plans to update ErrorAtlas as new benchmarks emerge.",
        "timestamp": "2025-11-02T17:12:31.884Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Error Analysis",
          "Taxonomy"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 581,
        "object_id": "paper:openreview.G9uucwJUSi",
        "created_at": "2025-11-02T17:12:32+00:00",
        "updated_at": "2025-11-02T17:12:54+00:00",
        "version": 1
      }
    },
    "interactions:openreview.QsBhmlagrv": {
      "data": {
        "sourceId": "openreview",
        "paperId": "QsBhmlagrv",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T17:12:21.874Z",
            "data": {
              "session_id": "session_1762103541312_j40wccz",
              "source_id": "openreview",
              "paper_id": "QsBhmlagrv",
              "start_time": "2025-11-02T17:12:09.692Z",
              "end_time": "2025-11-02T17:12:21.312Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "issue_number": 580,
        "object_id": "interactions:openreview.QsBhmlagrv",
        "created_at": "2025-11-02T17:12:22+00:00",
        "updated_at": "2025-11-02T17:12:46+00:00",
        "version": 1
      }
    },
    "paper:openreview.QsBhmlagrv": {
      "data": {
        "sourceId": "openreview",
        "paperId": "QsBhmlagrv",
        "url": "https://openreview.net/forum?id=QsBhmlagrv&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "The Mighty ToRR: A Benchmark for Table Reasoning and Robustness in LLMs",
        "authors": "Shir Ashury-Tahan, Yifan Mai, Rajmohan C, Ariel Gera, Yotam Perlitz, Asaf Yehudai, Elron Bandel, Leshem Choshen, Eyal Shnarch, Percy Liang, Michal Shmueli-Scheuer",
        "abstract": "Despite its real-world significance, model performance on tabular data remains underexplored, leaving uncertainty about which model to rely on and which prompt configuration to adopt. To address this gap, we create ToRR, a benchmark for Table Reasoning and Robustness, measuring model performance and robustness on table-related tasks. The benchmark includes $10$ datasets that cover different types of table reasoning capabilities across varied domains. ToRR goes beyond model performance rankings, and is designed to reflect whether models can handle tabular data consistently and robustly, across a variety of common table representation formats. We present a leaderboard as well as comprehensive analyses of the results of leading models over ToRR. Our results reveal a striking pattern of brittle model behavior, where even strong models are unable to perform robustly on tabular data tasks. We further find that no single table format consistently yields superior performance. However, evaluating models across multiple formats is essential for a reliable assessment of their capabilities. Moreover, we show that the reliability boost from testing multiple prompts can be equivalent to adding more test examples. Overall, our findings show that reasoning over table tasks remains a significant challenge. The leaderboard, data and code are publicly available.",
        "timestamp": "2025-11-02T17:12:09.946Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Structured Data",
          "Benchmark",
          "Evaluation"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 579,
        "object_id": "paper:openreview.QsBhmlagrv",
        "created_at": "2025-11-02T17:12:10+00:00",
        "updated_at": "2025-11-02T17:12:35+00:00",
        "version": 1
      }
    },
    "paper:openreview.94awfhTHjh": {
      "data": {
        "sourceId": "openreview",
        "paperId": "94awfhTHjh",
        "url": "https://openreview.net/forum?id=94awfhTHjh&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Can LLMs Help Encoder Models Maintain Both High Accuracy and Consistency in Temporal Relation Classification?",
        "authors": "Adiel Meir, Kfir Bar",
        "abstract": "Temporal relation classification (TRC) demands both accuracy and temporal consistency in event timeline extraction. Encoder-based models achieve high accuracy but introduce inconsistencies because they rely on pairwise classification, while LLMs leverage global context to generate temporal graphs, improving consistency at the cost of accuracy.\nWe assess LLM prompting strategies for TRC and their effectiveness in assisting encoder models with cycle resolution. Results show that while LLMs improve consistency, they struggle with accuracy and do not outperform a simple confidence-based cycle resolution approach.",
        "timestamp": "2025-11-02T17:11:58.540Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "NLP",
          "zero/few-shot extraction",
          "TRC"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 578,
        "object_id": "paper:openreview.94awfhTHjh",
        "created_at": "2025-11-02T17:11:58+00:00",
        "updated_at": "2025-11-02T17:12:21+00:00",
        "version": 1
      }
    },
    "interactions:openreview.dxR1hzsfUg": {
      "data": {
        "sourceId": "openreview",
        "paperId": "dxR1hzsfUg",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T17:11:43.926Z",
            "data": {
              "session_id": "session_1762103503436_2m9u4z6",
              "source_id": "openreview",
              "paper_id": "dxR1hzsfUg",
              "start_time": "2025-11-02T17:11:37.152Z",
              "end_time": "2025-11-02T17:11:43.436Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 577,
        "object_id": "interactions:openreview.dxR1hzsfUg",
        "created_at": "2025-11-02T17:11:44+00:00",
        "updated_at": "2025-11-02T17:12:13+00:00",
        "version": 1
      }
    },
    "paper:openreview.dxR1hzsfUg": {
      "data": {
        "sourceId": "openreview",
        "paperId": "dxR1hzsfUg",
        "url": "https://openreview.net/forum?id=dxR1hzsfUg&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Inferring Functionality of Attention Heads from their Parameters",
        "authors": "Amit Elhelo, Mor Geva",
        "abstract": "Attention heads are one of the building blocks of large language models (LLMs). Prior work on investigating their operation mostly focused on analyzing their behavior during inference for specific circuits or tasks. In this work, we seek a comprehensive mapping of the operations they implement in a model. We propose MAPS (Mapping Attention head ParameterS), an efficient framework that infers the functionality of attention heads from their parameters, without any model training or inference. We showcase the utility of MAPS for answering two types of questions: (a) given a predefined operation, mapping how strongly heads across the model implement it, and (b) given an attention head, inferring its salient functionality. Evaluating MAPS on 20 operations across 6 popular LLMs shows its estimations correlate with the head's outputs during inference and are causally linked to the model's predictions. Moreover, its mappings reveal attention heads of certain operations that were overlooked in previous studies, and valuable insights on function universality and architecture biases in LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS to characterize the salient operations of a given head. Our pipeline produces plausible operation descriptions for most heads, as assessed by human judgment, while revealing diverse operations. ACL 2025 Main.",
        "timestamp": "2025-11-02T17:11:37.398Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "feature attribution",
          "free-text language explanations",
          "natural language explanations",
          "knowledge tracing",
          "knowledge discovering",
          "language resources"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 576,
        "object_id": "paper:openreview.dxR1hzsfUg",
        "created_at": "2025-11-02T17:11:37+00:00",
        "updated_at": "2025-11-02T17:11:57+00:00",
        "version": 1
      }
    },
    "interactions:openreview.5L5OGV9yTz": {
      "data": {
        "sourceId": "openreview",
        "paperId": "5L5OGV9yTz",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T17:11:35.468Z",
            "data": {
              "session_id": "session_1762103494974_2upx41p",
              "source_id": "openreview",
              "paper_id": "5L5OGV9yTz",
              "start_time": "2025-11-02T17:11:26.517Z",
              "end_time": "2025-11-02T17:11:34.974Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 575,
        "object_id": "interactions:openreview.5L5OGV9yTz",
        "created_at": "2025-11-02T17:11:36+00:00",
        "updated_at": "2025-11-02T17:11:58+00:00",
        "version": 1
      }
    },
    "paper:openreview.5L5OGV9yTz": {
      "data": {
        "sourceId": "openreview",
        "paperId": "5L5OGV9yTz",
        "url": "https://openreview.net/forum?id=5L5OGV9yTz&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "CRISP: Persistent Concept Unlearning via Sparse Autoencoders",
        "authors": "Tomer Ashuach, Dana Arad, Aaron Mueller, Martin Tutek, Yonatan Belinkov",
        "abstract": "As large language models (LLMs) are increasingly deployed in real-world applications, the need to selectively remove unwanted knowledge while preserving model utility has become paramount. Recent work has explored sparse autoencoders (SAEs) to perform precise interventions on monosemantic features. However, most SAE-based methods operate at inference time, failing to create persistent changes in the model's parameters. Such interventions can be bypassed or reversed by malicious actors with parameter access. We introduce CRISP, a parameter-efficient method for persistent concept unlearning using SAEs. CRISP automatically identifies salient SAE features across multiple layers and suppresses their activations. We experiment with two LLMs and show that our method outperforms prior approaches on safety-critical unlearning tasks from the WMDP benchmark, successfully removing harmful knowledge while preserving general and in-domain capabilities. Feature-level analysis reveals that CRISP achieves semantically coherent separation between target and benign concepts, allowing precise suppression of the target features.",
        "timestamp": "2025-11-02T17:11:26.815Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Language Models",
          "Concept Unlearning",
          "Model Editing",
          "Sparse Autoencoders",
          "Unlearning",
          "Knowledge Removal",
          "Safety in NLP",
          "Interpretability"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 574,
        "object_id": "paper:openreview.5L5OGV9yTz",
        "created_at": "2025-11-02T17:11:27+00:00",
        "updated_at": "2025-11-02T17:11:49+00:00",
        "version": 1
      }
    },
    "interactions:openreview.G9uucwJUSi": {
      "data": {
        "sourceId": "openreview",
        "paperId": "G9uucwJUSi",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-02T17:13:16.045Z",
            "data": {
              "session_id": "session_1762103595444_82oyc3i",
              "source_id": "openreview",
              "paper_id": "G9uucwJUSi",
              "start_time": "2025-11-02T17:12:31.642Z",
              "end_time": "2025-11-02T17:13:15.444Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 4,
              "total_elapsed_seconds": 44
            }
          }
        ]
      },
      "meta": {
        "issue_number": 582,
        "object_id": "interactions:openreview.G9uucwJUSi",
        "created_at": "2025-11-02T17:13:16+00:00",
        "updated_at": "2025-11-02T17:13:39+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2205.11758": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2205.11758",
        "interactions": []
      },
      "meta": {
        "issue_number": 586,
        "object_id": "interactions:arxiv.2205.11758",
        "created_at": "2025-11-03T04:48:35+00:00",
        "updated_at": "2025-11-03T04:48:36+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2205.11758": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2205.11758",
        "url": "https://arxiv.org/abs/2205.11758",
        "title": "Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models",
        "authors": "Terra Blevins, Hila Gonen, Luke Zettlemoyer",
        "abstract": "The emergent cross-lingual transfer seen in multilingual pretrained models has sparked significant interest in studying their behavior. However, because these analyses have focused on fully trained multilingual models, little is known about the dynamics of the multilingual pretraining process. We investigate when these models acquire their in-language and cross-lingual abilities by probing checkpoints taken from throughout XLM-R pretraining, using a suite of linguistic tasks. Our analysis shows that the model achieves high in-language performance early on, with lower-level linguistic skills acquired before more complex ones. In contrast, the point in pretraining when the model learns to transfer cross-lingually differs across language pairs. Interestingly, we also observe that, across many languages and tasks, the final model layer exhibits significant performance degradation over time, while linguistic knowledge propagates to lower layers of the network. Taken together, these insights highlight the complexity of multilingual pretraining and the resulting varied behavior for different languages over time.",
        "timestamp": "2025-11-03T04:48:26.743Z",
        "rating": "novote",
        "publishedDate": "2022/05/24",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 585,
        "object_id": "paper:arxiv.2205.11758",
        "created_at": "2025-11-03T04:48:27+00:00",
        "updated_at": "2025-11-03T04:48:49+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.02768": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.02768",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T04:48:01.132Z",
            "data": {
              "session_id": "session_1762145280644_wlcyxhe",
              "source_id": "arxiv",
              "paper_id": "2504.02768",
              "start_time": "2025-11-03T04:47:52.051Z",
              "end_time": "2025-11-03T04:48:00.644Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 584,
        "object_id": "interactions:arxiv.2504.02768",
        "created_at": "2025-11-03T04:48:01+00:00",
        "updated_at": "2025-11-03T04:48:22+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.02768": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.02768",
        "url": "https://arxiv.org/abs/2504.02768",
        "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs",
        "authors": "Jaap Jumelet, Leonie Weissweiler, Joakim Nivre, Arianna Bisazza",
        "abstract": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic minimal pairs, covering 101 languages and 2 types of subject-verb agreement, containing more than 128,000 minimal pairs. Our minimal pairs are created using a fully automated pipeline, leveraging the large-scale linguistic resources of Universal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs at an unprecedented multilingual scale, and highlights the shortcomings of the current state-of-the-art in modelling low-resource languages.",
        "timestamp": "2025-11-03T04:47:52.371Z",
        "rating": "novote",
        "publishedDate": "2025/04/03",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 583,
        "object_id": "paper:arxiv.2504.02768",
        "created_at": "2025-11-03T04:47:52+00:00",
        "updated_at": "2025-11-03T04:48:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.13229": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.13229",
        "url": "https://arxiv.org/abs/2406.13229",
        "title": "Probing the Emergence of Cross-lingual Alignment during LLM Training",
        "authors": "Hetong Wang, Pasquale Minervini, Edoardo M. Ponti",
        "abstract": "Multilingual Large Language Models (LLMs) achieve remarkable levels of zero-shot cross-lingual transfer performance. We speculate that this is predicated on their ability to align languages without explicit supervision from parallel sentences. While representations of translationally equivalent sentences in different languages are known to be similar after convergence, however, it remains unclear how such cross-lingual alignment emerges during pre-training of LLMs. Our study leverages intrinsic probing techniques, which identify which subsets of neurons encode linguistic features, to correlate the degree of cross-lingual neuron overlap with the zero-shot cross-lingual transfer performance for a given model. In particular, we rely on checkpoints of BLOOM, a multilingual autoregressive LLM, across different training steps and model scales. We observe a high correlation between neuron overlap and downstream performance, which supports our hypothesis on the conditions leading to effective cross-lingual transfer. Interestingly, we also detect a degradation of both implicit alignment and multilingual abilities in certain phases of the pre-training process, providing new insights into the multilingual pretraining dynamics.",
        "timestamp": "2025-11-03T04:49:19.221Z",
        "rating": "novote",
        "publishedDate": "2024/06/19",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 587,
        "object_id": "paper:arxiv.2406.13229",
        "created_at": "2025-11-03T04:49:19+00:00",
        "updated_at": "2025-11-03T04:49:42+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2403.06265": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.06265",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T05:14:49.307Z",
            "data": {
              "session_id": "session_1762146889281_k2londp",
              "source_id": "arxiv",
              "paper_id": "2403.06265",
              "start_time": "2025-11-03T05:14:43.598Z",
              "end_time": "2025-11-03T05:14:49.281Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T05:16:00.753Z",
            "data": {
              "session_id": "session_1762146960734_qheqars",
              "source_id": "arxiv",
              "paper_id": "2403.06265",
              "start_time": "2025-11-03T05:15:51.021Z",
              "end_time": "2025-11-03T05:16:00.734Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "issue_number": 590,
        "object_id": "interactions:arxiv.2403.06265",
        "created_at": "2025-11-03T04:51:09+00:00",
        "updated_at": "2025-11-03T05:16:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.06265": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.06265",
        "url": "https://arxiv.org/abs/2403.06265",
        "title": "Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance",
        "authors": "Omer Goldman, Avi Caciularu, Matan Eyal, Kris Cao, Idan Szpektor, Reut Tsarfaty",
        "abstract": "Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens. We also demonstrate the empirical importance of compression for downstream success of pre-trained language models. We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and fine-tune them over several tasks. We show that there is a correlation between tokenizers' compression and models' downstream performance, suggesting that compression is a reliable intrinsic indicator of tokenization quality. These correlations are more pronounced for generation tasks (over classification) or for smaller models (over large ones). We replicated a representative part of our experiments on Turkish and found similar results, confirming that our results hold for languages with typological characteristics dissimilar to English. We conclude that building better compressing tokenizers is a fruitful avenue for further research and for improving overall model performance.",
        "timestamp": "2025-11-03T04:51:03.500Z",
        "rating": "novote",
        "publishedDate": "2024/03/10",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 589,
        "object_id": "paper:arxiv.2403.06265",
        "created_at": "2025-11-03T04:51:03+00:00",
        "updated_at": "2025-11-03T04:51:26+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.13229": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.13229",
        "interactions": []
      },
      "meta": {
        "issue_number": 588,
        "object_id": "interactions:arxiv.2406.13229",
        "created_at": "2025-11-03T04:50:55+00:00",
        "updated_at": "2025-11-03T04:50:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.10441": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.10441",
        "url": "https://arxiv.org/abs/2408.10441",
        "title": "Goldfish: Monolingual Language Models for 350 Languages",
        "authors": "Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin K. Bergen",
        "abstract": "For many low-resource languages, the only available language models are large multilingual models trained on many languages simultaneously. However, using FLORES perplexity as a metric, we find that these models perform worse than bigrams for many languages (e.g. 24% of languages in XGLM 4.5B; 43% in BLOOM 7.1B). To facilitate research that focuses on low-resource languages, we pre-train and release Goldfish, a suite of monolingual autoregressive Transformer language models up to 125M parameters for 350 languages. The Goldfish reach lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98 of 204 FLORES languages, despite each Goldfish model being over 10x smaller. However, the Goldfish significantly underperform larger multilingual models on reasoning benchmarks, suggesting that for low-resource languages, multilinguality primarily improves general reasoning abilities rather than basic text generation. We release models trained on 5MB (350 languages), 10MB (288 languages), 100MB (166 languages), and 1GB (83 languages) of text data where available. The Goldfish models are available as baselines, fine-tuning sources, or augmentations to existing models in low-resource NLP research, and they are further useful for crosslinguistic studies requiring maximally comparable models across languages.",
        "timestamp": "2025-11-03T05:13:40.040Z",
        "rating": "novote",
        "publishedDate": "2024/08/19",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 591,
        "object_id": "paper:arxiv.2408.10441",
        "created_at": "2025-11-03T05:13:40+00:00",
        "updated_at": "2025-11-03T05:14:04+00:00",
        "version": 1
      }
    },
    "paper:openreview.EKvqw9k3lC": {
      "data": {
        "sourceId": "openreview",
        "paperId": "EKvqw9k3lC",
        "url": "https://openreview.net/forum?id=EKvqw9k3lC",
        "title": "Backtracking Mathematical Reasoning of Language Models to the Pretraining Data",
        "authors": "Yasaman Razeghi, Hamish Ivison, Sameer Singh, Yanai Elazar",
        "abstract": "In-context learning and chain-of-thought prompting have demonstrated surprising performance improvements on mathematical reasoning benchmarks.\nTherefore, understanding the underlying factors enabling these capabilities is crucial.\nHowever, the specific aspects of pretraining data that equip models with mathematical reasoning capabilities remain largely unexplored and are less studied systematically.\nIn this study, we identify subsets of model pretraining data that contribute to math reasoning ability of the model, and evaluate it on several mathematical operations (e.g. addition, multiplication) and tasks (e.g. the asdiv dataset).\nWe measure the importance of such subsets by continual training of the model on pretraining data subsets, and then we quantify the change in performance on the mathematical benchmark to assess their importance. \nIf a subset results in an improved performance, we conjecture that such subset contributes to a model's overall mathematical ability.\nOur results unveil that while training on math-only data contributes to simple arithmetic abilities, it does not solely explain performance on more complex reasoning abilities like chain-of-thought reasoning. \nWe also find that code data contributes to chain-of-thought reasoning while reducing the arithmetic performance.",
        "timestamp": "2025-11-03T09:44:08.775Z",
        "rating": "novote",
        "publishedDate": "27 Oct 2023",
        "tags": [
          "pretraining data; math reasoning; data attribution"
        ],
        "doi": "",
        "journalName": "ATTRIB Poster",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 592,
        "object_id": "paper:openreview.EKvqw9k3lC",
        "created_at": "2025-11-03T09:44:09+00:00",
        "updated_at": "2025-11-03T09:44:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.00838": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.00838",
        "url": "https://arxiv.org/abs/2402.00838",
        "title": "OLMo: Accelerating the Science of Language Models",
        "authors": "Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi",
        "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.",
        "timestamp": "2025-11-03T09:44:49.585Z",
        "rating": "novote",
        "publishedDate": "2024/02/01",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 594,
        "object_id": "paper:arxiv.2402.00838",
        "created_at": "2025-11-03T09:44:49+00:00",
        "updated_at": "2025-11-03T09:45:14+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.00159": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.00159",
        "url": "https://arxiv.org/abs/2402.00159",
        "title": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research",
        "authors": "Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo",
        "abstract": "Information about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.",
        "timestamp": "2025-11-03T09:44:48.722Z",
        "rating": "novote",
        "publishedDate": "2024/01/31",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 593,
        "object_id": "paper:arxiv.2402.00159",
        "created_at": "2025-11-03T09:44:49+00:00",
        "updated_at": "2025-11-03T09:45:09+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.15511": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.15511",
        "url": "https://www.arxiv.org/pdf/2510.15511",
        "title": "Language Models are Injective and Hence Invertible",
        "authors": "Giorgos Nikolaou, Tommaso Mencattini, Donato Crisostomi, Andrea Santilli, Yannis Panagakis, Emanuele Rodol\u00e0",
        "abstract": "Transformer components such as non-linear activations and normalization are\ninherently non-injective, suggesting that different inputs could map to the\nsame output and prevent exact recovery of the input from a model's\nrepresentations. In this paper, we challenge this view. First, we prove\nmathematically that transformer language models mapping discrete input\nsequences to their corresponding sequence of continuous representations are\ninjective and therefore lossless, a property established at initialization and\npreserved during training. Second, we confirm this result empirically through\nbillions of collision tests on six state-of-the-art language models, and\nobserve no collisions. Third, we operationalize injectivity: we introduce\nSipIt, the first algorithm that provably and efficiently reconstructs the exact\ninput text from hidden activations, establishing linear-time guarantees and\ndemonstrating exact invertibility in practice. Overall, our work establishes\ninjectivity as a fundamental and exploitable property of language models, with\ndirect implications for transparency, interpretability, and safe deployment.",
        "timestamp": "2025-11-03T10:00:25.984Z",
        "rating": "novote",
        "publishedDate": "2025-10-17T10:25:30Z",
        "tags": [
          "cs.LG",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 595,
        "object_id": "paper:arxiv.2510.15511",
        "created_at": "2025-11-03T10:00:26+00:00",
        "updated_at": "2025-11-03T10:00:46+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.15511": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.15511",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T10:01:52.852Z",
            "data": {
              "session_id": "session_1762164112136_hejonkb",
              "source_id": "arxiv",
              "paper_id": "2510.15511",
              "start_time": "2025-11-03T10:00:25.101Z",
              "end_time": "2025-11-03T10:01:52.136Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 2,
              "total_elapsed_seconds": 87
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T10:53:58.117Z",
            "data": {
              "session_id": "session_1762167237361_5efw98j",
              "source_id": "arxiv",
              "paper_id": "2510.15511",
              "start_time": "2025-11-03T10:01:52.815Z",
              "end_time": "2025-11-03T10:53:57.361Z",
              "heartbeat_count": 624,
              "duration_seconds": 3120,
              "idle_seconds": 5,
              "total_elapsed_seconds": 3125
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T11:05:45.079Z",
            "data": {
              "session_id": "session_1762167945073_j6y52so",
              "source_id": "arxiv",
              "paper_id": "2510.15511",
              "start_time": "2025-11-03T11:05:36.198Z",
              "end_time": "2025-11-03T11:05:45.073Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T14:52:54.945Z",
            "data": {
              "session_id": "session_1762267974234_h8iqqvy",
              "source_id": "arxiv",
              "paper_id": "2510.15511",
              "start_time": "2025-11-04T14:51:46.538Z",
              "end_time": "2025-11-04T14:52:54.234Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 3,
              "total_elapsed_seconds": 68
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T14:55:49.327Z",
            "data": {
              "session_id": "session_1762268148670_pguf3i3",
              "source_id": "arxiv",
              "paper_id": "2510.15511",
              "start_time": "2025-11-04T14:55:08.417Z",
              "end_time": "2025-11-04T14:55:48.670Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 0,
              "total_elapsed_seconds": 40
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T15:03:44.735Z",
            "data": {
              "session_id": "session_1762268624718_3ugsikv",
              "source_id": "arxiv",
              "paper_id": "2510.15511",
              "start_time": "2025-11-04T15:03:13.908Z",
              "end_time": "2025-11-04T15:03:44.718Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 1,
              "total_elapsed_seconds": 31
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T15:08:09.443Z",
            "data": {
              "session_id": "session_1762268888999_y0wrd4k",
              "source_id": "arxiv",
              "paper_id": "2510.15511",
              "start_time": "2025-11-04T15:07:34.069Z",
              "end_time": "2025-11-04T15:08:08.999Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 5,
              "total_elapsed_seconds": 35
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T15:09:44.292Z",
            "data": {
              "session_id": "session_1762268983882_4lfox2a",
              "source_id": "arxiv",
              "paper_id": "2510.15511",
              "start_time": "2025-11-04T15:09:32.381Z",
              "end_time": "2025-11-04T15:09:43.882Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T15:28:20.801Z",
            "data": {
              "session_id": "session_1762270100190_y0izlm8",
              "source_id": "arxiv",
              "paper_id": "2510.15511",
              "start_time": "2025-11-04T15:26:27.273Z",
              "end_time": "2025-11-04T15:28:20.190Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 3,
              "total_elapsed_seconds": 113
            }
          }
        ]
      },
      "meta": {
        "issue_number": 596,
        "object_id": "interactions:arxiv.2510.15511",
        "created_at": "2025-11-03T10:01:53+00:00",
        "updated_at": "2025-11-04T15:28:49+00:00",
        "version": 1
      }
    },
    "paper:openreview.rpegsGgoQb": {
      "data": {
        "sourceId": "openreview",
        "paperId": "rpegsGgoQb",
        "url": "https://openreview.net/forum?id=rpegsGgoQb&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "TwoHillsLab: A Scalable Platform for  Quantitative Analysis of Biblical Hebrew",
        "authors": "Guy Shaked",
        "abstract": "TwoHillsLab: A Scalable Platform for \nQuantitative Analysis of Biblical Hebrew\nGuy Shaked, Bar-Ilan University\n1. Motivation\nLarge, digitized textual collections of the Bible invite methods from computational linguistics to revisit long-standing questions in biblical studies and historical philology. TwoHillsLab is a free, web-based platform that operationalizes such questions at scale, enabling reproducible, data-driven analysis across the Hebrew Bible and related corpora. The platform\u2019s first analytical release centers on a simple, interpretable signal\u2014average verse length (AVL) computed at chapter resolution. Despite its simplicity, AVL yields robust, cross-book patterns that align with (and at times challenge) traditional expectations.\n2. Key Findings\nGenre dominates authorship/period. Quantitatively, composition type (genre) exerts a stronger statistical signal than author/editor or historical period, suggesting stylistic regimes are better explained by genre than by source or time alone \nProse vs. poetry separation. Prose exhibits longer verses on average, while poetry trends shorter, yielding a clear, empirical distinction between the two at scale \nTwoHillsLabLecture19thCongress\nPoetry subtypes differentiate. The data distinguish EMET-style poetry (with its distinctive Masoretic accentuation) from other lyric poetry, and both from elegiac/laments \n3. Limitations\nNot all passages or books align cleanly with global patterns; such deviations are expected\u2014and indeed valuable\u2014as prompts for further inquiry. Average verse length is only one facet; a fuller account requires additional features and multi-parameter analyses.\n5. Ongoing & Future Work\nRicher feature sets: Moving beyond a single metric (e.g., AVL) to multi-feature, multi-parameter.\nGenAI/LLM integration: Leveraging modern generative models for large-scale text anlysis.\n6. Conclusion\nTwoHillsLab demonstrates that even minimalist, interpretable statistics can recover meaningful literary structure at scale in the Hebrew Bible. The platform positions computational linguistics not as a replacement for close reading, but as a complementary lens.",
        "timestamp": "2025-11-03T11:33:17.962Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "TwoHillsLab; Biblical Hebrew; computational linguistics; digital humanities; stylometry; quantitative philology; LLM/GenAI for text analysis"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 597,
        "object_id": "paper:openreview.rpegsGgoQb",
        "created_at": "2025-11-03T11:33:18+00:00",
        "updated_at": "2025-11-03T11:33:43+00:00",
        "version": 1
      }
    },
    "paper:openreview.ujiCRwuqYg": {
      "data": {
        "sourceId": "openreview",
        "paperId": "ujiCRwuqYg",
        "url": "https://openreview.net/forum?id=ujiCRwuqYg&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "CToT: Causal Tree of Thoughts for Inference-Time Compute",
        "authors": "Zachary Elisha Bamberger, Till Raphael Saenger, Ofra Amir, Brandon M. Stewart, Amir Feder",
        "abstract": "Building a persuasive argument involves constructing a sequence of claims that balance logical coherence, credibility, and emotional resonance. However, the space of possible arguments is both difficult to define and challenging to explore systematically. We present $\\textbf{Causal Tree of Thoughts (CToT)}$, an inference-time compute method that augments beam search with targeted interventions along dimensions of content, structure, and style. CToT is designed to support principled exploration of diverse reasoning trajectories while enabling causal analysis of how specific interventions shape outcomes. We evaluate CTOT along three axes: performance on measurable tasks (instruction following and argument generation), diversity induced by intervention-based branching (relative to temperature-based sampling), and causal attribution of outcome differences to specific interventions. Our approach instantiates patterns from the argumentation literature as explicit intervention templates and demonstrates their effectiveness when enforced through controlled generation. While we focus on persuasive argumentation as our primary application, we validate the generalizability of our framework across natural language generation tasks.",
        "timestamp": "2025-11-03T12:09:31.545Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "reasoning",
          "argumentation",
          "inference time compute",
          "tree-search"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 598,
        "object_id": "paper:openreview.ujiCRwuqYg",
        "created_at": "2025-11-03T12:09:31+00:00",
        "updated_at": "2025-11-03T12:09:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2206.09755": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2206.09755",
        "url": "https://arxiv.org/abs/2206.09755",
        "title": "Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research Manifold",
        "authors": "Sebastian Ruder, Ivan Vuli\u0107, Anders S\u00f8gaard",
        "abstract": "The prototypical NLP experiment trains a standard architecture on labeled English data and optimizes for accuracy, without accounting for other dimensions such as fairness, interpretability, or computational efficiency. We show through a manual classification of recent NLP research papers that this is indeed the case and refer to it as the square one experimental setup. We observe that NLP research often goes beyond the square one setup, e.g, focusing not only on accuracy, but also on fairness or interpretability, but typically only along a single dimension. Most work targeting multilinguality, for example, considers only accuracy; most work on fairness or interpretability considers only English; and so on. We show this through manual classification of recent NLP research papers and ACL Test-of-Time award recipients. Such one-dimensionality of most research means we are only exploring a fraction of the NLP research search space. We provide historical and recent examples of how the square one bias has led researchers to draw false conclusions or make unwise choices, point to promising yet unexplored directions on the research manifold, and make practical recommendations to enable more multi-dimensional research. We open-source the results of our annotations to enable further analysis at this https URL",
        "timestamp": "2025-11-03T14:51:27.730Z",
        "rating": "novote",
        "publishedDate": "2022/06/20",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 601,
        "object_id": "paper:arxiv.2206.09755",
        "created_at": "2025-11-03T14:51:28+00:00",
        "updated_at": "2025-11-03T14:51:54+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.16677": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.16677",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T14:51:17.093Z",
            "data": {
              "session_id": "session_1762181477082_k3x54np",
              "source_id": "arxiv",
              "paper_id": "2504.16677",
              "start_time": "2025-11-03T14:51:11.183Z",
              "end_time": "2025-11-03T14:51:17.082Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T15:11:37.334Z",
            "data": {
              "session_id": "session_1762182696496_e7rh830",
              "source_id": "arxiv",
              "paper_id": "2504.16677",
              "start_time": "2025-11-03T15:10:27.414Z",
              "end_time": "2025-11-03T15:11:36.496Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 4,
              "total_elapsed_seconds": 69
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T13:26:18.352Z",
            "data": {
              "session_id": "session_1762262778275_dc9lmc5",
              "source_id": "arxiv",
              "paper_id": "2504.16677",
              "start_time": "2025-11-04T13:26:10.418Z",
              "end_time": "2025-11-04T13:26:18.275Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T07:52:37.345Z",
            "data": {
              "session_id": "session_1762761157043_owhu08o",
              "source_id": "arxiv",
              "paper_id": "2504.16677",
              "start_time": "2025-11-10T07:52:06.620Z",
              "end_time": "2025-11-10T07:52:37.043Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 0,
              "total_elapsed_seconds": 30
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T10:07:22.558Z",
            "data": {
              "session_id": "session_1762769242248_hjiwus7",
              "source_id": "arxiv",
              "paper_id": "2504.16677",
              "start_time": "2025-11-10T10:07:15.315Z",
              "end_time": "2025-11-10T10:07:22.248Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T10:09:23.308Z",
            "data": {
              "session_id": "session_1762769363298_nek0tj5",
              "source_id": "arxiv",
              "paper_id": "2504.16677",
              "start_time": "2025-11-10T10:08:36.041Z",
              "end_time": "2025-11-10T10:09:23.298Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 2,
              "total_elapsed_seconds": 47
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T10:51:06.477Z",
            "data": {
              "session_id": "session_1762771865073_qwd9bpu",
              "source_id": "arxiv",
              "paper_id": "2504.16677",
              "start_time": "2025-11-10T10:09:24.466Z",
              "end_time": "2025-11-10T10:51:05.073Z",
              "heartbeat_count": 500,
              "duration_seconds": 2500,
              "idle_seconds": 1,
              "total_elapsed_seconds": 2501
            }
          }
        ]
      },
      "meta": {
        "issue_number": 600,
        "object_id": "interactions:arxiv.2504.16677",
        "created_at": "2025-11-03T14:51:18+00:00",
        "updated_at": "2025-11-10T10:51:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.16677": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.16677",
        "url": "https://arxiv.org/pdf/2504.16677",
        "title": "A Post-trainer's Guide to Multilingual Training Data: Uncovering\n  Cross-lingual Transfer Dynamics",
        "authors": "Luisa Shimabucoro, Ahmet Ustun, Marzieh Fadaee, Sebastian Ruder",
        "abstract": "In order for large language models to be useful across the globe, they are\nfine-tuned to follow instructions on multilingual data. Despite the ubiquity of\nsuch post-training, a clear understanding of the dynamics that enable\ncross-lingual transfer remains elusive. This study examines cross-lingual\ntransfer (CLT) dynamics in realistic post-training settings. We study two model\nfamilies of up to 35B parameters in size trained on carefully controlled\nmixtures of multilingual data on three generative tasks with varying levels of\ncomplexity (summarization, instruction following, and mathematical reasoning)\nin both single-task and multi-task instruction tuning settings. Overall, we\nfind that the dynamics of cross-lingual transfer and multilingual performance\ncannot be explained by isolated variables, varying depending on the combination\nof post-training settings. Finally, we identify the conditions that lead to\neffective cross-lingual transfer in practice.",
        "timestamp": "2025-11-03T14:51:07.542Z",
        "rating": "novote",
        "publishedDate": "2025-04-23T12:52:49Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 599,
        "object_id": "paper:arxiv.2504.16677",
        "created_at": "2025-11-03T14:51:07+00:00",
        "updated_at": "2025-11-03T14:51:29+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2206.09755": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2206.09755",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T14:52:15.620Z",
            "data": {
              "session_id": "session_1762181535613_pj9hubw",
              "source_id": "arxiv",
              "paper_id": "2206.09755",
              "start_time": "2025-11-03T14:52:09.971Z",
              "end_time": "2025-11-03T14:52:15.613Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T15:04:11.439Z",
            "data": {
              "session_id": "session_1762182251433_3rftmo3",
              "source_id": "arxiv",
              "paper_id": "2206.09755",
              "start_time": "2025-11-03T15:03:41.971Z",
              "end_time": "2025-11-03T15:04:11.433Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T15:32:37.730Z",
            "data": {
              "session_id": "session_1762183957256_bgih5dj",
              "source_id": "arxiv",
              "paper_id": "2206.09755",
              "start_time": "2025-11-03T15:32:21.234Z",
              "end_time": "2025-11-03T15:32:37.256Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "issue_number": 602,
        "object_id": "interactions:arxiv.2206.09755",
        "created_at": "2025-11-03T14:52:06+00:00",
        "updated_at": "2025-11-03T15:33:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.07186": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.07186",
        "url": "https://arxiv.org/abs/2507.07186",
        "title": "Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs",
        "authors": "Itay Itzhak, Yonatan Belinkov, Gabriel Stanovsky",
        "abstract": "Large language models (LLMs) exhibit cognitive biases -- systematic tendencies of irrational decision-making, similar to those seen in humans. Prior work has found that these biases vary across models and can be amplified by instruction tuning. However, it remains unclear if these differences in biases stem from pretraining, finetuning, or even random noise due to training stochasticity. We propose a two-step causal experimental approach to disentangle these factors. First, we finetune models multiple times using different random seeds to study how training randomness affects over 3030 cognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping instruction datasets between models to isolate bias sources. This swap uses datasets that led to different bias patterns, directly testing whether biases are dataset-dependent. Our findings reveal that while training randomness introduces some variability, biases are mainly shaped by pretraining: models with the same pretrained backbone exhibit more similar bias patterns than those sharing only finetuning data. These insights suggest that understanding biases in finetuned models requires considering their pretraining origins beyond finetuning effects. This perspective can guide future efforts to develop principled strategies for evaluating and mitigating bias in LLMs.",
        "timestamp": "2025-11-03T15:10:21.345Z",
        "rating": "novote",
        "publishedDate": "2025/07/09",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 603,
        "object_id": "paper:arxiv.2507.07186",
        "created_at": "2025-11-03T15:10:21+00:00",
        "updated_at": "2025-11-03T15:10:45+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.13028": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.13028",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T20:05:51.463Z",
            "data": {
              "session_id": "session_1762200351388_bu7nwba",
              "source_id": "arxiv",
              "paper_id": "2502.13028",
              "start_time": "2025-11-03T20:05:40.650Z",
              "end_time": "2025-11-03T20:05:51.388Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 607,
        "object_id": "interactions:arxiv.2502.13028",
        "created_at": "2025-11-03T20:05:52+00:00",
        "updated_at": "2025-11-03T20:06:29+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2501.15654": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.15654",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T20:05:41.310Z",
            "data": {
              "session_id": "session_1762200340566_1ttc34h",
              "source_id": "arxiv",
              "paper_id": "2501.15654",
              "start_time": "2025-11-03T20:05:18.760Z",
              "end_time": "2025-11-03T20:05:40.566Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T20:16:56.473Z",
            "data": {
              "session_id": "session_1762201016467_lrdj3ze",
              "source_id": "arxiv",
              "paper_id": "2501.15654",
              "start_time": "2025-11-03T20:16:48.946Z",
              "end_time": "2025-11-03T20:16:56.466Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T19:56:57.893Z",
            "data": {
              "session_id": "session_1762286217358_idiahst",
              "source_id": "arxiv",
              "paper_id": "2501.15654",
              "start_time": "2025-11-04T19:56:04.241Z",
              "end_time": "2025-11-04T19:56:57.358Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 3,
              "total_elapsed_seconds": 53
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T07:49:23.320Z",
            "data": {
              "session_id": "session_1762328963309_ah4jggl",
              "source_id": "arxiv",
              "paper_id": "2501.15654",
              "start_time": "2025-11-05T07:49:10.353Z",
              "end_time": "2025-11-05T07:49:23.309Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T08:44:48.123Z",
            "data": {
              "session_id": "session_1762332287843_6okcq73",
              "source_id": "arxiv",
              "paper_id": "2501.15654",
              "start_time": "2025-11-05T08:44:14.752Z",
              "end_time": "2025-11-05T08:44:47.843Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 3,
              "total_elapsed_seconds": 33
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T10:01:30.165Z",
            "data": {
              "session_id": "session_1762336889609_h2ntor2",
              "source_id": "arxiv",
              "paper_id": "2501.15654",
              "start_time": "2025-11-05T09:59:22.468Z",
              "end_time": "2025-11-05T10:01:29.609Z",
              "heartbeat_count": 25,
              "duration_seconds": 125,
              "idle_seconds": 2,
              "total_elapsed_seconds": 127
            }
          }
        ]
      },
      "meta": {
        "issue_number": 606,
        "object_id": "interactions:arxiv.2501.15654",
        "created_at": "2025-11-03T20:05:42+00:00",
        "updated_at": "2025-11-05T10:01:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.15654": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.15654",
        "url": "https://arxiv.org/pdf/2501.15654",
        "title": "People who frequently use ChatGPT for writing tasks are accurate and\n  robust detectors of AI-generated text",
        "authors": "Jenna Russell, Marzena Karpinska, Mohit Iyyer",
        "abstract": "In this paper, we study how well humans can detect text generated by\ncommercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300\nnon-fiction English articles, label them as either human-written or\nAI-generated, and provide paragraph-length explanations for their decisions.\nOur experiments show that annotators who frequently use LLMs for writing tasks\nexcel at detecting AI-generated text, even without any specialized training or\nfeedback. In fact, the majority vote among five such \"expert\" annotators\nmisclassifies only 1 of 300 articles, significantly outperforming most\ncommercial and open-source detectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humanization. Qualitative analysis of the\nexperts' free-form explanations shows that while they rely heavily on specific\nlexical clues ('AI vocabulary'), they also pick up on more complex phenomena\nwithin the text (e.g., formality, originality, clarity) that are challenging to\nassess for automatic detectors. We release our annotated dataset and code to\nspur future research into both human and automated detection of AI-generated\ntext.",
        "timestamp": "2025-11-03T20:05:19.076Z",
        "rating": "novote",
        "publishedDate": "2025-01-26T19:31:34Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 605,
        "object_id": "paper:arxiv.2501.15654",
        "created_at": "2025-11-03T20:05:19+00:00",
        "updated_at": "2025-11-03T20:05:39+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.13028": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.13028",
        "url": "https://arxiv.org/pdf/2502.13028",
        "title": "Whose story is it? Personalizing story generation by inferring author\n  styles",
        "authors": "Nischal Ashok Kumar, Chau Minh Pham, Mohit Iyyer, Andrew Lan",
        "abstract": "Personalization is critical for improving user experience in interactive\nwriting and educational applications, yet remains understudied in story\ngeneration. We study the task of personalizing story generation, where our goal\nis to mimic an author's writing style, given other stories written by them. We\ncollect Mythos, a dataset of 3.6k stories from 112 authors, with an average of\n16 stories per author, across five distinct sources reflecting diverse\nstory-writing settings. We propose a two-stage pipeline for personalized story\ngeneration: first, we infer authors' implicit writing characteristics and\norganize them into an Author Writing Sheet, which is validated by humans to be\nof high quality; second, we simulate the author's persona using tailored\npersona descriptions and personalized story rules. We find that stories\npersonalized using the Author Writing Sheet outperform a non-personalized\nbaseline, achieving a 78% win-rate in capturing authors' past style and 59% in\nsimilarity to ground-truth author stories. Human evaluation supports these\nfindings and further highlights trends, such as Reddit stories being easier to\npersonalize, and the Creativity and Language Use aspects of stories being\neasier to personalize than the Plot.",
        "timestamp": "2025-11-03T20:05:18.186Z",
        "rating": "novote",
        "publishedDate": "2025-02-18T16:45:41Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 604,
        "object_id": "paper:arxiv.2502.13028",
        "created_at": "2025-11-03T20:05:18+00:00",
        "updated_at": "2025-11-03T20:05:44+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.03154": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.03154",
        "url": "https://arxiv.org/pdf/2510.03154",
        "title": "EditLens: Quantifying the Extent of AI Editing in Text",
        "authors": "Katherine Thai, Bradley Emi, Elyas Masrour, Mohit Iyyer",
        "abstract": "A significant proportion of queries to large language models ask them to edit\nuser-provided text, rather than generate new text from scratch. While previous\nwork focuses on detecting fully AI-generated text, we demonstrate that\nAI-edited text is distinguishable from human-written and AI-generated text.\nFirst, we propose using lightweight similarity metrics to quantify the\nmagnitude of AI editing present in a text given the original human-written text\nand validate these metrics with human annotators. Using these similarity\nmetrics as intermediate supervision, we then train EditLens, a regression model\nthat predicts the amount of AI editing present within a text. Our model\nachieves state-of-the-art performance on both binary (F1=94.7%) and ternary\n(F1=90.4%) classification tasks in distinguishing human, AI, and mixed writing.\nNot only do we show that AI-edited text can be detected, but also that the\ndegree of change made by AI to human writing can be detected, which has\nimplications for authorship attribution, education, and policy. Finally, as a\ncase study, we use our model to analyze the effects of AI-edits applied by\nGrammarly, a popular writing assistance tool. To encourage further research, we\ncommit to publicly releasing our models and dataset.",
        "timestamp": "2025-11-03T20:07:22.568Z",
        "rating": "novote",
        "publishedDate": "2025-10-03T16:27:48Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 608,
        "object_id": "paper:arxiv.2510.03154",
        "created_at": "2025-11-03T20:07:22+00:00",
        "updated_at": "2025-11-03T20:07:42+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.03154": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.03154",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T20:08:58.256Z",
            "data": {
              "session_id": "session_1762200538239_iclw7y2",
              "source_id": "arxiv",
              "paper_id": "2510.03154",
              "start_time": "2025-11-03T20:08:41.092Z",
              "end_time": "2025-11-03T20:08:58.239Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T20:10:04.891Z",
            "data": {
              "session_id": "session_1762200604863_684edo4",
              "source_id": "arxiv",
              "paper_id": "2510.03154",
              "start_time": "2025-11-03T20:09:46.333Z",
              "end_time": "2025-11-03T20:10:04.863Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 4,
              "total_elapsed_seconds": 19
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T20:11:54.838Z",
            "data": {
              "session_id": "session_1762200714817_izcpzig",
              "source_id": "arxiv",
              "paper_id": "2510.03154",
              "start_time": "2025-11-03T20:11:42.025Z",
              "end_time": "2025-11-03T20:11:54.817Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T20:17:19.783Z",
            "data": {
              "session_id": "session_1762201039489_ius876p",
              "source_id": "arxiv",
              "paper_id": "2510.03154",
              "start_time": "2025-11-03T20:16:56.951Z",
              "end_time": "2025-11-03T20:17:19.489Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T20:18:43.425Z",
            "data": {
              "session_id": "session_1762201123414_infiu41",
              "source_id": "arxiv",
              "paper_id": "2510.03154",
              "start_time": "2025-11-03T20:18:35.440Z",
              "end_time": "2025-11-03T20:18:43.414Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 609,
        "object_id": "interactions:arxiv.2510.03154",
        "created_at": "2025-11-03T20:08:11+00:00",
        "updated_at": "2025-11-03T20:18:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.14873": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.14873",
        "url": "https://arxiv.org/abs/2402.14873",
        "title": "Technical Report on the Pangram AI-Generated Text Classifier",
        "authors": "Bradley Emi, Max Spero",
        "abstract": "We present Pangram Text, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Pangram Text outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 38 times lower error rates on a comprehensive benchmark comprised of 10 text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q&A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Pangram Text is not biased against nonnative English speakers and generalizes to domains and models unseen during training.",
        "timestamp": "2025-11-03T20:18:53.697Z",
        "rating": "novote",
        "publishedDate": "2024/02/21",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 610,
        "object_id": "paper:arxiv.2402.14873",
        "created_at": "2025-11-03T20:18:54+00:00",
        "updated_at": "2025-11-03T20:19:16+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2402.14873": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.14873",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T20:21:29.822Z",
            "data": {
              "session_id": "session_1762201289552_vj1o3vi",
              "source_id": "arxiv",
              "paper_id": "2402.14873",
              "start_time": "2025-11-03T20:21:20.656Z",
              "end_time": "2025-11-03T20:21:29.552Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T19:52:25.281Z",
            "data": {
              "session_id": "session_1762285945216_pvq935k",
              "source_id": "arxiv",
              "paper_id": "2402.14873",
              "start_time": "2025-11-04T19:51:57.395Z",
              "end_time": "2025-11-04T19:52:25.216Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T20:02:43.298Z",
            "data": {
              "session_id": "session_1762286562953_6ws4o26",
              "source_id": "arxiv",
              "paper_id": "2402.14873",
              "start_time": "2025-11-04T20:02:34.164Z",
              "end_time": "2025-11-04T20:02:42.953Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 611,
        "object_id": "interactions:arxiv.2402.14873",
        "created_at": "2025-11-03T20:21:30+00:00",
        "updated_at": "2025-11-04T20:03:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.04782": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.04782",
        "url": "https://arxiv.org/abs/2507.04782",
        "title": "Reason to Rote: Rethinking Memorization in Reasoning",
        "authors": "Yupei Du, Philipp Mondorf, Silvia Casola, Yuekun Yao, Robert Litschko, Barbara Plank",
        "abstract": "Large language models readily memorize arbitrary training instances, such as label noise, yet they perform strikingly well on reasoning tasks. In this work, we investigate how language models memorize label noise, and why such memorization in many cases does not heavily affect generalizable reasoning capabilities. Using two controllable synthetic reasoning datasets with noisy labels, four-digit addition (FDA) and two-hop relational reasoning (THR), we discover a reliance of memorization on generalizable reasoning mechanisms: models continue to compute intermediate reasoning outputs even when retrieving memorized noisy labels, and intervening reasoning adversely affects memorization. We further show that memorization operates through distributed encoding, i.e., aggregating various inputs and intermediate results, rather than building a look-up mechanism from inputs to noisy labels. Moreover, our FDA case study reveals memorization occurs via outlier heuristics, where existing neuron activation patterns are slightly shifted to fit noisy labels. Together, our findings suggest that memorization of label noise in language models builds on, rather than overrides, the underlying reasoning mechanisms, shedding lights on the intriguing phenomenon of benign memorization.",
        "timestamp": "2025-11-03T20:57:52.068Z",
        "rating": "novote",
        "publishedDate": "2025/07/07",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 612,
        "object_id": "paper:arxiv.2507.04782",
        "created_at": "2025-11-03T20:57:52+00:00",
        "updated_at": "2025-11-03T20:58:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.04782": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.04782",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-03T20:59:01.718Z",
            "data": {
              "session_id": "session_1762203541088_nn76cv4",
              "source_id": "arxiv",
              "paper_id": "2507.04782",
              "start_time": "2025-11-03T20:57:53.816Z",
              "end_time": "2025-11-03T20:59:01.088Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 2,
              "total_elapsed_seconds": 67
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T08:45:39.632Z",
            "data": {
              "session_id": "session_1762418739625_7yr1y1q",
              "source_id": "arxiv",
              "paper_id": "2507.04782",
              "start_time": "2025-11-06T08:45:31.632Z",
              "end_time": "2025-11-06T08:45:39.625Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T08:46:57.975Z",
            "data": {
              "session_id": "session_1762418817254_vvpeh79",
              "source_id": "arxiv",
              "paper_id": "2507.04782",
              "start_time": "2025-11-06T08:45:40.699Z",
              "end_time": "2025-11-06T08:46:57.254Z",
              "heartbeat_count": 15,
              "duration_seconds": 75,
              "idle_seconds": 2,
              "total_elapsed_seconds": 77
            }
          }
        ]
      },
      "meta": {
        "issue_number": 613,
        "object_id": "interactions:arxiv.2507.04782",
        "created_at": "2025-11-03T20:59:02+00:00",
        "updated_at": "2025-11-06T08:47:26+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.03405": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.03405",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T07:47:09.946Z",
            "data": {
              "session_id": "session_1762242429931_x195mve",
              "source_id": "arxiv",
              "paper_id": "2509.03405",
              "start_time": "2025-11-04T07:46:42.247Z",
              "end_time": "2025-11-04T07:47:09.931Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T12:01:36.588Z",
            "data": {
              "session_id": "session_1762257695624_7445v7t",
              "source_id": "arxiv",
              "paper_id": "2509.03405",
              "start_time": "2025-11-04T12:00:48.465Z",
              "end_time": "2025-11-04T12:01:35.624Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 2,
              "total_elapsed_seconds": 47
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T12:04:15.827Z",
            "data": {
              "session_id": "session_1762257855694_8trilam",
              "source_id": "arxiv",
              "paper_id": "2509.03405",
              "start_time": "2025-11-04T12:04:06.115Z",
              "end_time": "2025-11-04T12:04:15.694Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "issue_number": 615,
        "object_id": "interactions:arxiv.2509.03405",
        "created_at": "2025-11-04T07:46:12+00:00",
        "updated_at": "2025-11-04T12:04:37+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.03405": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.03405",
        "url": "https://arxiv.org/pdf/2509.03405",
        "title": "LMEnt: A Suite for Analyzing Knowledge in Language Models from\n  Pretraining Data to Representations",
        "authors": "Daniela Gottesman, Alon Gilae-Dotan, Ido Cohen, Yoav Gur-Arieh, Marius Mosbach, Ori Yoran, Mor Geva",
        "abstract": "Language models (LMs) increasingly drive real-world applications that require\nworld knowledge. However, the internal processes through which models turn data\ninto representations of knowledge and beliefs about the world, are poorly\nunderstood. Insights into these processes could pave the way for developing LMs\nwith knowledge representations that are more consistent, robust, and complete.\nTo facilitate studying these questions, we present LMEnt, a suite for analyzing\nknowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a\nknowledge-rich pretraining corpus, fully annotated with entity mentions, based\non Wikipedia, (2) an entity-based retrieval method over pretraining data that\noutperforms previous approaches by as much as 80.4%, and (3) 12 pretrained\nmodels with up to 1B parameters and 4K intermediate checkpoints, with\ncomparable performance to popular open-sourced models on knowledge benchmarks.\nTogether, these resources provide a controlled environment for analyzing\nconnections between entity mentions in pretraining and downstream performance,\nand the effects of causal interventions in pretraining data. We show the\nutility of LMEnt by studying knowledge acquisition across checkpoints, finding\nthat fact frequency is key, but does not fully explain learning trends. We\nrelease LMEnt to support studies of knowledge in LMs, including knowledge\nrepresentations, plasticity, editing, attribution, and learning dynamics.",
        "timestamp": "2025-11-04T07:45:54.557Z",
        "rating": "novote",
        "publishedDate": "2025-09-03T15:31:18Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 614,
        "object_id": "paper:arxiv.2509.03405",
        "created_at": "2025-11-04T07:45:54+00:00",
        "updated_at": "2025-11-04T07:46:18+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.14824": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.14824",
        "url": "https://arxiv.org/pdf/2505.14824",
        "title": "Tracing Multilingual Factual Knowledge Acquisition in Pretraining",
        "authors": "Yihong Liu, Mingyang Wang, Amir Hossein Kargaran, Felicia K\u00f6rner, Ercong Nie, Barbara Plank, Fran\u00e7ois Yvon, Hinrich Sch\u00fctze",
        "abstract": "Large Language Models (LLMs) are capable of recalling multilingual factual\nknowledge present in their pretraining data. However, most studies evaluate\nonly the final model, leaving the development of factual recall and\ncrosslingual consistency throughout pretraining largely unexplored. In this\nwork, we trace how factual recall and crosslingual consistency evolve during\npretraining, focusing on OLMo-7B as a case study. We find that both accuracy\nand consistency improve over time for most languages. We show that this\nimprovement is primarily driven by the fact frequency in the pretraining\ncorpus: more frequent facts are more likely to be recalled correctly,\nregardless of language. Yet, some low-frequency facts in non-English languages\ncan still be correctly recalled. Our analysis reveals that these instances\nlargely benefit from crosslingual transfer of their English counterparts -- an\neffect that emerges predominantly in the early stages of pretraining. We\npinpoint two distinct pathways through which multilingual factual knowledge\nacquisition occurs: (1) frequency-driven learning, which is dominant and\nlanguage-agnostic, and (2) crosslingual transfer, which is limited in scale and\ntypically constrained to relation types involving named entities. We release\nour code and data to facilitate further research at\nhttps://github.com/cisnlp/multilingual-fact-tracing.",
        "timestamp": "2025-11-04T13:24:28.756Z",
        "rating": "novote",
        "publishedDate": "2025-05-20T18:39:56Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 617,
        "object_id": "paper:arxiv.2505.14824",
        "created_at": "2025-11-04T13:24:29+00:00",
        "updated_at": "2025-11-04T13:24:48+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.24256": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.24256",
        "url": "https://arxiv.org/pdf/2510.24256",
        "title": "From Memorization to Reasoning in the Spectrum of Loss Curvature",
        "authors": "Jack Merullo, Srihita Vatsavaya, Lucius Bushnaq, Owen Lewis",
        "abstract": "We characterize how memorization is represented in transformer models and\nshow that it can be disentangled in the weights of both language models (LMs)\nand vision transformers (ViTs) using a decomposition based on the loss\nlandscape curvature. This insight is based on prior theoretical and empirical\nwork showing that the curvature for memorized training points is much sharper\nthan non memorized, meaning ordering weight components from high to low\ncurvature can reveal a distinction without explicit labels. This motivates a\nweight editing procedure that suppresses far more recitation of untargeted\nmemorized data more effectively than a recent unlearning method\n(BalancedSubnet), while maintaining lower perplexity. Since the basis of\ncurvature has a natural interpretation for shared structure in model weights,\nwe analyze the editing procedure extensively on its effect on downstream tasks\nin LMs, and find that fact retrieval and arithmetic are specifically and\nconsistently negatively affected, even though open book fact retrieval and\ngeneral logical reasoning is conserved. We posit these tasks rely heavily on\nspecialized directions in weight space rather than general purpose mechanisms,\nregardless of whether those individual datapoints are memorized. We support\nthis by showing a correspondence between task data's activation strength with\nlow curvature components that we edit out, and the drop in task performance\nafter the edit. Our work enhances the understanding of memorization in neural\nnetworks with practical applications towards removing it, and provides evidence\nfor idiosyncratic, narrowly-used structures involved in solving tasks like math\nand fact retrieval.",
        "timestamp": "2025-11-04T13:24:16.117Z",
        "rating": "novote",
        "publishedDate": "2025-10-28T10:09:35Z",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 616,
        "object_id": "paper:arxiv.2510.24256",
        "created_at": "2025-11-04T13:24:16+00:00",
        "updated_at": "2025-11-04T13:24:35+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.14824": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.14824",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T13:25:33.739Z",
            "data": {
              "session_id": "session_1762262733730_bvdhnta",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-04T13:25:21.874Z",
              "end_time": "2025-11-04T13:25:33.730Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T13:27:34.252Z",
            "data": {
              "session_id": "session_1762262852943_5p94lng",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-04T13:26:21.885Z",
              "end_time": "2025-11-04T13:27:32.943Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 1,
              "total_elapsed_seconds": 71
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T13:35:31.837Z",
            "data": {
              "session_id": "session_1762263327247_tttvmvd",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-04T13:32:46.538Z",
              "end_time": "2025-11-04T13:35:27.247Z",
              "heartbeat_count": 32,
              "duration_seconds": 160,
              "idle_seconds": 1,
              "total_elapsed_seconds": 161
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:02:08.928Z",
            "data": {
              "session_id": "session_1762693328668_81b8wpb",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-09T13:02:00.801Z",
              "end_time": "2025-11-09T13:02:08.668Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T09:25:09.856Z",
            "data": {
              "session_id": "session_1763717109217_b8llh0q",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-21T09:22:45.452Z",
              "end_time": "2025-11-21T09:25:09.217Z",
              "heartbeat_count": 28,
              "duration_seconds": 140,
              "idle_seconds": 4,
              "total_elapsed_seconds": 144
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T09:43:11.005Z",
            "data": {
              "session_id": "session_1763718190422_y3mabyi",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-21T09:41:07.487Z",
              "end_time": "2025-11-21T09:43:10.422Z",
              "heartbeat_count": 24,
              "duration_seconds": 120,
              "idle_seconds": 3,
              "total_elapsed_seconds": 123
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T09:45:19.587Z",
            "data": {
              "session_id": "session_1763718318996_7gfje1j",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-21T09:43:52.285Z",
              "end_time": "2025-11-21T09:45:18.996Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 2,
              "total_elapsed_seconds": 87
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T09:45:57.734Z",
            "data": {
              "session_id": "session_1763718357569_pofwhgb",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-21T09:45:51.041Z",
              "end_time": "2025-11-21T09:45:57.569Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T09:48:17.242Z",
            "data": {
              "session_id": "session_1763718497235_rz5rjcx",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-21T09:47:41.161Z",
              "end_time": "2025-11-21T09:48:17.235Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 1,
              "total_elapsed_seconds": 36
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T09:49:16.360Z",
            "data": {
              "session_id": "session_1763718556354_9w04e8o",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-21T09:49:05.303Z",
              "end_time": "2025-11-21T09:49:16.354Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T09:51:49.318Z",
            "data": {
              "session_id": "session_1763718708754_hhzbtk5",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-21T09:49:20.874Z",
              "end_time": "2025-11-21T09:51:48.754Z",
              "heartbeat_count": 29,
              "duration_seconds": 145,
              "idle_seconds": 3,
              "total_elapsed_seconds": 148
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T09:54:09.025Z",
            "data": {
              "session_id": "session_1763718848447_b86zd7v",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-21T09:51:50.597Z",
              "end_time": "2025-11-21T09:54:08.447Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 3,
              "total_elapsed_seconds": 138
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T09:56:15.580Z",
            "data": {
              "session_id": "session_1763718975243_imldz1u",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-21T09:55:53.359Z",
              "end_time": "2025-11-21T09:56:15.243Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T10:04:20.487Z",
            "data": {
              "session_id": "session_1763719459830_8tmwp9v",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-21T10:00:25.505Z",
              "end_time": "2025-11-21T10:04:19.830Z",
              "heartbeat_count": 46,
              "duration_seconds": 230,
              "idle_seconds": 4,
              "total_elapsed_seconds": 234
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T13:47:01.245Z",
            "data": {
              "session_id": "session_1763732820607_20zinjr",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-21T13:43:07.618Z",
              "end_time": "2025-11-21T13:47:00.607Z",
              "heartbeat_count": 46,
              "duration_seconds": 230,
              "idle_seconds": 3,
              "total_elapsed_seconds": 233
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T21:03:04.078Z",
            "data": {
              "session_id": "session_1763931784067_75n5i80",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-23T21:02:48.506Z",
              "end_time": "2025-11-23T21:03:04.067Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T21:06:43.378Z",
            "data": {
              "session_id": "session_1763932002785_6t3h4fs",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-23T21:06:02.847Z",
              "end_time": "2025-11-23T21:06:42.785Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 5,
              "total_elapsed_seconds": 40
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T21:08:37.273Z",
            "data": {
              "session_id": "session_1763932116476_u9u2qdz",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-23T21:07:40.833Z",
              "end_time": "2025-11-23T21:08:36.476Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 1,
              "total_elapsed_seconds": 56
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T21:10:53.195Z",
            "data": {
              "session_id": "session_1763932252915_y1b2hov",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-23T21:10:13.719Z",
              "end_time": "2025-11-23T21:10:52.915Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 4,
              "total_elapsed_seconds": 39
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T21:12:22.295Z",
            "data": {
              "session_id": "session_1763932341963_zqxwvl7",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-23T21:11:57.102Z",
              "end_time": "2025-11-23T21:12:21.963Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 5,
              "total_elapsed_seconds": 25
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-24T06:19:07.802Z",
            "data": {
              "session_id": "session_1763965147468_5jzr8mp",
              "source_id": "arxiv",
              "paper_id": "2505.14824",
              "start_time": "2025-11-24T06:19:02.032Z",
              "end_time": "2025-11-24T06:19:07.468Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 619,
        "object_id": "interactions:arxiv.2505.14824",
        "created_at": "2025-11-04T13:25:34+00:00",
        "updated_at": "2025-11-24T06:19:40+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.24256": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.24256",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T10:02:19.013Z",
            "data": {
              "session_id": "session_1762336938243_fe1toyh",
              "source_id": "arxiv",
              "paper_id": "2510.24256",
              "start_time": "2025-11-05T10:01:35.698Z",
              "end_time": "2025-11-05T10:02:18.243Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 3,
              "total_elapsed_seconds": 43
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-07T06:14:23.998Z",
            "data": {
              "session_id": "session_1762496063453_6l4rsng",
              "source_id": "arxiv",
              "paper_id": "2510.24256",
              "start_time": "2025-11-07T06:11:51.053Z",
              "end_time": "2025-11-07T06:14:23.453Z",
              "heartbeat_count": 30,
              "duration_seconds": 150,
              "idle_seconds": 2,
              "total_elapsed_seconds": 152
            }
          }
        ]
      },
      "meta": {
        "issue_number": 618,
        "object_id": "interactions:arxiv.2510.24256",
        "created_at": "2025-11-04T13:25:22+00:00",
        "updated_at": "2025-11-07T06:14:50+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.11721": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.11721",
        "url": "https://arxiv.org/pdf/2408.11721?",
        "title": "Detection-Driven Object Count Optimization for Text-to-Image Diffusion\n  Models",
        "authors": "Oz Zafar, Yuval Cohen, Lior Wolf, Idan Schwartz",
        "abstract": "Accurately controlling object count in text-to-image generation remains a key\nchallenge. Supervised methods often fail, as training data rarely covers all\ncount variations. Methods that manipulate the denoising process to add or\nremove objects can help; however, they still require labeled data, limit\nrobustness and image quality, and rely on a slow, iterative process.\n  Pre-trained differentiable counting models that rely on soft object density\nsummation exist and could steer generation, but employing them presents three\nmain challenges: (i) they are pre-trained on clean images, making them less\neffective during denoising steps that operate on noisy inputs; (ii) they are\nnot robust to viewpoint changes; and (iii) optimization is computationally\nexpensive, requiring repeated model evaluations per image.\n  We propose a new framework that uses pre-trained object counting techniques\nand object detectors to guide generation. First, we optimize a counting token\nusing an outer-loop loss computed on fully generated images. Second, we\nintroduce a detection-driven scaling term that corrects errors caused by\nviewpoint and proportion shifts, among other factors, without requiring\nbackpropagation through the detection model. Third, we show that the optimized\nparameters can be reused for new prompts, removing the need for repeated\noptimization. Our method provides efficiency through token reuse, flexibility\nvia compatibility with various detectors, and accuracy with improved counting\nacross diverse object categories.",
        "timestamp": "2025-11-04T14:56:16.935Z",
        "rating": "novote",
        "publishedDate": "2024-08-21T15:51:46Z",
        "tags": [
          "cs.CV",
          "cs.AI",
          "cs.GR",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 621,
        "object_id": "paper:arxiv.2408.11721",
        "created_at": "2025-11-04T14:56:17+00:00",
        "updated_at": "2025-11-04T14:56:38+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.02226": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.02226",
        "url": "https://arxiv.org/abs/2510.02226",
        "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models",
        "authors": "Shira Schiber, Ofir Lindenbaum, Idan Schwartz",
        "abstract": "Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal shape with a control signal (via correlation), amplifying it where visibility is needed (via energy), and maintaining spatial focus (via entropy). TempoControl allows precise control over timing while ensuring high video quality and diversity. We demonstrate its effectiveness across various video generation applications, including temporal reordering for single and multiple objects, as well as action and audio-aligned generation.",
        "timestamp": "2025-11-04T14:56:06.825Z",
        "rating": "novote",
        "publishedDate": "2025/10/02",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 620,
        "object_id": "paper:arxiv.2510.02226",
        "created_at": "2025-11-04T14:56:07+00:00",
        "updated_at": "2025-11-04T14:56:33+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2408.11721": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.11721",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T14:57:12.325Z",
            "data": {
              "session_id": "session_1762268232318_svd9no7",
              "source_id": "arxiv",
              "paper_id": "2408.11721",
              "start_time": "2025-11-04T14:56:50.265Z",
              "end_time": "2025-11-04T14:57:12.318Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T15:03:02.321Z",
            "data": {
              "session_id": "session_1762268582253_x74upwk",
              "source_id": "arxiv",
              "paper_id": "2408.11721",
              "start_time": "2025-11-04T15:02:21.421Z",
              "end_time": "2025-11-04T15:03:02.253Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 1,
              "total_elapsed_seconds": 41
            }
          }
        ]
      },
      "meta": {
        "issue_number": 623,
        "object_id": "interactions:arxiv.2408.11721",
        "created_at": "2025-11-04T14:57:13+00:00",
        "updated_at": "2025-11-04T15:03:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.02226": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.02226",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T14:56:50.063Z",
            "data": {
              "session_id": "session_1762268210055_nqb7pvk",
              "source_id": "arxiv",
              "paper_id": "2510.02226",
              "start_time": "2025-11-04T14:56:39.191Z",
              "end_time": "2025-11-04T14:56:50.055Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-04T15:02:20.714Z",
            "data": {
              "session_id": "session_1762268540140_emelq3g",
              "source_id": "arxiv",
              "paper_id": "2510.02226",
              "start_time": "2025-11-04T14:57:17.982Z",
              "end_time": "2025-11-04T15:02:20.140Z",
              "heartbeat_count": 60,
              "duration_seconds": 300,
              "idle_seconds": 2,
              "total_elapsed_seconds": 302
            }
          }
        ]
      },
      "meta": {
        "issue_number": 622,
        "object_id": "interactions:arxiv.2510.02226",
        "created_at": "2025-11-04T14:56:50+00:00",
        "updated_at": "2025-11-04T15:02:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2508.13650": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.13650",
        "url": "https://arxiv.org/pdf/2508.13650",
        "title": "CRISP: Persistent Concept Unlearning via Sparse Autoencoders",
        "authors": "Tomer Ashuach, Dana Arad, Aaron Mueller, Martin Tutek, Yonatan Belinkov",
        "abstract": "As large language models (LLMs) are increasingly deployed in real-world\napplications, the need to selectively remove unwanted knowledge while\npreserving model utility has become paramount. Recent work has explored sparse\nautoencoders (SAEs) to perform precise interventions on monosemantic features.\nHowever, most SAE-based methods operate at inference time, which does not\ncreate persistent changes in the model's parameters. Such interventions can be\nbypassed or reversed by malicious actors with parameter access. We introduce\nCRISP, a parameter-efficient method for persistent concept unlearning using\nSAEs. CRISP automatically identifies salient SAE features across multiple\nlayers and suppresses their activations. We experiment with two LLMs and show\nthat our method outperforms prior approaches on safety-critical unlearning\ntasks from the WMDP benchmark, successfully removing harmful knowledge while\npreserving general and in-domain capabilities. Feature-level analysis reveals\nthat CRISP achieves semantically coherent separation between target and benign\nconcepts, allowing precise suppression of the target features.",
        "timestamp": "2025-11-05T09:42:47.874Z",
        "rating": "novote",
        "publishedDate": "2025-08-19T09:01:22Z",
        "tags": [
          "cs.CL",
          "I.2.7",
          "I.2.7"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 626,
        "object_id": "paper:arxiv.2508.13650",
        "created_at": "2025-11-05T09:42:48+00:00",
        "updated_at": "2025-11-05T09:43:08+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.09325": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.09325",
        "url": "https://arxiv.org/pdf/2406.09325v3",
        "title": "REVS: Unlearning Sensitive Information in Language Models via Rank\n  Editing in the Vocabulary Space",
        "authors": "Tomer Ashuach, Martin Tutek, Yonatan Belinkov",
        "abstract": "Language models (LMs) risk inadvertently memorizing and divulging sensitive\nor personally identifiable information (PII) seen in training data, causing\nprivacy concerns. Current approaches to address this issue involve costly\ndataset scrubbing, or model filtering through unlearning and model editing,\nwhich can be bypassed through extraction attacks. We propose REVS, a novel\nnon-gradient-based method for unlearning sensitive information from LMs. REVS\nidentifies and modifies a small subset of neurons relevant for constituent\ntokens that form sensitive information. To adequately evaluate our method on\ntruly sensitive information, we curate three datasets: email and URL datasets\nnaturally memorized by the models, and a synthetic social security number\ndataset that we tune the models to memorize. Compared to other methods, REVS\ndemonstrates superior performance in unlearning sensitive information and\nrobustness to extraction attacks, while retaining underlying model integrity.",
        "timestamp": "2025-11-05T09:42:44.072Z",
        "rating": "novote",
        "publishedDate": "2024-06-13T17:02:32Z",
        "tags": [
          "cs.CL",
          "I.2.7"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 624,
        "object_id": "paper:arxiv.2406.09325",
        "created_at": "2025-11-05T09:42:44+00:00",
        "updated_at": "2025-11-05T09:43:13+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.09325": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.09325",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T09:45:51.371Z",
            "data": {
              "session_id": "session_1762335951066_fxxhtsx",
              "source_id": "arxiv",
              "paper_id": "2406.09325",
              "start_time": "2025-11-05T09:45:45.963Z",
              "end_time": "2025-11-05T09:45:51.066Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T09:48:27.699Z",
            "data": {
              "session_id": "session_1762336107613_h7rdohz",
              "source_id": "arxiv",
              "paper_id": "2406.09325",
              "start_time": "2025-11-05T09:48:20.644Z",
              "end_time": "2025-11-05T09:48:27.613Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T09:49:27.127Z",
            "data": {
              "session_id": "session_1762336167108_vczs0ny",
              "source_id": "arxiv",
              "paper_id": "2406.09325",
              "start_time": "2025-11-05T09:49:16.342Z",
              "end_time": "2025-11-05T09:49:27.108Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 627,
        "object_id": "interactions:arxiv.2406.09325",
        "created_at": "2025-11-05T09:45:52+00:00",
        "updated_at": "2025-11-05T09:49:39+00:00",
        "version": 1
      }
    },
    "interactions:openreview.oymuTd7MjZ": {
      "data": {
        "sourceId": "openreview",
        "paperId": "oymuTd7MjZ",
        "interactions": []
      },
      "meta": {
        "issue_number": 629,
        "object_id": "interactions:openreview.oymuTd7MjZ",
        "created_at": "2025-11-05T19:01:40+00:00",
        "updated_at": "2025-11-05T19:01:42+00:00",
        "version": 1
      }
    },
    "paper:openreview.oymuTd7MjZ": {
      "data": {
        "sourceId": "openreview",
        "paperId": "oymuTd7MjZ",
        "url": "https://openreview.net/forum?id=oymuTd7MjZ&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Unveiling the spectrum of Arabic offensive  language: Taxonomy and insights",
        "authors": "Chaya Liebeskind, Yossef Haim Shrem, Marina Litvak, Natalia Vanetik",
        "abstract": "This paper presents a novel taxonomy designed to classify offensive language in Arabic, filling a notable void in existing literature primarily concentrated on Indo-European languages. Our taxonomy delineates offensive language into seven distinct levels, comprising six explicit levels and one implicit level. Drawing inspiration from the simplified offensive language (SOL) taxonomy outlined in prior work, we adapted it to accommodate the intricacies and linguistic richness of Arabic. In our study, we analyzed existing datasets containing offensive language in Arabic, examining the range of annotations employed within these datasets. This exploration allowed us to gain insights into the diversity of offensive language instances and the methodologies used for their annotation, thereby informing the development of our streamlined taxonomy for categorizing such expressions. Initial examination of datasets uncovers compelling trends and distributions, emphasizing the intricate and distinct nature of offensive expressions in Arabic. We have also analyzed the performance of pre-trained and fine-tuned Arabic transformer offensive language detection models on these datasets. Our results underscore the importance of acknowledging linguistic and cultural diversity in the study and mitigation of online abusive language. We posit that our refined taxonomy and accompanying dataset will be pivotal in advancing research across Semitic languages, including sociocultural studies, natural language processing, and linguistic analyses",
        "timestamp": "2025-11-05T19:01:21.860Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Arabic offensive language",
          "hate speech detection",
          "abusive language taxonomy",
          "transformer models",
          "Arabic dialects",
          "natural language processing"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 628,
        "object_id": "paper:openreview.oymuTd7MjZ",
        "created_at": "2025-11-05T19:01:22+00:00",
        "updated_at": "2025-11-05T19:01:46+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.20810": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.20810",
        "url": "https://arxiv.org/pdf/2510.20810",
        "title": "On the Detectability of LLM-Generated Text: What Exactly Is\n  LLM-Generated Text?",
        "authors": "Mingmeng Geng, Thierry Poibeau",
        "abstract": "With the widespread use of large language models (LLMs), many researchers\nhave turned their attention to detecting text generated by them. However, there\nis no consistent or precise definition of their target, namely \"LLM-generated\ntext\". Differences in usage scenarios and the diversity of LLMs further\nincrease the difficulty of detection. What is commonly regarded as the\ndetecting target usually represents only a subset of the text that LLMs can\npotentially produce. Human edits to LLM outputs, together with the subtle\ninfluences that LLMs exert on their users, are blurring the line between\nLLM-generated and human-written text. Existing benchmarks and evaluation\napproaches do not adequately address the various conditions in real-world\ndetector applications. Hence, the numerical results of detectors are often\nmisunderstood, and their significance is diminishing. Therefore, detectors\nremain useful under specific conditions, but their results should be\ninterpreted only as references rather than decisive indicators.",
        "timestamp": "2025-11-05T20:11:25.111Z",
        "rating": "novote",
        "publishedDate": "2025-10-23T17:59:06Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CY",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 630,
        "object_id": "paper:arxiv.2510.20810",
        "created_at": "2025-11-05T20:11:25+00:00",
        "updated_at": "2025-11-05T20:11:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.12014": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.12014",
        "url": "https://arxiv.org/pdf/2506.12014",
        "title": "code_transformed: The Influence of Large Language Models on Code",
        "authors": "Yuliang Xu, Siming Huang, Mingmeng Geng, Yao Wan, Xuanhua Shi, Dongping Chen",
        "abstract": "Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.",
        "timestamp": "2025-11-05T20:12:59.697Z",
        "rating": "novote",
        "publishedDate": "2025-06-13T17:59:39Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG",
          "cs.SE"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 632,
        "object_id": "paper:arxiv.2506.12014",
        "created_at": "2025-11-05T20:13:00+00:00",
        "updated_at": "2025-11-05T20:13:21+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.20810": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.20810",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:12:47.920Z",
            "data": {
              "session_id": "session_1762373567225_r6yepno",
              "source_id": "arxiv",
              "paper_id": "2510.20810",
              "start_time": "2025-11-05T20:11:24.796Z",
              "end_time": "2025-11-05T20:12:47.225Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 2,
              "total_elapsed_seconds": 82
            }
          }
        ]
      },
      "meta": {
        "issue_number": 631,
        "object_id": "interactions:arxiv.2510.20810",
        "created_at": "2025-11-05T20:12:48+00:00",
        "updated_at": "2025-11-05T20:13:13+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.12014": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.12014",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:14:17.688Z",
            "data": {
              "session_id": "session_1762373657042_zz7c27t",
              "source_id": "arxiv",
              "paper_id": "2506.12014",
              "start_time": "2025-11-05T20:12:59.417Z",
              "end_time": "2025-11-05T20:14:17.042Z",
              "heartbeat_count": 15,
              "duration_seconds": 75,
              "idle_seconds": 3,
              "total_elapsed_seconds": 78
            }
          }
        ]
      },
      "meta": {
        "issue_number": 633,
        "object_id": "interactions:arxiv.2506.12014",
        "created_at": "2025-11-05T20:14:18+00:00",
        "updated_at": "2025-11-05T20:14:43+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.09606": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.09606",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:15:04.083Z",
            "data": {
              "session_id": "session_1762373703545_cwh88rv",
              "source_id": "arxiv",
              "paper_id": "2502.09606",
              "start_time": "2025-11-05T20:14:55.782Z",
              "end_time": "2025-11-05T20:15:03.545Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:16:10.690Z",
            "data": {
              "session_id": "session_1762373770075_vb3egso",
              "source_id": "arxiv",
              "paper_id": "2502.09606",
              "start_time": "2025-11-05T20:15:07.573Z",
              "end_time": "2025-11-05T20:16:10.075Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 3,
              "total_elapsed_seconds": 63
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:23:48.315Z",
            "data": {
              "session_id": "session_1762374228308_mqrb8nd",
              "source_id": "arxiv",
              "paper_id": "2502.09606",
              "start_time": "2025-11-05T20:23:42.811Z",
              "end_time": "2025-11-05T20:23:48.308Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:24:38.428Z",
            "data": {
              "session_id": "session_1762374278399_5dscga3",
              "source_id": "arxiv",
              "paper_id": "2502.09606",
              "start_time": "2025-11-05T20:24:31.184Z",
              "end_time": "2025-11-05T20:24:38.399Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:25:12.247Z",
            "data": {
              "session_id": "session_1762374312220_306dmeb",
              "source_id": "arxiv",
              "paper_id": "2502.09606",
              "start_time": "2025-11-05T20:25:01.234Z",
              "end_time": "2025-11-05T20:25:12.220Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:36:34.730Z",
            "data": {
              "session_id": "session_1762374994727_gb4196u",
              "source_id": "arxiv",
              "paper_id": "2502.09606",
              "start_time": "2025-11-05T20:36:27.241Z",
              "end_time": "2025-11-05T20:36:34.727Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:39:26.381Z",
            "data": {
              "session_id": "session_1762375165547_51zk51a",
              "source_id": "arxiv",
              "paper_id": "2502.09606",
              "start_time": "2025-11-05T20:36:50.090Z",
              "end_time": "2025-11-05T20:39:25.547Z",
              "heartbeat_count": 31,
              "duration_seconds": 155,
              "idle_seconds": 0,
              "total_elapsed_seconds": 155
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T05:13:38.285Z",
            "data": {
              "session_id": "session_1762406018277_6vhw3w9",
              "source_id": "arxiv",
              "paper_id": "2502.09606",
              "start_time": "2025-11-06T05:13:27.064Z",
              "end_time": "2025-11-06T05:13:38.277Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 640,
        "object_id": "interactions:arxiv.2502.09606",
        "created_at": "2025-11-05T20:15:05+00:00",
        "updated_at": "2025-11-06T05:14:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2409.13686": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.13686",
        "url": "https://arxiv.org/pdf/2409.13686?",
        "title": "The Impact of Large Language Models in Academia: from Writing to\n  Speaking",
        "authors": "Mingmeng Geng, Caixi Chen, Yanru Wu, Dongping Chen, Yao Wan, Pan Zhou",
        "abstract": "Large language models (LLMs) are increasingly impacting human society,\nparticularly in textual information. Based on more than 30,000 papers and 1,000\npresentations from machine learning conferences, we examined and compared the\nwords used in writing and speaking, representing the first large-scale study of\nhow LLMs influence the two main modes of verbal communication and expression\nwithin the same group of people. Our empirical results show that LLM-style\nwords such as \"significant\" have been used more frequently in abstracts and\noral presentations. The impact on speaking is beginning to emerge and is likely\nto grow in the future, calling attention to the implicit influence and ripple\neffect of LLMs on human society.",
        "timestamp": "2025-11-05T20:14:56.864Z",
        "rating": "novote",
        "publishedDate": "2024-09-20T17:54:16Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CY",
          "cs.DL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 639,
        "object_id": "paper:arxiv.2409.13686",
        "created_at": "2025-11-05T20:14:57+00:00",
        "updated_at": "2025-11-05T20:15:22+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.08627": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.08627",
        "url": "https://arxiv.org/pdf/2404.08627?",
        "title": "Is ChatGPT Transforming Academics' Writing Style?",
        "authors": "Mingmeng Geng, Roberto Trotta",
        "abstract": "Based on one million arXiv papers submitted from May 2018 to January 2024, we\nassess the textual density of ChatGPT's writing style in their abstracts\nthrough a statistical analysis of word frequency changes. Our model is\ncalibrated and validated on a mixture of real abstracts and ChatGPT-modified\nabstracts (simulated data) after a careful noise analysis. The words used for\nestimation are not fixed but adaptive, including those with decreasing\nfrequency. We find that large language models (LLMs), represented by ChatGPT,\nare having an increasing impact on arXiv abstracts, especially in the field of\ncomputer science, where the fraction of LLM-style abstracts is estimated to be\napproximately 35%, if we take the responses of GPT-3.5 to one simple prompt,\n\"revise the following sentences\", as a baseline. We conclude with an analysis\nof both positive and negative aspects of the penetration of LLMs into\nacademics' writing style.",
        "timestamp": "2025-11-05T20:14:56.026Z",
        "rating": "novote",
        "publishedDate": "2024-04-12T17:41:05Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.DL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 638,
        "object_id": "paper:arxiv.2404.08627",
        "created_at": "2025-11-05T20:14:56+00:00",
        "updated_at": "2025-11-05T20:15:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2405.19323": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.19323",
        "url": "https://arxiv.org/pdf/2405.19323?",
        "title": "Are Large Language Models Chameleons? An Attempt to Simulate Social\n  Surveys",
        "authors": "Mingmeng Geng, Sihong He, Roberto Trotta",
        "abstract": "Can large language models (LLMs) simulate social surveys? To answer this\nquestion, we conducted millions of simulations in which LLMs were asked to\nanswer subjective questions. A comparison of different LLM responses with the\nEuropean Social Survey (ESS) data suggests that the effect of prompts on bias\nand variability is fundamental, highlighting major cultural, age, and gender\nbiases. We further discussed statistical methods for measuring the difference\nbetween LLM answers and survey data and proposed a novel measure inspired by\nJaccard similarity, as LLM-generated responses are likely to have a smaller\nvariance. Our experiments also reveal that it is important to analyze the\nrobustness and variability of prompts before using LLMs to simulate social\nsurveys, as their imitation abilities are approximate at best.",
        "timestamp": "2025-11-05T20:14:55.676Z",
        "rating": "novote",
        "publishedDate": "2024-05-29T17:54:22Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CY",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 635,
        "object_id": "paper:arxiv.2405.19323",
        "created_at": "2025-11-05T20:14:56+00:00",
        "updated_at": "2025-11-05T20:15:28+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.09606": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.09606",
        "url": "https://arxiv.org/pdf/2502.09606?",
        "title": "Human-LLM Coevolution: Evidence from Academic Writing",
        "authors": "Mingmeng Geng, Roberto Trotta",
        "abstract": "With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency due to\nLLMs' disfavor.",
        "timestamp": "2025-11-05T20:14:54.443Z",
        "rating": "novote",
        "publishedDate": "2025-02-13T18:55:56Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CY",
          "cs.DL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 634,
        "object_id": "paper:arxiv.2502.09606",
        "created_at": "2025-11-05T20:14:55+00:00",
        "updated_at": "2025-11-05T20:15:12+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.16887": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.16887",
        "url": "https://arxiv.org/pdf/2403.16887",
        "title": "ChatGPT \"contamination\": estimating the prevalence of LLMs in the\n  scholarly literature",
        "authors": "Andrew Gray",
        "abstract": "The use of ChatGPT and similar Large Language Model (LLM) tools in scholarly\ncommunication and academic publishing has been widely discussed since they\nbecame easily accessible to a general audience in late 2022. This study uses\nkeywords known to be disproportionately present in LLM-generated text to\nprovide an overall estimate for the prevalence of LLM-assisted writing in the\nscholarly literature. For the publishing year 2023, it is found that several of\nthose keywords show a distinctive and disproportionate increase in their\nprevalence, individually and in combination. It is estimated that at least\n60,000 papers (slightly over 1% of all articles) were LLM-assisted, though this\nnumber could be extended and refined by analysis of other characteristics of\nthe papers or by identification of further indicative keywords.",
        "timestamp": "2025-11-05T20:24:42.508Z",
        "rating": "novote",
        "publishedDate": "2024-03-25T15:56:37Z",
        "tags": [
          "cs.DL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 646,
        "object_id": "paper:arxiv.2403.16887",
        "created_at": "2025-11-05T20:24:42+00:00",
        "updated_at": "2025-11-05T20:25:03+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.01268": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.01268",
        "url": "https://arxiv.org/pdf/2404.01268?",
        "title": "Mapping the Increasing Use of LLMs in Scientific Papers",
        "authors": "Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher D Manning, James Y. Zou",
        "abstract": "Scientific publishing lays the foundation of science by disseminating\nresearch findings, fostering collaboration, encouraging reproducibility, and\nensuring that scientific knowledge is accessible, verifiable, and built upon\nover time. Recently, there has been immense speculation about how many people\nare using large language models (LLMs) like ChatGPT in their academic writing,\nand to what extent this tool might have an effect on global scientific\npractices. However, we lack a precise measure of the proportion of academic\nwriting substantially modified or produced by LLMs. To address this gap, we\nconduct the first systematic, large-scale analysis across 950,965 papers\npublished between January 2020 and February 2024 on the arXiv, bioRxiv, and\nNature portfolio journals, using a population-level statistical framework to\nmeasure the prevalence of LLM-modified content over time. Our statistical\nestimation operates on the corpus level and is more robust than inference on\nindividual instances. Our findings reveal a steady increase in LLM usage, with\nthe largest and fastest growth observed in Computer Science papers (up to\n17.5%). In comparison, Mathematics papers and the Nature portfolio showed the\nleast LLM modification (up to 6.3%). Moreover, at an aggregate level, our\nanalysis reveals that higher levels of LLM-modification are associated with\npapers whose first authors post preprints more frequently, papers in more\ncrowded research areas, and papers of shorter lengths. Our findings suggests\nthat LLMs are being broadly used in scientific writings.",
        "timestamp": "2025-11-05T20:24:27.816Z",
        "rating": "novote",
        "publishedDate": "2024-04-01T17:45:15Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.DL",
          "cs.LG",
          "cs.SI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 645,
        "object_id": "paper:arxiv.2404.01268",
        "created_at": "2025-11-05T20:24:28+00:00",
        "updated_at": "2025-11-05T20:24:47+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.07183": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.07183",
        "url": "https://arxiv.org/pdf/2403.07183?",
        "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of\n  ChatGPT on AI Conference Peer Reviews",
        "authors": "Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, Daniel A. McFarland, James Y. Zou",
        "abstract": "We present an approach for estimating the fraction of text in a large corpus\nwhich is likely to be substantially modified or produced by a large language\nmodel (LLM). Our maximum likelihood model leverages expert-written and\nAI-generated reference texts to accurately and efficiently examine real-world\nLLM-use at the corpus level. We apply this approach to a case study of\nscientific peer review in AI conferences that took place after the release of\nChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest\nthat between 6.5% and 16.9% of text submitted as peer reviews to these\nconferences could have been substantially modified by LLMs, i.e. beyond\nspell-checking or minor writing updates. The circumstances in which generated\ntext occurs offer insight into user behavior: the estimated fraction of\nLLM-generated text is higher in reviews which report lower confidence, were\nsubmitted close to the deadline, and from reviewers who are less likely to\nrespond to author rebuttals. We also observe corpus-level trends in generated\ntext which may be too subtle to detect at the individual level, and discuss the\nimplications of such trends on peer review. We call for future\ninterdisciplinary work to examine how LLM use is changing our information and\nknowledge practices.",
        "timestamp": "2025-11-05T20:24:09.387Z",
        "rating": "novote",
        "publishedDate": "2024-03-11T21:51:39Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG",
          "cs.SI",
          "I.2.7"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 644,
        "object_id": "paper:arxiv.2403.07183",
        "created_at": "2025-11-05T20:24:09+00:00",
        "updated_at": "2025-11-05T20:24:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2409.13686": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.13686",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:23:42.132Z",
            "data": {
              "session_id": "session_1762374221491_bdkqm7u",
              "source_id": "arxiv",
              "paper_id": "2409.13686",
              "start_time": "2025-11-05T20:23:32.273Z",
              "end_time": "2025-11-05T20:23:41.491Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 643,
        "object_id": "interactions:arxiv.2409.13686",
        "created_at": "2025-11-05T20:23:42+00:00",
        "updated_at": "2025-11-05T20:24:11+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.08627": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.08627",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:23:32.408Z",
            "data": {
              "session_id": "session_1762374212086_gsyg7zr",
              "source_id": "arxiv",
              "paper_id": "2404.08627",
              "start_time": "2025-11-05T20:23:19.419Z",
              "end_time": "2025-11-05T20:23:32.086Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:44:16.908Z",
            "data": {
              "session_id": "session_1762375456598_ljn0l5l",
              "source_id": "arxiv",
              "paper_id": "2404.08627",
              "start_time": "2025-11-05T20:43:39.756Z",
              "end_time": "2025-11-05T20:44:16.598Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          }
        ]
      },
      "meta": {
        "issue_number": 642,
        "object_id": "interactions:arxiv.2404.08627",
        "created_at": "2025-11-05T20:23:33+00:00",
        "updated_at": "2025-11-05T20:44:38+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2405.19323": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.19323",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:23:19.512Z",
            "data": {
              "session_id": "session_1762374199215_eul0raf",
              "source_id": "arxiv",
              "paper_id": "2405.19323",
              "start_time": "2025-11-05T20:23:11.457Z",
              "end_time": "2025-11-05T20:23:19.215Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 641,
        "object_id": "interactions:arxiv.2405.19323",
        "created_at": "2025-11-05T20:23:20+00:00",
        "updated_at": "2025-11-05T20:23:45+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2403.07183": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.07183",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:26:07.409Z",
            "data": {
              "session_id": "session_1762374367400_hrmy1l6",
              "source_id": "arxiv",
              "paper_id": "2403.07183",
              "start_time": "2025-11-05T20:25:15.550Z",
              "end_time": "2025-11-05T20:26:07.400Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 2,
              "total_elapsed_seconds": 52
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:32:20.865Z",
            "data": {
              "session_id": "session_1762374740295_u8d6cp8",
              "source_id": "arxiv",
              "paper_id": "2403.07183",
              "start_time": "2025-11-05T20:27:11.029Z",
              "end_time": "2025-11-05T20:32:20.295Z",
              "heartbeat_count": 61,
              "duration_seconds": 305,
              "idle_seconds": 4,
              "total_elapsed_seconds": 309
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T04:15:16.245Z",
            "data": {
              "session_id": "session_1762402515697_l6u2705",
              "source_id": "arxiv",
              "paper_id": "2403.07183",
              "start_time": "2025-11-06T04:13:56.067Z",
              "end_time": "2025-11-06T04:15:15.697Z",
              "heartbeat_count": 15,
              "duration_seconds": 75,
              "idle_seconds": 5,
              "total_elapsed_seconds": 80
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T04:27:38.196Z",
            "data": {
              "session_id": "session_1762403258192_l47o8ap",
              "source_id": "arxiv",
              "paper_id": "2403.07183",
              "start_time": "2025-11-06T04:27:27.031Z",
              "end_time": "2025-11-06T04:27:38.192Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T04:28:14.515Z",
            "data": {
              "session_id": "session_1762403294500_jh8jznk",
              "source_id": "arxiv",
              "paper_id": "2403.07183",
              "start_time": "2025-11-06T04:28:08.688Z",
              "end_time": "2025-11-06T04:28:14.500Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T04:30:56.301Z",
            "data": {
              "session_id": "session_1762403456281_6402tzf",
              "source_id": "arxiv",
              "paper_id": "2403.07183",
              "start_time": "2025-11-06T04:30:15.844Z",
              "end_time": "2025-11-06T04:30:56.281Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 0,
              "total_elapsed_seconds": 40
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T04:55:15.904Z",
            "data": {
              "session_id": "session_1762404915626_m8gqtk9",
              "source_id": "arxiv",
              "paper_id": "2403.07183",
              "start_time": "2025-11-06T04:55:05.319Z",
              "end_time": "2025-11-06T04:55:15.626Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T05:13:43.483Z",
            "data": {
              "session_id": "session_1762406023470_w2x9uyi",
              "source_id": "arxiv",
              "paper_id": "2403.07183",
              "start_time": "2025-11-06T05:13:38.414Z",
              "end_time": "2025-11-06T05:13:43.470Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T08:36:43.842Z",
            "data": {
              "session_id": "session_1762418203349_07t4upb",
              "source_id": "arxiv",
              "paper_id": "2403.07183",
              "start_time": "2025-11-06T08:35:59.493Z",
              "end_time": "2025-11-06T08:36:43.349Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 4,
              "total_elapsed_seconds": 44
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T16:31:27.112Z",
            "data": {
              "session_id": "session_1762446686440_x819dra",
              "source_id": "arxiv",
              "paper_id": "2403.07183",
              "start_time": "2025-11-06T16:29:51.640Z",
              "end_time": "2025-11-06T16:31:26.440Z",
              "heartbeat_count": 18,
              "duration_seconds": 90,
              "idle_seconds": 5,
              "total_elapsed_seconds": 95
            }
          }
        ]
      },
      "meta": {
        "issue_number": 647,
        "object_id": "interactions:arxiv.2403.07183",
        "created_at": "2025-11-05T20:26:08+00:00",
        "updated_at": "2025-11-06T16:31:55+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.15799": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.15799",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:36:26.547Z",
            "data": {
              "session_id": "session_1762374985779_oj695fc",
              "source_id": "arxiv",
              "paper_id": "2404.15799",
              "start_time": "2025-11-05T20:36:13.096Z",
              "end_time": "2025-11-05T20:36:25.779Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-05T20:51:10.616Z",
            "data": {
              "session_id": "session_1762375870045_6u92t8e",
              "source_id": "arxiv",
              "paper_id": "2404.15799",
              "start_time": "2025-11-05T20:44:58.718Z",
              "end_time": "2025-11-05T20:51:10.045Z",
              "heartbeat_count": 74,
              "duration_seconds": 370,
              "idle_seconds": 1,
              "total_elapsed_seconds": 371
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T05:13:56.449Z",
            "data": {
              "session_id": "session_1762406036211_32yecqr",
              "source_id": "arxiv",
              "paper_id": "2404.15799",
              "start_time": "2025-11-06T05:13:49.731Z",
              "end_time": "2025-11-06T05:13:56.211Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 649,
        "object_id": "interactions:arxiv.2404.15799",
        "created_at": "2025-11-05T20:36:27+00:00",
        "updated_at": "2025-11-06T05:14:19+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.15799": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.15799",
        "url": "https://arxiv.org/pdf/2404.15799",
        "title": "Towards the relationship between AIGC in manuscript writing and author\n  profiles: evidence from preprints in LLMs",
        "authors": "Jialin Liu, Yi Bu",
        "abstract": "AIGC tools such as ChatGPT have profoundly changed scientific research,\nleading to widespread attention on its use on academic writing. Leveraging\npreprints from large language models, this study examined the use of AIGC in\nmanuscript writing and its correlation with author profiles. We found that: (1)\nsince the release of ChatGPT, the likelihood of abstracts being AI-generated\nhas gradually increased; (2) scientists from English-speaking countries are\nless likely to use AIGC tools for writing assistance, while those from\ncountries with linguistic differences from English are more likely to use these\ntools; (3) there is weak correlation between a paper's AI-generated probability\nand authors' academic performance; and (4) authors who have previously\npublished papers with high AI-generated probabilities are more likely to\ncontinue using AIGC tools. We believe that this paper provides insightful\nresults for relevant policies and norms and in enhancing the understanding of\nthe relationship between humans and AI.",
        "timestamp": "2025-11-05T20:36:13.646Z",
        "rating": "novote",
        "publishedDate": "2024-04-24T10:53:39Z",
        "tags": [
          "cs.DL",
          "J.0"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 648,
        "object_id": "paper:arxiv.2404.15799",
        "created_at": "2025-11-05T20:36:14+00:00",
        "updated_at": "2025-11-05T20:36:38+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.01268": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.01268",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T04:27:56.773Z",
            "data": {
              "session_id": "session_1762403276763_7a53tt1",
              "source_id": "arxiv",
              "paper_id": "2404.01268",
              "start_time": "2025-11-06T04:27:45.384Z",
              "end_time": "2025-11-06T04:27:56.763Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T04:30:16.012Z",
            "data": {
              "session_id": "session_1762403415491_mbl276o",
              "source_id": "arxiv",
              "paper_id": "2404.01268",
              "start_time": "2025-11-06T04:28:15.164Z",
              "end_time": "2025-11-06T04:30:15.491Z",
              "heartbeat_count": 24,
              "duration_seconds": 120,
              "idle_seconds": 0,
              "total_elapsed_seconds": 120
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T04:34:28.289Z",
            "data": {
              "session_id": "session_1762403668271_roy6gwh",
              "source_id": "arxiv",
              "paper_id": "2404.01268",
              "start_time": "2025-11-06T04:33:57.511Z",
              "end_time": "2025-11-06T04:34:28.271Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 1,
              "total_elapsed_seconds": 31
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T04:54:47.010Z",
            "data": {
              "session_id": "session_1762404886367_v78dn9r",
              "source_id": "arxiv",
              "paper_id": "2404.01268",
              "start_time": "2025-11-06T04:49:59.261Z",
              "end_time": "2025-11-06T04:54:46.367Z",
              "heartbeat_count": 57,
              "duration_seconds": 285,
              "idle_seconds": 2,
              "total_elapsed_seconds": 287
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T04:56:31.815Z",
            "data": {
              "session_id": "session_1762404991218_fvxkfel",
              "source_id": "arxiv",
              "paper_id": "2404.01268",
              "start_time": "2025-11-06T04:55:32.095Z",
              "end_time": "2025-11-06T04:56:31.218Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 4,
              "total_elapsed_seconds": 59
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T05:14:29.365Z",
            "data": {
              "session_id": "session_1762406069348_alezmao",
              "source_id": "arxiv",
              "paper_id": "2404.01268",
              "start_time": "2025-11-06T05:14:18.682Z",
              "end_time": "2025-11-06T05:14:29.348Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T05:15:48.818Z",
            "data": {
              "session_id": "session_1762406148537_50w1zip",
              "source_id": "arxiv",
              "paper_id": "2404.01268",
              "start_time": "2025-11-06T05:15:13.240Z",
              "end_time": "2025-11-06T05:15:48.537Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 0,
              "total_elapsed_seconds": 35
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T05:54:14.623Z",
            "data": {
              "session_id": "session_1762408454050_brs3nyk",
              "source_id": "arxiv",
              "paper_id": "2404.01268",
              "start_time": "2025-11-06T05:51:55.503Z",
              "end_time": "2025-11-06T05:54:14.050Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 4,
              "total_elapsed_seconds": 139
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T08:16:15.317Z",
            "data": {
              "session_id": "session_1762416974591_bkp6ggm",
              "source_id": "arxiv",
              "paper_id": "2404.01268",
              "start_time": "2025-11-06T08:14:12.747Z",
              "end_time": "2025-11-06T08:16:14.591Z",
              "heartbeat_count": 24,
              "duration_seconds": 120,
              "idle_seconds": 2,
              "total_elapsed_seconds": 122
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T12:11:13.351Z",
            "data": {
              "session_id": "session_1763899873282_awfa33y",
              "source_id": "arxiv",
              "paper_id": "2404.01268",
              "start_time": "2025-11-23T12:11:05.364Z",
              "end_time": "2025-11-23T12:11:13.282Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 651,
        "object_id": "interactions:arxiv.2404.01268",
        "created_at": "2025-11-06T04:27:12+00:00",
        "updated_at": "2025-11-23T12:11:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.10": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "10",
        "url": "https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/?ref=404media.co",
        "title": "Attention Authors: Updated Practice for Review Articles and Position Papers in arXiv CS Category \u2013 arXiv blog",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-06T04:34:47.703Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 652,
        "object_id": "paper:arxiv.10",
        "created_at": "2025-11-06T04:34:48+00:00",
        "updated_at": "2025-11-06T04:35:08+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.10": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "10",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T04:39:33.797Z",
            "data": {
              "session_id": "session_1762403973217_85n9obr",
              "source_id": "arxiv",
              "paper_id": "10",
              "start_time": "2025-11-06T04:34:47.393Z",
              "end_time": "2025-11-06T04:39:33.217Z",
              "heartbeat_count": 57,
              "duration_seconds": 285,
              "idle_seconds": 1,
              "total_elapsed_seconds": 286
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T05:03:40.204Z",
            "data": {
              "session_id": "session_1762405419621_h2vui1y",
              "source_id": "arxiv",
              "paper_id": "10",
              "start_time": "2025-11-06T04:56:36.422Z",
              "end_time": "2025-11-06T05:03:39.621Z",
              "heartbeat_count": 84,
              "duration_seconds": 420,
              "idle_seconds": 3,
              "total_elapsed_seconds": 423
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T12:20:01.359Z",
            "data": {
              "session_id": "session_1763900401065_ohufcmm",
              "source_id": "arxiv",
              "paper_id": "10",
              "start_time": "2025-11-23T12:19:54.170Z",
              "end_time": "2025-11-23T12:20:01.065Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T14:33:52.016Z",
            "data": {
              "session_id": "session_1764167631676_78c6a5c",
              "source_id": "arxiv",
              "paper_id": "10",
              "start_time": "2025-11-26T14:33:35.730Z",
              "end_time": "2025-11-26T14:33:51.676Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T14:34:36.322Z",
            "data": {
              "session_id": "session_1764167676024_fdvyje9",
              "source_id": "arxiv",
              "paper_id": "10",
              "start_time": "2025-11-26T14:34:22.479Z",
              "end_time": "2025-11-26T14:34:36.024Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T14:45:06.948Z",
            "data": {
              "session_id": "session_1764168306185_9fkgqnv",
              "source_id": "arxiv",
              "paper_id": "10",
              "start_time": "2025-11-26T14:34:45.667Z",
              "end_time": "2025-11-26T14:45:06.185Z",
              "heartbeat_count": 124,
              "duration_seconds": 620,
              "idle_seconds": 1,
              "total_elapsed_seconds": 621
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T14:50:06.780Z",
            "data": {
              "session_id": "session_1764168606381_vj3sozg",
              "source_id": "arxiv",
              "paper_id": "10",
              "start_time": "2025-11-26T14:50:00.947Z",
              "end_time": "2025-11-26T14:50:06.381Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 653,
        "object_id": "interactions:arxiv.10",
        "created_at": "2025-11-06T04:39:34+00:00",
        "updated_at": "2025-11-26T14:50:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.07192": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.07192",
        "url": "https://arxiv.org/abs/2510.07192",
        "title": "Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples",
        "authors": "Alexandra Souly, Javier Rando, Ed Chapman, Xander Davies, Burak Hasircioglu, Ezzeldin Shereen, Carlos Mougan, Vasilios Mavroudis, Erik Jones, Chris Hicks, Nicholas Carlini, Yarin Gal, Robert Kirk",
        "abstract": "Poisoning attacks can compromise the safety of large language models (LLMs) by injecting malicious documents into their training data. Existing work has studied pretraining poisoning assuming adversaries control a percentage of the training corpus. However, for large models, even small percentages translate to impractically large amounts of data. This work demonstrates for the first time that poisoning attacks instead require a near-constant number of documents regardless of dataset size. We conduct the largest pretraining poisoning experiments to date, pretraining models from 600M to 13B parameters on chinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned documents similarly compromise models across all model and dataset sizes, despite the largest models training on more than 20 times more clean data. We also run smaller-scale experiments to ablate factors that could influence attack success, including broader ratios of poisoned to clean data and non-random distributions of poisoned samples. Finally, we demonstrate the same dynamics for poisoning during fine-tuning. Altogether, our results suggest that injecting backdoors through data poisoning may be easier for large models than previously believed as the number of poisons required does not scale up with model size, highlighting the need for more research on defences to mitigate this risk in future models.",
        "timestamp": "2025-11-06T09:05:40.275Z",
        "rating": "novote",
        "publishedDate": "2025/10/08",
        "tags": [
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 654,
        "object_id": "paper:arxiv.2510.07192",
        "created_at": "2025-11-06T09:05:40+00:00",
        "updated_at": "2025-11-06T09:06:08+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.07192": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.07192",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T09:07:22.930Z",
            "data": {
              "session_id": "session_1762420041501_uuwx51r",
              "source_id": "arxiv",
              "paper_id": "2510.07192",
              "start_time": "2025-11-06T09:07:15.042Z",
              "end_time": "2025-11-06T09:07:21.501Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 656,
        "object_id": "interactions:arxiv.2510.07192",
        "created_at": "2025-11-06T09:07:15+00:00",
        "updated_at": "2025-11-06T09:07:56+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2304.14530": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2304.14530",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-19T07:48:19.600Z",
            "data": {
              "session_id": "session_1763538499347_d4l3kuu",
              "source_id": "arxiv",
              "paper_id": "2304.14530",
              "start_time": "2025-11-19T07:48:07.864Z",
              "end_time": "2025-11-19T07:48:19.347Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 658,
        "object_id": "interactions:arxiv.2304.14530",
        "created_at": "2025-11-06T13:16:04+00:00",
        "updated_at": "2025-11-19T07:48:40+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2304.14530": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2304.14530",
        "url": "https://arxiv.org/pdf/2304.14530",
        "title": "Generating images of rare concepts using pre-trained diffusion models",
        "authors": "Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, Gal Chechik",
        "abstract": "Text-to-image diffusion models can synthesize high-quality images, but they\nhave various limitations. Here we highlight a common failure mode of these\nmodels, namely, generating uncommon concepts and structured concepts like hand\npalms. We show that their limitation is partly due to the long-tail nature of\ntheir training data: web-crawled data sets are strongly unbalanced, causing\nmodels to under-represent concepts from the tail of the distribution. We\ncharacterize the effect of unbalanced training data on text-to-image models and\noffer a remedy. We show that rare concepts can be correctly generated by\ncarefully selecting suitable generation seeds in the noise space, using a small\nreference set of images, a technique that we call SeedSelect. SeedSelect does\nnot require retraining or finetuning the diffusion model. We assess the\nfaithfulness, quality and diversity of SeedSelect in creating rare objects and\ngenerating complex formations like hand images, and find it consistently\nachieves superior performance. We further show the advantage of SeedSelect in\nsemantic data augmentation. Generating semantically appropriate images can\nsuccessfully improve performance in few-shot recognition benchmarks, for\nclasses from the head and from the tail of the training data of diffusion\nmodels",
        "timestamp": "2025-11-06T13:15:17.653Z",
        "rating": "novote",
        "publishedDate": "2023-04-27T20:55:38Z",
        "tags": [
          "cs.CV",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 657,
        "object_id": "paper:arxiv.2304.14530",
        "created_at": "2025-11-06T13:15:18+00:00",
        "updated_at": "2025-11-06T13:15:41+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1906.01327": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1906.01327",
        "url": "https://arxiv.org/abs/1906.01327",
        "title": "How Large Are Lions? Inducing Distributions over Quantitative Attributes",
        "authors": "Yanai Elazar, Abhijit Mahabal, Deepak Ramachandran, Tania Bedrax-Weiss, Dan Roth",
        "abstract": "Most current NLP systems have little knowledge about quantitative attributes of objects and events. We propose an unsupervised method for collecting quantitative information from large amounts of web data, and use it to create a new, very large resource consisting of distributions over physical quantities associated with objects, adjectives, and verbs which we call Distributions over Quantitative (DoQ). This contrasts with recent work in this area which has focused on making only relative comparisons such as \"Is a lion bigger than a wolf?\". Our evaluation shows that DoQ compares favorably with state of the art results on existing datasets for relative comparisons of nouns and adjectives, and on a new dataset we introduce.",
        "timestamp": "2025-11-06T17:20:29.999Z",
        "rating": "novote",
        "publishedDate": "2019/06/04",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 659,
        "object_id": "paper:arxiv.1906.01327",
        "created_at": "2025-11-06T17:20:30+00:00",
        "updated_at": "2025-11-06T17:20:50+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1906.01327": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1906.01327",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-06T17:29:55.534Z",
            "data": {
              "session_id": "session_1762450194846_t4xipb9",
              "source_id": "arxiv",
              "paper_id": "1906.01327",
              "start_time": "2025-11-06T17:20:45.922Z",
              "end_time": "2025-11-06T17:29:54.846Z",
              "heartbeat_count": 109,
              "duration_seconds": 545,
              "idle_seconds": 4,
              "total_elapsed_seconds": 549
            }
          }
        ]
      },
      "meta": {
        "issue_number": 660,
        "object_id": "interactions:arxiv.1906.01327",
        "created_at": "2025-11-06T17:29:56+00:00",
        "updated_at": "2025-11-06T17:30:25+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2310.18362": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.18362",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T08:45:14.500Z",
            "data": {
              "session_id": "session_1762677913820_v9lhqnu",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T08:44:03.467Z",
              "end_time": "2025-11-09T08:45:13.820Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 0,
              "total_elapsed_seconds": 70
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T11:47:34.085Z",
            "data": {
              "session_id": "session_1762688853797_r6om7bn",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T11:47:24.768Z",
              "end_time": "2025-11-09T11:47:33.797Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:01:18.545Z",
            "data": {
              "session_id": "session_1762689678067_gexk19g",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:01:04.596Z",
              "end_time": "2025-11-09T12:01:18.067Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:08:19.795Z",
            "data": {
              "session_id": "session_1762690098866_zg8ohcv",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:07:06.611Z",
              "end_time": "2025-11-09T12:08:18.866Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 2,
              "total_elapsed_seconds": 72
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:09:45.036Z",
            "data": {
              "session_id": "session_1762690184366_qpoauba",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:08:30.238Z",
              "end_time": "2025-11-09T12:09:44.366Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 4,
              "total_elapsed_seconds": 74
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:10:49.602Z",
            "data": {
              "session_id": "session_1762690248377_jurgdxg",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:09:45.446Z",
              "end_time": "2025-11-09T12:10:48.377Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 3,
              "total_elapsed_seconds": 63
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:13:34.426Z",
            "data": {
              "session_id": "session_1762690414411_6qgs8xy",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:13:16.385Z",
              "end_time": "2025-11-09T12:13:34.411Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:14:55.147Z",
            "data": {
              "session_id": "session_1762690494859_96117hf",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:14:38.207Z",
              "end_time": "2025-11-09T12:14:54.859Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:17:34.303Z",
            "data": {
              "session_id": "session_1762690654290_x8ow96u",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:17:26.257Z",
              "end_time": "2025-11-09T12:17:34.290Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:18:43.940Z",
            "data": {
              "session_id": "session_1762690723292_w4ftk8i",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:17:35.537Z",
              "end_time": "2025-11-09T12:18:43.292Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 3,
              "total_elapsed_seconds": 68
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:23:46.406Z",
            "data": {
              "session_id": "session_1762691025725_0ylbcxi",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:22:36.998Z",
              "end_time": "2025-11-09T12:23:45.725Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 4,
              "total_elapsed_seconds": 69
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:24:53.341Z",
            "data": {
              "session_id": "session_1762691092354_9j17zql",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:23:49.077Z",
              "end_time": "2025-11-09T12:24:52.354Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 3,
              "total_elapsed_seconds": 63
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:26:50.277Z",
            "data": {
              "session_id": "session_1762691210265_y2ve729",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:26:32.756Z",
              "end_time": "2025-11-09T12:26:50.265Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:30:23.849Z",
            "data": {
              "session_id": "session_1762691423180_163nisi",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:29:08.800Z",
              "end_time": "2025-11-09T12:30:23.180Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 4,
              "total_elapsed_seconds": 74
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:31:09.624Z",
            "data": {
              "session_id": "session_1762691469607_zh8vxry",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:30:59.222Z",
              "end_time": "2025-11-09T12:31:09.607Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:33:12.646Z",
            "data": {
              "session_id": "session_1762691592353_4rja4uc",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:32:43.159Z",
              "end_time": "2025-11-09T12:33:12.352Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:34:45.786Z",
            "data": {
              "session_id": "session_1762691685762_lpoo15a",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:34:12.176Z",
              "end_time": "2025-11-09T12:34:45.762Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:37:51.547Z",
            "data": {
              "session_id": "session_1762691870453_3m9gfdz",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:34:46.749Z",
              "end_time": "2025-11-09T12:37:50.453Z",
              "heartbeat_count": 36,
              "duration_seconds": 180,
              "idle_seconds": 4,
              "total_elapsed_seconds": 184
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:42:02.424Z",
            "data": {
              "session_id": "session_1762692121727_7q4hlho",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:37:52.132Z",
              "end_time": "2025-11-09T12:42:01.727Z",
              "heartbeat_count": 49,
              "duration_seconds": 245,
              "idle_seconds": 5,
              "total_elapsed_seconds": 250
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:43:55.625Z",
            "data": {
              "session_id": "session_1762692235618_cweh5dn",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:43:47.741Z",
              "end_time": "2025-11-09T12:43:55.618Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:45:17.395Z",
            "data": {
              "session_id": "session_1762692316390_fh707ft",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:44:19.906Z",
              "end_time": "2025-11-09T12:45:16.390Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 1,
              "total_elapsed_seconds": 56
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:50:40.450Z",
            "data": {
              "session_id": "session_1762692639687_2642tyu",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:45:18.642Z",
              "end_time": "2025-11-09T12:50:39.687Z",
              "heartbeat_count": 64,
              "duration_seconds": 320,
              "idle_seconds": 1,
              "total_elapsed_seconds": 321
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:55:07.186Z",
            "data": {
              "session_id": "session_1762692907165_h6nmm56",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T12:54:17.669Z",
              "end_time": "2025-11-09T12:55:07.165Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 4,
              "total_elapsed_seconds": 49
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:02:19.898Z",
            "data": {
              "session_id": "session_1762693339880_4ochixw",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:02:10.135Z",
              "end_time": "2025-11-09T13:02:19.880Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:03:18.021Z",
            "data": {
              "session_id": "session_1762693397276_9rkik4p",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:02:56.429Z",
              "end_time": "2025-11-09T13:03:17.276Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:05:26.885Z",
            "data": {
              "session_id": "session_1762693525799_2ohyp5j",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:03:32.448Z",
              "end_time": "2025-11-09T13:05:25.799Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 3,
              "total_elapsed_seconds": 113
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:06:59.179Z",
            "data": {
              "session_id": "session_1762693617947_25ubwze",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:05:28.200Z",
              "end_time": "2025-11-09T13:06:57.947Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 5,
              "total_elapsed_seconds": 90
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:09:16.886Z",
            "data": {
              "session_id": "session_1762693756623_0qj7hom",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:09:00.530Z",
              "end_time": "2025-11-09T13:09:16.623Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:11:20.482Z",
            "data": {
              "session_id": "session_1762693880183_fsi8rwg",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:10:51.620Z",
              "end_time": "2025-11-09T13:11:20.183Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:14:02.866Z",
            "data": {
              "session_id": "session_1762694042854_r3pjj24",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:13:52.890Z",
              "end_time": "2025-11-09T13:14:02.854Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:15:23.770Z",
            "data": {
              "session_id": "session_1762694122819_xt7ofhp",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:14:40.167Z",
              "end_time": "2025-11-09T13:15:22.819Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 3,
              "total_elapsed_seconds": 43
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:16:32.915Z",
            "data": {
              "session_id": "session_1762694192240_4l3mmsx",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:16:10.544Z",
              "end_time": "2025-11-09T13:16:32.240Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:19:51.470Z",
            "data": {
              "session_id": "session_1762694391057_dpgaop8",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:19:06.876Z",
              "end_time": "2025-11-09T13:19:51.057Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 4,
              "total_elapsed_seconds": 44
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:28:32.736Z",
            "data": {
              "session_id": "session_1762694911667_zfx46vd",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:27:32.574Z",
              "end_time": "2025-11-09T13:28:31.667Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 4,
              "total_elapsed_seconds": 59
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:30:23.673Z",
            "data": {
              "session_id": "session_1762695022980_2delu0f",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:28:32.439Z",
              "end_time": "2025-11-09T13:30:22.980Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 1,
              "total_elapsed_seconds": 111
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:34:19.267Z",
            "data": {
              "session_id": "session_1762695258515_pjm0z69",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:32:33.311Z",
              "end_time": "2025-11-09T13:34:18.514Z",
              "heartbeat_count": 21,
              "duration_seconds": 105,
              "idle_seconds": 0,
              "total_elapsed_seconds": 105
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:38:00.915Z",
            "data": {
              "session_id": "session_1762695480036_bn4o9v1",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:34:58.825Z",
              "end_time": "2025-11-09T13:38:00.036Z",
              "heartbeat_count": 36,
              "duration_seconds": 180,
              "idle_seconds": 1,
              "total_elapsed_seconds": 181
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:39:52.068Z",
            "data": {
              "session_id": "session_1762695592062_67sa511",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:39:46.466Z",
              "end_time": "2025-11-09T13:39:52.062Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:42:22.925Z",
            "data": {
              "session_id": "session_1762695742249_ccj8qzc",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:40:55.066Z",
              "end_time": "2025-11-09T13:42:22.249Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 2,
              "total_elapsed_seconds": 87
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:45:16.378Z",
            "data": {
              "session_id": "session_1762695916374_lvsyoq0",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:45:09.074Z",
              "end_time": "2025-11-09T13:45:16.374Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T13:51:10.710Z",
            "data": {
              "session_id": "session_1762696270094_pphfa38",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T13:50:54.225Z",
              "end_time": "2025-11-09T13:51:10.094Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T14:53:14.338Z",
            "data": {
              "session_id": "session_1762699993756_tk7hn0t",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T14:53:04.074Z",
              "end_time": "2025-11-09T14:53:13.756Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T20:45:37.571Z",
            "data": {
              "session_id": "session_1762721137565_1sdzuv4",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-09T20:45:32.302Z",
              "end_time": "2025-11-09T20:45:37.565Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T11:26:55.322Z",
            "data": {
              "session_id": "session_1764070015234_uiwgcqe",
              "source_id": "arxiv",
              "paper_id": "2310.18362",
              "start_time": "2025-11-25T11:26:47.824Z",
              "end_time": "2025-11-25T11:26:55.234Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 662,
        "object_id": "interactions:arxiv.2310.18362",
        "created_at": "2025-11-07T06:15:09+00:00",
        "updated_at": "2025-11-25T11:27:35+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.18362": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.18362",
        "url": "https://arxiv.org/pdf/2310.18362",
        "title": "SoK: Memorization in General-Purpose Large Language Models",
        "authors": "Valentin Hartmann, Anshuman Suri, Vincent Bindschaedler, David Evans, Shruti Tople, Robert West",
        "abstract": "Large Language Models (LLMs) are advancing at a remarkable pace, with myriad\napplications under development. Unlike most earlier machine learning models,\nthey are no longer built for one specific application but are designed to excel\nin a wide range of tasks. A major part of this success is due to their huge\ntraining datasets and the unprecedented number of model parameters, which allow\nthem to memorize large amounts of information contained in the training data.\nThis memorization goes beyond mere language, and encompasses information only\npresent in a few documents. This is often desirable since it is necessary for\nperforming tasks such as question answering, and therefore an important part of\nlearning, but also brings a whole array of issues, from privacy and security to\ncopyright and beyond. LLMs can memorize short secrets in the training data, but\ncan also memorize concepts like facts or writing styles that can be expressed\nin text in many different ways. We propose a taxonomy for memorization in LLMs\nthat covers verbatim text, facts, ideas and algorithms, writing styles,\ndistributional properties, and alignment goals. We describe the implications of\neach type of memorization - both positive and negative - for model performance,\nprivacy, security and confidentiality, copyright, and auditing, and ways to\ndetect and prevent memorization. We further highlight the challenges that arise\nfrom the predominant way of defining memorization with respect to model\nbehavior instead of model weights, due to LLM-specific phenomena such as\nreasoning capabilities or differences between decoding algorithms. Throughout\nthe paper, we describe potential risks and opportunities arising from\nmemorization in LLMs that we hope will motivate new research directions.",
        "timestamp": "2025-11-07T06:14:28.170Z",
        "rating": "novote",
        "publishedDate": "2023-10-24T14:25:53Z",
        "tags": [
          "cs.CL",
          "cs.CR",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 661,
        "object_id": "paper:arxiv.2310.18362",
        "created_at": "2025-11-07T06:14:28+00:00",
        "updated_at": "2025-11-07T06:14:49+00:00",
        "version": 1
      }
    },
    "paper:openreview.qEDsgiA7gp": {
      "data": {
        "sourceId": "openreview",
        "paperId": "qEDsgiA7gp",
        "url": "https://openreview.net/forum?id=qEDsgiA7gp&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Don\u2019t lie to your friends: Learning what you know from collaborative self-play",
        "authors": "Jonathan Berant, Reza Aghajani, Jacob Eisenstein, Adam Fisch, Dheeru Dua, Fantine Eri Huot, Mirella Lapata, Vicky Zayats",
        "abstract": "To be helpful assistants, AI agents must be aware of their own capabilities and limitations. This includes knowing when to answer from parametric knowledge versus using tools, when to trust tool outputs, and when to abstain or hedge. Such capabilities are hard to teach through supervised fine-tuning because they require constructing examples that reflect the agent's specific capabilities. We therefore propose a radically new approach to teaching agents what they know: \\emph{collaborative self-play}. We construct multi-agent collaborations in which the group is rewarded for collectively arriving at correct answers. The desired meta-knowledge emerges from the incentives built into the structure of the interaction. We focus on small societies of agents that have access to heterogeneous tools (corpus-specific retrieval), and therefore must collaborate to maximize their success with minimal effort. Experiments show that group-level rewards for multi-agent communities can induce policies that \\emph{transfer} to improve tool use and selective prediction in single-agent scenarios.",
        "timestamp": "2025-11-07T07:22:20.296Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "self-play",
          "calibration",
          "collaboration"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 663,
        "object_id": "paper:openreview.qEDsgiA7gp",
        "created_at": "2025-11-07T07:22:20+00:00",
        "updated_at": "2025-11-07T07:22:39+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.26202": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.26202",
        "url": "https://arxiv.org/abs/2510.26202",
        "title": "What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data",
        "authors": "Rajiv Movva, Smitha Milli, Sewon Min, Emma Pierson",
        "abstract": "Human feedback can alter language models in unpredictable and undesirable ways, as practitioners lack a clear understanding of what feedback data encodes. While prior work studies preferences over certain attributes (e.g., length or sycophancy), automatically extracting relevant features without pre-specifying hypotheses remains challenging. We introduce What's In My Human Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders. WIMHF characterizes both (1) the preferences a dataset is capable of measuring and (2) the preferences that the annotators actually express. Across 7 datasets, WIMHF identifies a small number of human-interpretable features that account for the majority of the preference prediction signal achieved by black-box models. These features reveal a wide diversity in what humans prefer, and the role of dataset-level context: for example, users on Reddit prefer informality and jokes, while annotators in HH-RLHF and PRISM disprefer them. WIMHF also surfaces potentially unsafe preferences, such as that LMArena users tend to vote against refusals, often in favor of toxic content. The learned features enable effective data curation: re-labeling the harmful examples in Arena yields large safety gains (+37%) with no cost to general performance. They also allow fine-grained personalization: on the Community Alignment dataset, we learn annotator-specific weights over subjective features that improve preference prediction. WIMHF provides a human-centered analysis method for practitioners to better understand and use preference data.",
        "timestamp": "2025-11-07T07:53:18.556Z",
        "rating": "novote",
        "publishedDate": "2025/10/30",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 664,
        "object_id": "paper:arxiv.2510.26202",
        "created_at": "2025-11-07T07:53:19+00:00",
        "updated_at": "2025-11-07T07:53:41+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.26202": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.26202",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-07T07:55:18.397Z",
            "data": {
              "session_id": "session_1762502117802_7zz5rkm",
              "source_id": "arxiv",
              "paper_id": "2510.26202",
              "start_time": "2025-11-07T07:53:28.649Z",
              "end_time": "2025-11-07T07:55:17.802Z",
              "heartbeat_count": 21,
              "duration_seconds": 105,
              "idle_seconds": 4,
              "total_elapsed_seconds": 109
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-07T08:01:42.261Z",
            "data": {
              "session_id": "session_1762502501706_reiv0ue",
              "source_id": "arxiv",
              "paper_id": "2510.26202",
              "start_time": "2025-11-07T07:59:19.637Z",
              "end_time": "2025-11-07T08:01:41.706Z",
              "heartbeat_count": 28,
              "duration_seconds": 140,
              "idle_seconds": 2,
              "total_elapsed_seconds": 142
            }
          }
        ]
      },
      "meta": {
        "issue_number": 665,
        "object_id": "interactions:arxiv.2510.26202",
        "created_at": "2025-11-07T07:55:19+00:00",
        "updated_at": "2025-11-07T08:02:14+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2511.01836": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.01836",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-07T10:34:14.458Z",
            "data": {
              "session_id": "session_1762511653907_wm0bbdp",
              "source_id": "arxiv",
              "paper_id": "2511.01836",
              "start_time": "2025-11-07T10:33:12.687Z",
              "end_time": "2025-11-07T10:34:13.907Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 1,
              "total_elapsed_seconds": 61
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-08T14:09:09.199Z",
            "data": {
              "session_id": "session_1762610948673_ukuq5bc",
              "source_id": "arxiv",
              "paper_id": "2511.01836",
              "start_time": "2025-11-08T14:03:03.464Z",
              "end_time": "2025-11-08T14:09:08.673Z",
              "heartbeat_count": 73,
              "duration_seconds": 365,
              "idle_seconds": 0,
              "total_elapsed_seconds": 365
            }
          }
        ]
      },
      "meta": {
        "issue_number": 667,
        "object_id": "interactions:arxiv.2511.01836",
        "created_at": "2025-11-07T10:33:13+00:00",
        "updated_at": "2025-11-08T14:09:33+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2511.01836": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.01836",
        "url": "https://arxiv.org/pdf/2511.01836",
        "title": "Priors in Time: Missing Inductive Biases for Language Model\n  Interpretability",
        "authors": "Ekdeep Singh Lubana, Can Rager, Sai Sumedh R. Hindupur, Valerie Costa, Greta Tuckute, Oam Patel, Sonia Krishna Murthy, Thomas Fel, Daniel Wurgaft, Eric J. Bigelow, Johnny Lin, Demba Ba, Martin Wattenberg, Fernanda Viegas, Melanie Weber, Aaron Mueller",
        "abstract": "Recovering meaningful concepts from language model activations is a central\naim of interpretability. While existing feature extraction methods aim to\nidentify concepts that are independent directions, it is unclear if this\nassumption can capture the rich temporal structure of language. Specifically,\nvia a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose\npriors that assume independence of concepts across time, implying stationarity.\nMeanwhile, language model representations exhibit rich temporal dynamics,\nincluding systematic growth in conceptual dimensionality, context-dependent\ncorrelations, and pronounced non-stationarity, in direct conflict with the\npriors of SAEs. Taking inspiration from computational neuroscience, we\nintroduce a new interpretability objective -- Temporal Feature Analysis --\nwhich possesses a temporal inductive bias to decompose representations at a\ngiven time into two parts: a predictable component, which can be inferred from\nthe context, and a residual component, which captures novel information\nunexplained by the context. Temporal Feature Analyzers correctly parse garden\npath sentences, identify event boundaries, and more broadly delineate abstract,\nslow-moving information from novel, fast-moving information, while existing\nSAEs show significant pitfalls in all the above tasks. Overall, our results\nunderscore the need for inductive biases that match the data in designing\nrobust interpretability tools.",
        "timestamp": "2025-11-07T10:32:54.369Z",
        "rating": "novote",
        "publishedDate": "2025-11-03T18:43:48Z",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 666,
        "object_id": "paper:arxiv.2511.01836",
        "created_at": "2025-11-07T10:32:54+00:00",
        "updated_at": "2025-11-07T10:33:18+00:00",
        "version": 1
      }
    },
    "paper:openreview.wsFMgZr1wG": {
      "data": {
        "sourceId": "openreview",
        "paperId": "wsFMgZr1wG",
        "url": "https://openreview.net/forum?id=wsFMgZr1wG&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "User-Centric Evidence Ranking for Attribution and Fact Verification",
        "authors": "Guy Alt, Eran Hirsch, Oren Glickman, Serwar Basch, Ido Dagan",
        "abstract": "Attribution and fact verification are critical challenges in natural language processing for assessing information reliability. While automated systems and Large Language Models (LLMs) aim to retrieve and select concise evidence to support or refute claims, current inaccuracies present users with an overwhelming volume of text, leading to inefficient and error-prone verification. To address this, we propose the User-Centric Evidence Ranking, a novel task that prioritizes presenting sufficient information as early as possible in a ranked list. This minimizes user reading effort while still making all available evidence accessible for sequential verification. We introduce a novel evaluation framework, inspired by information retrieval metrics, and construct a unified benchmark by aggregating existing fact verification datasets. Our extensive experiments with diverse models reveal that incremental ranking strategies more effectively capture complementary evidence. Additionally, LLM-based methods outperform shallower baselines, but they still face challenges in optimally balancing sufficiency with redundancy avoidance. This work establishes a foundational step toward developing more interpretable, efficient, and user aligned information verification systems",
        "timestamp": "2025-11-07T14:10:30.211Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Fact Checking",
          "Attribution",
          "Information Retrieval and Text Mining"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 668,
        "object_id": "paper:openreview.wsFMgZr1wG",
        "created_at": "2025-11-07T14:10:30+00:00",
        "updated_at": "2025-11-07T14:10:53+00:00",
        "version": 1
      }
    },
    "paper:openreview.BReW": {
      "data": {
        "sourceId": "openreview",
        "paperId": "BReW",
        "url": "https://openreview.net/forum?id=BReW_Yxq_kWe&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3Daclweb.org%2FACL%2F2026%2FWorkshop%2FBigPicture%2FProgram_Chairs)",
        "title": "The Big Picture v2: Crafting a Research Narrative",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-08T09:12:13.013Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 669,
        "object_id": "paper:openreview.BReW",
        "created_at": "2025-11-08T09:12:13+00:00",
        "updated_at": "2025-11-08T09:12:38+00:00",
        "version": 1
      }
    },
    "interactions:openreview.BReW": {
      "data": {
        "sourceId": "openreview",
        "paperId": "BReW",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-08T09:13:20.941Z",
            "data": {
              "session_id": "session_1762593200425_g8wdt51",
              "source_id": "openreview",
              "paper_id": "BReW",
              "start_time": "2025-11-08T09:12:36.621Z",
              "end_time": "2025-11-08T09:13:20.425Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 4,
              "total_elapsed_seconds": 44
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-08T09:15:11.772Z",
            "data": {
              "session_id": "session_1762593311748_htvigdr",
              "source_id": "openreview",
              "paper_id": "BReW",
              "start_time": "2025-11-08T09:14:21.712Z",
              "end_time": "2025-11-08T09:15:11.748Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 0,
              "total_elapsed_seconds": 50
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-08T09:16:16.234Z",
            "data": {
              "session_id": "session_1762593376222_1vjsjam",
              "source_id": "openreview",
              "paper_id": "BReW",
              "start_time": "2025-11-08T09:16:08.647Z",
              "end_time": "2025-11-08T09:16:16.222Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-08T09:21:38.177Z",
            "data": {
              "session_id": "session_1762593698139_qd0t5ub",
              "source_id": "openreview",
              "paper_id": "BReW",
              "start_time": "2025-11-08T09:21:29.837Z",
              "end_time": "2025-11-08T09:21:38.139Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-08T09:22:30.732Z",
            "data": {
              "session_id": "session_1762593750690_ucu90l3",
              "source_id": "openreview",
              "paper_id": "BReW",
              "start_time": "2025-11-08T09:22:10.339Z",
              "end_time": "2025-11-08T09:22:30.690Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-08T09:24:25.593Z",
            "data": {
              "session_id": "session_1762593865239_f5nubfn",
              "source_id": "openreview",
              "paper_id": "BReW",
              "start_time": "2025-11-08T09:23:40.282Z",
              "end_time": "2025-11-08T09:24:25.239Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 5,
              "total_elapsed_seconds": 45
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-08T09:25:42.101Z",
            "data": {
              "session_id": "session_1762593941585_fhs2rag",
              "source_id": "openreview",
              "paper_id": "BReW",
              "start_time": "2025-11-08T09:24:41.333Z",
              "end_time": "2025-11-08T09:25:41.585Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 0,
              "total_elapsed_seconds": 60
            }
          }
        ]
      },
      "meta": {
        "issue_number": 670,
        "object_id": "interactions:openreview.BReW",
        "created_at": "2025-11-08T09:13:21+00:00",
        "updated_at": "2025-11-08T09:26:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2109.06024": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2109.06024",
        "url": "https://arxiv.org/pdf/2109.06024",
        "title": "Formalizing and Estimating Distribution Inference Risks",
        "authors": "Anshuman Suri, David Evans",
        "abstract": "Distribution inference, sometimes called property inference, infers\nstatistical properties about a training set from access to a model trained on\nthat data. Distribution inference attacks can pose serious risks when models\nare trained on private data, but are difficult to distinguish from the\nintrinsic purpose of statistical machine learning -- namely, to produce models\nthat capture statistical properties about a distribution. Motivated by Yeom et\nal.'s membership inference framework, we propose a formal definition of\ndistribution inference attacks that is general enough to describe a broad class\nof attacks distinguishing between possible training distributions. We show how\nour definition captures previous ratio-based property inference attacks as well\nas new kinds of attack including revealing the average node degree or\nclustering coefficient of a training graph. To understand distribution\ninference risks, we introduce a metric that quantifies observed leakage by\nrelating it to the leakage that would occur if samples from the training\ndistribution were provided directly to the adversary. We report on a series of\nexperiments across a range of different distributions using both novel\nblack-box attacks and improved versions of the state-of-the-art white-box\nattacks. Our results show that inexpensive attacks are often as effective as\nexpensive meta-classifier attacks, and that there are surprising asymmetries in\nthe effectiveness of attacks. Code is available at\nhttps://github.com/iamgroot42/FormEstDistRisks",
        "timestamp": "2025-11-09T12:31:13.436Z",
        "rating": "novote",
        "publishedDate": "2021-09-13T14:54:39Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CR"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 673,
        "object_id": "paper:arxiv.2109.06024",
        "created_at": "2025-11-09T12:31:13+00:00",
        "updated_at": "2025-11-09T12:31:35+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1306.4447": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1306.4447",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:30:42.460Z",
            "data": {
              "session_id": "session_1762691441878_y14qxz1",
              "source_id": "arxiv",
              "paper_id": "1306.4447",
              "start_time": "2025-11-09T12:30:27.736Z",
              "end_time": "2025-11-09T12:30:41.878Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-10T05:17:00.391Z",
            "data": {
              "session_id": "session_1762751819828_jnexz3q",
              "source_id": "arxiv",
              "paper_id": "1306.4447",
              "start_time": "2025-11-10T05:13:00.749Z",
              "end_time": "2025-11-10T05:16:59.828Z",
              "heartbeat_count": 47,
              "duration_seconds": 235,
              "idle_seconds": 4,
              "total_elapsed_seconds": 239
            }
          }
        ]
      },
      "meta": {
        "issue_number": 672,
        "object_id": "interactions:arxiv.1306.4447",
        "created_at": "2025-11-09T12:30:43+00:00",
        "updated_at": "2025-11-10T05:17:23+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1306.4447": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1306.4447",
        "url": "https://arxiv.org/pdf/1306.4447",
        "title": "Hacking Smart Machines with Smarter Ones: How to Extract Meaningful Data\n  from Machine Learning Classifiers",
        "authors": "Giuseppe Ateniese, Giovanni Felici, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali",
        "abstract": "Machine Learning (ML) algorithms are used to train computers to perform a\nvariety of complex tasks and improve with experience. Computers learn how to\nrecognize patterns, make unintended decisions, or react to a dynamic\nenvironment. Certain trained machines may be more effective than others because\nthey are based on more suitable ML algorithms or because they were trained\nthrough superior training sets. Although ML algorithms are known and publicly\nreleased, training sets may not be reasonably ascertainable and, indeed, may be\nguarded as trade secrets. While much research has been performed about the\nprivacy of the elements of training sets, in this paper we focus our attention\non ML classifiers and on the statistical information that can be unconsciously\nor maliciously revealed from them. We show that it is possible to infer\nunexpected but useful information from ML classifiers. In particular, we build\na novel meta-classifier and train it to hack other classifiers, obtaining\nmeaningful information about their training sets. This kind of information\nleakage can be exploited, for example, by a vendor to build more effective\nclassifiers or to simply acquire trade secrets from a competitor's apparatus,\npotentially violating its intellectual property rights.",
        "timestamp": "2025-11-09T12:30:27.989Z",
        "rating": "novote",
        "publishedDate": "2013-06-19T07:51:49Z",
        "tags": [
          "cs.CR",
          "cs.LG",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 671,
        "object_id": "paper:arxiv.1306.4447",
        "created_at": "2025-11-09T12:30:28+00:00",
        "updated_at": "2025-11-09T12:30:54+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2109.06024": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2109.06024",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-09T12:31:57.634Z",
            "data": {
              "session_id": "session_1762691517622_giiysgi",
              "source_id": "arxiv",
              "paper_id": "2109.06024",
              "start_time": "2025-11-09T12:31:16.707Z",
              "end_time": "2025-11-09T12:31:57.622Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 1,
              "total_elapsed_seconds": 41
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-11T16:49:52.033Z",
            "data": {
              "session_id": "session_1762879791687_veesa46",
              "source_id": "arxiv",
              "paper_id": "2109.06024",
              "start_time": "2025-11-11T16:49:21.976Z",
              "end_time": "2025-11-11T16:49:51.687Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 5,
              "total_elapsed_seconds": 30
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T11:17:56.469Z",
            "data": {
              "session_id": "session_1764069476449_vakfbp3",
              "source_id": "arxiv",
              "paper_id": "2109.06024",
              "start_time": "2025-11-25T11:17:50.524Z",
              "end_time": "2025-11-25T11:17:56.449Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 674,
        "object_id": "interactions:arxiv.2109.06024",
        "created_at": "2025-11-09T12:31:59+00:00",
        "updated_at": "2025-11-25T11:18:19+00:00",
        "version": 1
      }
    },
    "paper:openreview.FGTDe6EA0B": {
      "data": {
        "sourceId": "openreview",
        "paperId": "FGTDe6EA0B",
        "url": "https://openreview.net/forum?id=FGTDe6EA0B",
        "title": "Language Generation in the Limit",
        "authors": "Jon Kleinberg, Sendhil Mullainathan",
        "abstract": "Although current large language models are complex, the most basic specifications of the underlying language generation problem itself are simple to state: given a finite set of training samples from an unknown language, produce valid new strings from the language that don't already appear in the training data. Here we ask what we can conclude about language generation using only this specification, without further assumptions. In particular, suppose that an adversary enumerates the strings of an unknown target language L that is known only to come from one of a possibly infinite list of candidates. A computational agent is trying to learn to generate from this language; we say that the agent generates from $L$ in the limit if after some finite point in the enumeration of $L$, the agent is able to produce new elements that come exclusively from $L$ and that have not yet been presented by the adversary. Our main result is that there is an agent that is able to generate in the limit for every countable list of candidate languages. This contrasts dramatically with negative results due to Gold and Angluin in a well-studied model of language learning where the goal is to identify an unknown language from samples; the difference between these results suggests that identifying a language is a fundamentally different problem than generating from it.",
        "timestamp": "2025-11-11T05:56:06.160Z",
        "rating": "novote",
        "publishedDate": "25 Sept 2024",
        "tags": [
          "language generation",
          "large language models",
          "enumeration"
        ],
        "doi": "",
        "journalName": "NeurIPS 2024 spotlight",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 675,
        "object_id": "paper:openreview.FGTDe6EA0B",
        "created_at": "2025-11-11T05:56:06+00:00",
        "updated_at": "2025-11-11T05:56:27+00:00",
        "version": 1
      }
    },
    "interactions:openreview.ejaY9KJZuG": {
      "data": {
        "sourceId": "openreview",
        "paperId": "ejaY9KJZuG",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-11T12:29:56.305Z",
            "data": {
              "session_id": "session_1762864195739_cunbe1s",
              "source_id": "openreview",
              "paper_id": "ejaY9KJZuG",
              "start_time": "2025-11-11T12:29:48.109Z",
              "end_time": "2025-11-11T12:29:55.739Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 677,
        "object_id": "interactions:openreview.ejaY9KJZuG",
        "created_at": "2025-11-11T12:29:57+00:00",
        "updated_at": "2025-11-11T12:30:22+00:00",
        "version": 1
      }
    },
    "paper:openreview.ejaY9KJZuG": {
      "data": {
        "sourceId": "openreview",
        "paperId": "ejaY9KJZuG",
        "url": "https://openreview.net/forum?id=ejaY9KJZuG&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance",
        "authors": "Omer Nahum, Nitay Calderon, Orgad Keller, Idan Szpektor, Roi Reichart",
        "abstract": "NLP benchmarks rely on standardized datasets for training and evaluating models and are crucial for advancing the field. Traditionally, expert annotations ensure high-quality labels; however, the cost of expert annotation does not scale well with the growing demand for larger datasets required by modern models. While crowd-sourcing provides a more scalable solution, it often comes at the expense of annotation precision and consistency. Recent advancements in large language models (LLMs) offer new opportunities to enhance the annotation process, particularly for detecting label errors in existing datasets. In this work, we consider the recent approach of LLM-as-a-judge, leveraging an ensemble of LLMs to flag potentially mislabeled examples. We conduct a case study on four factual consistency datasets from the TRUE benchmark, spanning diverse NLP tasks, and on SummEval, which uses Likert-scale ratings of summary quality across multiple dimensions. We empirically analyze the labeling quality of existing datasets and compare expert, crowd-sourced, and LLM-based annotations in terms of the agreement, label quality, and efficiency, demonstrating the strengths and limitations of each annotation method. Our findings reveal a substantial number of label errors, which, when corrected, induce a significant upward shift in reported model performance. This suggests that many of the LLMs' so-called mistakes are due to label errors rather than genuine model failures. Additionally, we discuss the implications of mislabeled data and propose methods to mitigate them in training to improve performance.\n\n(Accepted to EMNLP 2025)",
        "timestamp": "2025-11-11T12:29:49.023Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "LLMs",
          "label errors detection",
          "label errors handling",
          "data annotation",
          "NLP datasets",
          "automatic evaluation of datasets",
          "factuality"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 676,
        "object_id": "paper:openreview.ejaY9KJZuG",
        "created_at": "2025-11-11T12:29:49+00:00",
        "updated_at": "2025-11-11T12:30:14+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.20208": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.20208",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-11T16:34:11.923Z",
            "data": {
              "session_id": "session_1762878851395_w7cjcdj",
              "source_id": "arxiv",
              "paper_id": "2507.20208",
              "start_time": "2025-11-11T16:33:55.562Z",
              "end_time": "2025-11-11T16:34:11.395Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-11T16:45:29.406Z",
            "data": {
              "session_id": "session_1762879529395_mm3meqb",
              "source_id": "arxiv",
              "paper_id": "2507.20208",
              "start_time": "2025-11-11T16:45:05.680Z",
              "end_time": "2025-11-11T16:45:29.395Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-11T21:23:20.312Z",
            "data": {
              "session_id": "session_1762896200044_qwrj7aj",
              "source_id": "arxiv",
              "paper_id": "2507.20208",
              "start_time": "2025-11-11T21:23:09.431Z",
              "end_time": "2025-11-11T21:23:20.044Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-11T21:24:38.022Z",
            "data": {
              "session_id": "session_1762896277741_cecfq4t",
              "source_id": "arxiv",
              "paper_id": "2507.20208",
              "start_time": "2025-11-11T21:24:27.344Z",
              "end_time": "2025-11-11T21:24:37.741Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "issue_number": 681,
        "object_id": "interactions:arxiv.2507.20208",
        "created_at": "2025-11-11T16:34:12+00:00",
        "updated_at": "2025-11-11T21:24:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.20208": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.20208",
        "url": "https://arxiv.org/pdf/2507.20208?",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-11T16:33:55.841Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 680,
        "object_id": "paper:arxiv.2507.20208",
        "created_at": "2025-11-11T16:33:56+00:00",
        "updated_at": "2025-11-11T16:34:20+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.11106": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.11106",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-11T16:33:42.397Z",
            "data": {
              "session_id": "session_1762878821870_3nwvfdo",
              "source_id": "arxiv",
              "paper_id": "2509.11106",
              "start_time": "2025-11-11T16:33:09.709Z",
              "end_time": "2025-11-11T16:33:41.870Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "issue_number": 679,
        "object_id": "interactions:arxiv.2509.11106",
        "created_at": "2025-11-11T16:33:43+00:00",
        "updated_at": "2025-11-11T16:34:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.11106": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.11106",
        "url": "https://arxiv.org/pdf/2509.11106?",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-11T16:33:09.977Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 678,
        "object_id": "paper:arxiv.2509.11106",
        "created_at": "2025-11-11T16:33:10+00:00",
        "updated_at": "2025-11-11T16:33:35+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.16227": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.16227",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T07:02:02.250Z",
            "data": {
              "session_id": "session_1762930921951_my87fnu",
              "source_id": "arxiv",
              "paper_id": "2510.16227",
              "start_time": "2025-11-12T07:01:47.094Z",
              "end_time": "2025-11-12T07:02:01.951Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 683,
        "object_id": "interactions:arxiv.2510.16227",
        "created_at": "2025-11-12T07:02:02+00:00",
        "updated_at": "2025-11-12T07:02:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.16227": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.16227",
        "url": "https://arxiv.org/abs/2510.16227",
        "title": "What Can String Probability Tell Us About Grammaticality?",
        "authors": "Jennifer Hu, Ethan Gotlieb Wilcox, Siyuan Song, Kyle Mahowald, Roger P. Levy",
        "abstract": "What have language models (LMs) learned about grammar? This question remains hotly debated, with major ramifications for linguistic theory. However, since probability and grammaticality are distinct notions in linguistics, it is not obvious what string probabilities can reveal about an LM's underlying grammatical knowledge. We present a theoretical analysis of the relationship between grammar, meaning, and string probability, based on simple assumptions about the generative process of corpus data. Our framework makes three predictions, which we validate empirically using 280K sentence pairs in English and Chinese: (1) correlation between the probability of strings within minimal pairs, i.e., string pairs with minimal semantic differences; (2) correlation between models' and humans' deltas within minimal pairs; and (3) poor separation in probability space between unpaired grammatical and ungrammatical strings. Our analyses give theoretical grounding for using probability to learn about LMs' structural knowledge, and suggest directions for future work in LM grammatical evaluation.",
        "timestamp": "2025-11-12T07:01:43.834Z",
        "rating": "novote",
        "publishedDate": "2025/10/17",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 682,
        "object_id": "paper:arxiv.2510.16227",
        "created_at": "2025-11-12T07:01:44+00:00",
        "updated_at": "2025-11-12T07:02:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.14016": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.14016",
        "url": "https://arxiv.org/pdf/2402.14016",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-12T07:27:06.313Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 689,
        "object_id": "paper:arxiv.2402.14016",
        "created_at": "2025-11-12T07:27:06+00:00",
        "updated_at": "2025-11-12T07:27:28+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2405.05894": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.05894",
        "url": "https://arxiv.org/pdf/2405.05894?",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-12T07:26:47.395Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 688,
        "object_id": "paper:arxiv.2405.05894",
        "created_at": "2025-11-12T07:26:47+00:00",
        "updated_at": "2025-11-12T07:27:08+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2405.13684": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2405.13684",
        "url": "https://arxiv.org/pdf/2405.13684?",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-12T07:26:46.859Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 685,
        "object_id": "paper:arxiv.2405.13684",
        "created_at": "2025-11-12T07:26:47+00:00",
        "updated_at": "2025-11-12T07:27:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2309.07606": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2309.07606",
        "url": "https://arxiv.org/pdf/2309.07606",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-12T07:26:45.391Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 684,
        "object_id": "paper:arxiv.2309.07606",
        "created_at": "2025-11-12T07:26:45+00:00",
        "updated_at": "2025-11-12T07:27:07+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2207.07051": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2207.07051",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T14:36:44.190Z",
            "data": {
              "session_id": "session_1762958203613_5jrahad",
              "source_id": "arxiv",
              "paper_id": "2207.07051",
              "start_time": "2025-11-12T14:36:16.497Z",
              "end_time": "2025-11-12T14:36:43.613Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T15:00:05.194Z",
            "data": {
              "session_id": "session_1762959605173_wyp5fev",
              "source_id": "arxiv",
              "paper_id": "2207.07051",
              "start_time": "2025-11-12T14:59:41.606Z",
              "end_time": "2025-11-12T15:00:05.173Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "issue_number": 691,
        "object_id": "interactions:arxiv.2207.07051",
        "created_at": "2025-11-12T14:36:45+00:00",
        "updated_at": "2025-11-12T15:00:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2207.07051": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2207.07051",
        "url": "https://arxiv.org/pdf/2207.07051",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-12T14:36:17.077Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 690,
        "object_id": "paper:arxiv.2207.07051",
        "created_at": "2025-11-12T14:36:17+00:00",
        "updated_at": "2025-11-12T14:36:39+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.02737": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.02737",
        "url": "https://arxiv.org/abs/2502.02737",
        "title": "SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model",
        "authors": "Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Mart\u00edn Bl\u00e1zquez, Guilherme Penedo, Lewis Tunstall, Andr\u00e9s Marafioti, Hynek Kydl\u00ed\u010dek, Agust\u00edn Piqueres Lajar\u00edn, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Cl\u00e9mentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, Thomas Wolf",
        "abstract": "While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, a state-of-the-art \"small\" (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations as well as a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as well as all of the datasets we prepared in the course of this project.",
        "timestamp": "2025-11-12T14:47:46.209Z",
        "rating": "novote",
        "publishedDate": "2025/02/04",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 692,
        "object_id": "paper:arxiv.2502.02737",
        "created_at": "2025-11-12T14:47:46+00:00",
        "updated_at": "2025-11-12T14:48:09+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.02737": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.02737",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T14:49:09.951Z",
            "data": {
              "session_id": "session_1762958949901_nfmm0je",
              "source_id": "arxiv",
              "paper_id": "2502.02737",
              "start_time": "2025-11-12T14:48:53.829Z",
              "end_time": "2025-11-12T14:49:09.901Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "issue_number": 693,
        "object_id": "interactions:arxiv.2502.02737",
        "created_at": "2025-11-12T14:48:52+00:00",
        "updated_at": "2025-11-12T14:49:16+00:00",
        "version": 1
      }
    },
    "paper:openreview.D9bLUj7wUW": {
      "data": {
        "sourceId": "openreview",
        "paperId": "D9bLUj7wUW",
        "url": "https://openreview.net/pdf?id=D9bLUj7wUW",
        "title": "D9bLUj7wUW",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-12T16:04:33.114Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 694,
        "object_id": "paper:openreview.D9bLUj7wUW",
        "created_at": "2025-11-12T16:04:33+00:00",
        "updated_at": "2025-11-12T16:04:59+00:00",
        "version": 1
      }
    },
    "interactions:openreview.D9bLUj7wUW": {
      "data": {
        "sourceId": "openreview",
        "paperId": "D9bLUj7wUW",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T16:18:34.688Z",
            "data": {
              "session_id": "session_1762964314679_vbp9ccb",
              "source_id": "openreview",
              "paper_id": "D9bLUj7wUW",
              "start_time": "2025-11-12T16:18:02.516Z",
              "end_time": "2025-11-12T16:18:34.679Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "issue_number": 695,
        "object_id": "interactions:openreview.D9bLUj7wUW",
        "created_at": "2025-11-12T16:17:48+00:00",
        "updated_at": "2025-11-12T16:18:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2511.07689": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.07689",
        "url": "https://arxiv.org/abs/2511.07689",
        "title": "Stress Testing Factual Consistency Metrics for Long-Document Summarization",
        "authors": "Zain Muhammad Mujahid, Dustin Wright, Isabelle Augenstein",
        "abstract": "Evaluating the factual consistency of abstractive text summarization remains a significant challenge, particularly for long documents, where conventional metrics struggle with input length limitations and long-range dependencies. In this work, we systematically evaluate the reliability of six widely used reference-free factuality metrics, originally proposed for short-form summarization, in the long-document setting. We probe metric robustness through seven factuality-preserving perturbations applied to summaries, namely paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion, and further analyze their sensitivity to retrieval context and claim information density. Across three long-form benchmark datasets spanning science fiction, legal, and scientific domains, our results reveal that existing short-form metrics produce inconsistent scores for semantically equivalent summaries and exhibit declining reliability for information-dense claims whose content is semantically similar to many parts of the source document. While expanding the retrieval context improves stability in some domains, no metric consistently maintains factual alignment under long-context conditions. Finally, our results highlight concrete directions for improving factuality evaluation, including multi-span reasoning, context-aware calibration, and training on meaning-preserving variations to enhance robustness in long-form summarization. We release all code, perturbed data, and scripts required to reproduce our results at this https URL.",
        "timestamp": "2025-11-12T20:43:42.094Z",
        "rating": "novote",
        "publishedDate": "2025/11/10",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 696,
        "object_id": "paper:arxiv.2511.07689",
        "created_at": "2025-11-12T20:43:42+00:00",
        "updated_at": "2025-11-12T20:44:05+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2511.07689": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.07689",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T20:46:35.964Z",
            "data": {
              "session_id": "session_1762980395394_t1i0qpi",
              "source_id": "arxiv",
              "paper_id": "2511.07689",
              "start_time": "2025-11-12T20:43:45.569Z",
              "end_time": "2025-11-12T20:46:35.394Z",
              "heartbeat_count": 33,
              "duration_seconds": 165,
              "idle_seconds": 5,
              "total_elapsed_seconds": 170
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T21:10:34.947Z",
            "data": {
              "session_id": "session_1762981834918_xryxhe3",
              "source_id": "arxiv",
              "paper_id": "2511.07689",
              "start_time": "2025-11-12T21:10:25.271Z",
              "end_time": "2025-11-12T21:10:34.918Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-12T21:16:23.212Z",
            "data": {
              "session_id": "session_1762982182333_kofu86c",
              "source_id": "arxiv",
              "paper_id": "2511.07689",
              "start_time": "2025-11-12T21:10:38.029Z",
              "end_time": "2025-11-12T21:16:22.333Z",
              "heartbeat_count": 68,
              "duration_seconds": 340,
              "idle_seconds": 4,
              "total_elapsed_seconds": 344
            }
          }
        ]
      },
      "meta": {
        "issue_number": 698,
        "object_id": "interactions:arxiv.2511.07689",
        "created_at": "2025-11-12T20:46:36+00:00",
        "updated_at": "2025-11-12T21:17:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.25771": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.25771",
        "url": "https://arxiv.org/pdf/2510.25771",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-13T06:23:27.707Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 699,
        "object_id": "paper:arxiv.2510.25771",
        "created_at": "2025-11-13T06:23:28+00:00",
        "updated_at": "2025-11-13T06:23:47+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.25771": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.25771",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T06:29:48.845Z",
            "data": {
              "session_id": "session_1763015387458_eke3flt",
              "source_id": "arxiv",
              "paper_id": "2510.25771",
              "start_time": "2025-11-13T06:23:27.070Z",
              "end_time": "2025-11-13T06:29:47.458Z",
              "heartbeat_count": 76,
              "duration_seconds": 380,
              "idle_seconds": 0,
              "total_elapsed_seconds": 380
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T07:02:10.119Z",
            "data": {
              "session_id": "session_1763017327876_eoktstz",
              "source_id": "arxiv",
              "paper_id": "2510.25771",
              "start_time": "2025-11-13T06:50:38.213Z",
              "end_time": "2025-11-13T07:02:07.876Z",
              "heartbeat_count": 137,
              "duration_seconds": 685,
              "idle_seconds": 5,
              "total_elapsed_seconds": 690
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T08:14:30.009Z",
            "data": {
              "session_id": "session_1763021669989_r9f52qy",
              "source_id": "arxiv",
              "paper_id": "2510.25771",
              "start_time": "2025-11-13T08:14:08.763Z",
              "end_time": "2025-11-13T08:14:29.989Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T09:49:40.705Z",
            "data": {
              "session_id": "session_1763027380675_nwkumcl",
              "source_id": "arxiv",
              "paper_id": "2510.25771",
              "start_time": "2025-11-13T09:49:29.162Z",
              "end_time": "2025-11-13T09:49:40.675Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T09:58:22.456Z",
            "data": {
              "session_id": "session_1763027902447_57r03wp",
              "source_id": "arxiv",
              "paper_id": "2510.25771",
              "start_time": "2025-11-13T09:57:36.154Z",
              "end_time": "2025-11-13T09:58:22.447Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 1,
              "total_elapsed_seconds": 46
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T10:21:35.548Z",
            "data": {
              "session_id": "session_1763029294382_zob4xje",
              "source_id": "arxiv",
              "paper_id": "2510.25771",
              "start_time": "2025-11-13T10:15:52.517Z",
              "end_time": "2025-11-13T10:21:34.382Z",
              "heartbeat_count": 68,
              "duration_seconds": 340,
              "idle_seconds": 2,
              "total_elapsed_seconds": 342
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T10:36:09.020Z",
            "data": {
              "session_id": "session_1763030167879_dwpahu6",
              "source_id": "arxiv",
              "paper_id": "2510.25771",
              "start_time": "2025-11-13T10:35:44.483Z",
              "end_time": "2025-11-13T10:36:07.879Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T16:57:58.803Z",
            "data": {
              "session_id": "session_1763053078142_cagth1u",
              "source_id": "arxiv",
              "paper_id": "2510.25771",
              "start_time": "2025-11-13T16:55:41.525Z",
              "end_time": "2025-11-13T16:57:58.142Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 2,
              "total_elapsed_seconds": 137
            }
          }
        ]
      },
      "meta": {
        "issue_number": 700,
        "object_id": "interactions:arxiv.2510.25771",
        "created_at": "2025-11-13T06:29:49+00:00",
        "updated_at": "2025-11-13T16:58:24+00:00",
        "version": 1
      }
    },
    "paper:openreview.qUtxOc1Akd": {
      "data": {
        "sourceId": "openreview",
        "paperId": "qUtxOc1Akd",
        "url": "https://openreview.net/pdf?id=qUtxOc1Akd",
        "title": "qUtxOc1Akd",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-13T10:14:43.200Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 701,
        "object_id": "paper:openreview.qUtxOc1Akd",
        "created_at": "2025-11-13T10:14:43+00:00",
        "updated_at": "2025-11-13T10:15:07+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2511.09493": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.09493",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T13:37:28.349Z",
            "data": {
              "session_id": "session_1763041047523_v582k7y",
              "source_id": "arxiv",
              "paper_id": "2511.09493",
              "start_time": "2025-11-13T13:35:23.050Z",
              "end_time": "2025-11-13T13:37:27.523Z",
              "heartbeat_count": 24,
              "duration_seconds": 120,
              "idle_seconds": 4,
              "total_elapsed_seconds": 124
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T13:39:51.221Z",
            "data": {
              "session_id": "session_1763041191155_tzwcyds",
              "source_id": "arxiv",
              "paper_id": "2511.09493",
              "start_time": "2025-11-13T13:39:16.323Z",
              "end_time": "2025-11-13T13:39:51.155Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 5,
              "total_elapsed_seconds": 35
            }
          }
        ]
      },
      "meta": {
        "issue_number": 703,
        "object_id": "interactions:arxiv.2511.09493",
        "created_at": "2025-11-13T13:34:57+00:00",
        "updated_at": "2025-11-13T13:40:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2511.09493": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.09493",
        "url": "https://arxiv.org/abs/2511.09493",
        "title": "Consensus Sampling for Safer Generative AI",
        "authors": "Adam Tauman Kalai, Yael Tauman Kalai, Or Zamir",
        "abstract": "Many approaches to AI safety rely on inspecting model outputs or activations, yet certain risks are inherently undetectable by inspection alone. We propose a complementary, architecture-agnostic approach that enhances safety through the aggregation of multiple generative models, with the aggregated model inheriting its safety from the safest subset of a given size among them. Specifically, we present a consensus sampling algorithm that, given kk models and a prompt, achieves risk competitive with the average risk of the safest ss of the kk models, where ss is a chosen parameter, while abstaining when there is insufficient agreement between them. The approach leverages the models' ability to compute output probabilities, and we bound the probability of abstention when sufficiently many models are safe and exhibit adequate agreement. The algorithm is inspired by the provable copyright protection algorithm of Vyas et al. (2023). It requires some overlap among safe models, offers no protection when all models are unsafe, and may accumulate risk over repeated use. Nonetheless, our results provide a new, model-agnostic approach for AI safety by amplifying safety guarantees from an unknown subset of models within a collection to that of a single reliable model.",
        "timestamp": "2025-11-13T13:34:44.208Z",
        "rating": "novote",
        "publishedDate": "2025/11/12",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 702,
        "object_id": "paper:arxiv.2511.09493",
        "created_at": "2025-11-13T13:34:44+00:00",
        "updated_at": "2025-11-13T13:35:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.14805": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.14805",
        "url": "https://arxiv.org/html/2507.14805v1",
        "title": "Subliminal Learning: language models transmit behavioral traits via hidden signals in data",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-13T14:22:33.319Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 704,
        "object_id": "paper:arxiv.2507.14805",
        "created_at": "2025-11-13T14:22:33+00:00",
        "updated_at": "2025-11-13T14:22:58+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.14805": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.14805",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T14:43:37.509Z",
            "data": {
              "session_id": "session_1763045016666_yt6t48f",
              "source_id": "arxiv",
              "paper_id": "2507.14805",
              "start_time": "2025-11-13T14:22:48.237Z",
              "end_time": "2025-11-13T14:43:36.666Z",
              "heartbeat_count": 249,
              "duration_seconds": 1245,
              "idle_seconds": 3,
              "total_elapsed_seconds": 1248
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T15:35:19.656Z",
            "data": {
              "session_id": "session_1763048118740_mf5x14s",
              "source_id": "arxiv",
              "paper_id": "2507.14805",
              "start_time": "2025-11-13T15:34:57.001Z",
              "end_time": "2025-11-13T15:35:18.740Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "issue_number": 705,
        "object_id": "interactions:arxiv.2507.14805",
        "created_at": "2025-11-13T14:43:38+00:00",
        "updated_at": "2025-11-13T15:35:57+00:00",
        "version": 1
      }
    },
    "paper:openreview.G4dmISzqIC": {
      "data": {
        "sourceId": "openreview",
        "paperId": "G4dmISzqIC",
        "url": "https://openreview.net/forum?id=G4dmISzqIC&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "LEVELA",
        "authors": "Orian Dabod, Amir David Nissan Cohen, Gabriel Stanovsky",
        "abstract": "Vocabulary expansion in Large Language Models (LLMs) is crucial for improving computational performance via token efficiency, especially for domains or languages like Hebrew that are underrepresented in the original tokenizer. We propose a lightweight and efficient methodology to extend an LLM's vocabulary without requiring full retraining, thus preserving most of the original model performance. Our approach involves three core enhancements, addressing distinct challenges in the expansion process\n\nFirst, we introduced Extend Tokenizer algorithm to intelligently select new tokens and expand the existing vocabulary. Second, we introduce Parent Logic a novel algorithm governing a patchscopes based initialization method to effectively initialize the new token parameters. Finally, we implement PLGZ (part layered grad zeroing) Training, a new adaptation technique specifically designed to significantly reduce the required training time for vocabulary integration.\n\nTested extensively across multiple languages, including Hebrew, Amharic, German, Chinese, Arabic, and Korean, our findings demonstrate that poor expansion methodology can severely degrade performance. We show that our three step enhancement is crucial for maximizing token efficiency while maintaining high model performance.",
        "timestamp": "2025-11-13T15:47:50.917Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "language adaptation",
          "vocab expansion"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 706,
        "object_id": "paper:openreview.G4dmISzqIC",
        "created_at": "2025-11-13T15:47:51+00:00",
        "updated_at": "2025-11-13T15:48:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2511.07722": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.07722",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-13T16:21:52.540Z",
            "data": {
              "session_id": "session_1763050912495_rwlm1y4",
              "source_id": "arxiv",
              "paper_id": "2511.07722",
              "start_time": "2025-11-13T16:21:25.107Z",
              "end_time": "2025-11-13T16:21:52.495Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          }
        ]
      },
      "meta": {
        "issue_number": 708,
        "object_id": "interactions:arxiv.2511.07722",
        "created_at": "2025-11-13T16:21:23+00:00",
        "updated_at": "2025-11-13T16:22:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2511.07722": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.07722",
        "url": "https://arxiv.org/pdf/2511.07722",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-13T16:21:00.607Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 707,
        "object_id": "paper:arxiv.2511.07722",
        "created_at": "2025-11-13T16:21:01+00:00",
        "updated_at": "2025-11-13T16:21:24+00:00",
        "version": 1
      }
    },
    "interactions:openreview.yfXYMxURxF": {
      "data": {
        "sourceId": "openreview",
        "paperId": "yfXYMxURxF",
        "interactions": []
      },
      "meta": {
        "issue_number": 710,
        "object_id": "interactions:openreview.yfXYMxURxF",
        "created_at": "2025-11-13T20:53:44+00:00",
        "updated_at": "2025-11-13T20:53:46+00:00",
        "version": 1
      }
    },
    "paper:openreview.yfXYMxURxF": {
      "data": {
        "sourceId": "openreview",
        "paperId": "yfXYMxURxF",
        "url": "https://openreview.net/forum?id=yfXYMxURxF&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Uncovering Measurement Biases in LLM Embedding Spaces: The Anna Karenina Principle and Its Implications for Automated Feedback",
        "authors": "Abigail Gurin Schleifer, Beata Beigman Klebanov, Giora Alexandron",
        "abstract": "Large Language Models (LLMs) are becoming\nincreasingly popular in assessment systems for\nanalyzing and providing personalized feedback\non student responses to open-ended questions.\nHowever, the quality of diagnosis provided by\nsuch systems depends heavily on the ability of\nthe LLMs to accurately capture the subtle differences\nbetween responses that represent the\nkey types of student reasoning, also referred to\nas Knowledge Profiles (KPs). In this study, we\ncompared expert-defined KPs with data-driven\nclusters generated from LLM embeddings of\nstudent responses in biology. We aimed to determine\nwhether LLM-based clusters align with\nthe theory-driven KPs that classify responses by\ntheir level of conceptual accuracy. Our findings\nrevealed a \u2018discoverability bias\u2019 where LLMderived\nclusters captured reasonably well the\nhigh-quality responses, but failed to distinguish\nbetween the different ways student responses\ncan be incorrect. We then traced this \u2018discoverability\nbias\u2019 to the representations of the KPs\nin the pre-trained LLM embedding space and\nshowed that as student responses become more\nwrong, they become less similar in the embedding\nspace to other responses that reveal the\nsame type of conceptual error. Furthermore, we\nfound a strong relationship between the quality\nof the KP responses (correct or various degrees\nof incorrect) and the shape and density of their\nembeddings-based representation. Specifically,\nwe found that the lower the quality of the KP,\nthe less similar its responses are to each other in\nthe embedding space. This phenomenon, which\nwe call the \u2018Anna Karenina Principle\u2019 and study\nin the context of automated short answer scoring,\nsuggests that LLM embeddings may not\nbe sufficiently sensitive out-of-the-box to the\nnuances that distinguish between key profiles\nof conceptual understanding. This limitation\nposes challenges for developing fair and effective\nLLM-based formative assessment systems.\nThe paper have been published at the International\nJournal of Artificial Intelligence in Education\n(Gurin Schleifer et al., 2025).",
        "timestamp": "2025-11-13T20:53:36.127Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "NLP \u00b7 Large language models (LLMs)",
          "Fairness",
          "Personalised feedback",
          "Automated assessment",
          "Bias",
          "Automated",
          "Short Answer Scoring",
          "Embeddings"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 709,
        "object_id": "paper:openreview.yfXYMxURxF",
        "created_at": "2025-11-13T20:53:36+00:00",
        "updated_at": "2025-11-13T20:53:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2511.05923": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.05923",
        "url": "https://arxiv.org/pdf/2511.05923",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-14T06:53:57.597Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 711,
        "object_id": "paper:arxiv.2511.05923",
        "created_at": "2025-11-14T06:53:58+00:00",
        "updated_at": "2025-11-14T06:54:22+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2511.05923": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.05923",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-14T06:54:32.849Z",
            "data": {
              "session_id": "session_1763103272245_bgbuphg",
              "source_id": "arxiv",
              "paper_id": "2511.05923",
              "start_time": "2025-11-14T06:53:57.294Z",
              "end_time": "2025-11-14T06:54:32.245Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 5,
              "total_elapsed_seconds": 35
            }
          }
        ]
      },
      "meta": {
        "issue_number": 712,
        "object_id": "interactions:arxiv.2511.05923",
        "created_at": "2025-11-14T06:54:33+00:00",
        "updated_at": "2025-11-14T06:54:55+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.07186": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.07186",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-15T11:58:50.001Z",
            "data": {
              "session_id": "session_1763207929321_frzpfpi",
              "source_id": "arxiv",
              "paper_id": "2507.07186",
              "start_time": "2025-11-15T11:57:50.455Z",
              "end_time": "2025-11-15T11:58:49.321Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 4,
              "total_elapsed_seconds": 59
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-15T12:03:58.082Z",
            "data": {
              "session_id": "session_1763208237488_vl6cy41",
              "source_id": "arxiv",
              "paper_id": "2507.07186",
              "start_time": "2025-11-15T12:00:07.362Z",
              "end_time": "2025-11-15T12:03:57.488Z",
              "heartbeat_count": 46,
              "duration_seconds": 230,
              "idle_seconds": 0,
              "total_elapsed_seconds": 230
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-15T12:06:24.853Z",
            "data": {
              "session_id": "session_1763208384837_wkxm028",
              "source_id": "arxiv",
              "paper_id": "2507.07186",
              "start_time": "2025-11-15T12:06:03.318Z",
              "end_time": "2025-11-15T12:06:24.837Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "issue_number": 713,
        "object_id": "interactions:arxiv.2507.07186",
        "created_at": "2025-11-15T11:58:50+00:00",
        "updated_at": "2025-11-15T12:06:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.12027": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.12027",
        "url": "https://arxiv.org/abs/2406.12027",
        "title": "Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI",
        "authors": "Robert H\u00f6nig, Javier Rando, Nicholas Carlini, Florian Tram\u00e8r",
        "abstract": "Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles. In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections -- with millions of downloads -- and show they only provide a false sense of security. We find that low-effort and \"off-the-shelf\" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate that all existing protections can be easily bypassed, leaving artists vulnerable to style mimicry. We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative non-technological solutions.",
        "timestamp": "2025-11-15T19:49:22.310Z",
        "rating": "novote",
        "publishedDate": "2024/06/17",
        "tags": [
          "Cryptography and Security (cs.CR)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 716,
        "object_id": "paper:arxiv.2406.12027",
        "created_at": "2025-11-15T19:49:22+00:00",
        "updated_at": "2025-11-15T19:49:42+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2302.04222": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2302.04222",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-15T19:51:03.224Z",
            "data": {
              "session_id": "session_1763236263179_hiqbxtj",
              "source_id": "arxiv",
              "paper_id": "2302.04222",
              "start_time": "2025-11-15T19:50:41.695Z",
              "end_time": "2025-11-15T19:51:03.179Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-15T19:57:34.125Z",
            "data": {
              "session_id": "session_1763236654121_p2cf18q",
              "source_id": "arxiv",
              "paper_id": "2302.04222",
              "start_time": "2025-11-15T19:57:27.546Z",
              "end_time": "2025-11-15T19:57:34.121Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-15T19:58:36.248Z",
            "data": {
              "session_id": "session_1763236716175_nqkljgg",
              "source_id": "arxiv",
              "paper_id": "2302.04222",
              "start_time": "2025-11-15T19:58:31.064Z",
              "end_time": "2025-11-15T19:58:36.175Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 715,
        "object_id": "interactions:arxiv.2302.04222",
        "created_at": "2025-11-15T19:49:14+00:00",
        "updated_at": "2025-11-15T19:58:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2302.04222": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2302.04222",
        "url": "https://arxiv.org/pdf/2302.04222",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-15T19:49:04.418Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 714,
        "object_id": "paper:arxiv.2302.04222",
        "created_at": "2025-11-15T19:49:04+00:00",
        "updated_at": "2025-11-15T19:49:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.12027": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.12027",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-15T19:51:22.342Z",
            "data": {
              "session_id": "session_1763236282331_fwv9ouq",
              "source_id": "arxiv",
              "paper_id": "2406.12027",
              "start_time": "2025-11-15T19:51:04.249Z",
              "end_time": "2025-11-15T19:51:22.331Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 3,
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-15T19:57:27.711Z",
            "data": {
              "session_id": "session_1763236647449_yfosnru",
              "source_id": "arxiv",
              "paper_id": "2406.12027",
              "start_time": "2025-11-15T19:57:16.636Z",
              "end_time": "2025-11-15T19:57:27.449Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-15T19:58:31.584Z",
            "data": {
              "session_id": "session_1763236710943_wy0ereq",
              "source_id": "arxiv",
              "paper_id": "2406.12027",
              "start_time": "2025-11-15T19:57:34.302Z",
              "end_time": "2025-11-15T19:58:30.943Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 2,
              "total_elapsed_seconds": 57
            }
          }
        ]
      },
      "meta": {
        "issue_number": 717,
        "object_id": "interactions:arxiv.2406.12027",
        "created_at": "2025-11-15T19:51:22+00:00",
        "updated_at": "2025-11-15T19:58:50+00:00",
        "version": 1
      }
    },
    "interactions:openreview.Moc28dU6le": {
      "data": {
        "sourceId": "openreview",
        "paperId": "Moc28dU6le",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-16T12:01:25.738Z",
            "data": {
              "session_id": "session_1763294485183_d5v3jdd",
              "source_id": "openreview",
              "paper_id": "Moc28dU6le",
              "start_time": "2025-11-16T12:01:16.460Z",
              "end_time": "2025-11-16T12:01:25.183Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 719,
        "object_id": "interactions:openreview.Moc28dU6le",
        "created_at": "2025-11-16T12:01:26+00:00",
        "updated_at": "2025-11-16T12:01:49+00:00",
        "version": 1
      }
    },
    "paper:openreview.Moc28dU6le": {
      "data": {
        "sourceId": "openreview",
        "paperId": "Moc28dU6le",
        "url": "https://openreview.net/forum?id=Moc28dU6le",
        "title": "Area Chair Emergency Reviewing Form",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-16T12:01:17.104Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "ACL ARR 2025 October Area Chairs Emergency Metareviewer Agreement Form",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 718,
        "object_id": "paper:openreview.Moc28dU6le",
        "created_at": "2025-11-16T12:01:17+00:00",
        "updated_at": "2025-11-16T12:01:36+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1912.13283": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1912.13283",
        "url": "https://arxiv.org/abs/1912.13283",
        "title": "oLMpics -- On what Language Model Pre-training Captures",
        "authors": "Alon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant",
        "abstract": "Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training.",
        "timestamp": "2025-11-16T18:13:56.286Z",
        "rating": "novote",
        "publishedDate": "2019/12/31",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 720,
        "object_id": "paper:arxiv.1912.13283",
        "created_at": "2025-11-16T18:13:56+00:00",
        "updated_at": "2025-11-16T18:14:18+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1912.13283": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1912.13283",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-16T18:15:10.878Z",
            "data": {
              "session_id": "session_1763316910224_c7zpld4",
              "source_id": "arxiv",
              "paper_id": "1912.13283",
              "start_time": "2025-11-16T18:13:58.048Z",
              "end_time": "2025-11-16T18:15:10.224Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 2,
              "total_elapsed_seconds": 72
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-16T18:20:04.021Z",
            "data": {
              "session_id": "session_1763317203415_2ir1edv",
              "source_id": "arxiv",
              "paper_id": "1912.13283",
              "start_time": "2025-11-16T18:15:12.096Z",
              "end_time": "2025-11-16T18:20:03.414Z",
              "heartbeat_count": 58,
              "duration_seconds": 290,
              "idle_seconds": 1,
              "total_elapsed_seconds": 291
            }
          }
        ]
      },
      "meta": {
        "issue_number": 721,
        "object_id": "interactions:arxiv.1912.13283",
        "created_at": "2025-11-16T18:15:11+00:00",
        "updated_at": "2025-11-16T18:20:25+00:00",
        "version": 1
      }
    },
    "paper:openreview.vxyYhY6chZ": {
      "data": {
        "sourceId": "openreview",
        "paperId": "vxyYhY6chZ",
        "url": "https://openreview.net/forum?id=vxyYhY6chZ&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Building an Accurate Open-Source Hebrew ASR System through Crowdsourcing",
        "authors": "Yanir Marmor, Yair Lifshitz, Yoad Snapir, Kinneret Misgav",
        "abstract": "Automatic Speech Recognition (ASR) for Hebrew faces significant challenges due to limited resources and rich morphology.\nWhile recent advances have improved high-resource languages ASR, Hebrew still lacks robust open-source solutions. Through\ncrowdsourcing efforts, we created a dataset of 314 hours of transcribed speech, which we used to train a new Hebrew ASR\nmodel based on the Whisper architecture. Our model demonstrates up to 29% reduction in error rates compared to existing\nWhisper solutions, particularly excelling in producing verbatim transcriptions. Additionally, we introduce a new evaluation\ndataset designed specifically for Hebrew ASR assessment. By making both the model and methodology freely available, we\nprovide a framework that can be adapted for developing ASR systems in other under-resourced languages. This work represents a step toward making speech technology more accessible in different languages",
        "timestamp": "2025-11-17T10:53:46.794Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "speech recognition",
          "Hebrew ASR",
          "crowdsourcing",
          "under-resourced languages",
          "open-source models",
          "Whisper architecture"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 722,
        "object_id": "paper:openreview.vxyYhY6chZ",
        "created_at": "2025-11-17T10:53:47+00:00",
        "updated_at": "2025-11-17T10:54:11+00:00",
        "version": 1
      }
    },
    "paper:openreview.JTayo79NT3": {
      "data": {
        "sourceId": "openreview",
        "paperId": "JTayo79NT3",
        "url": "https://openreview.net/forum?id=JTayo79NT3&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Decoding Reading Goals from Eye Movements",
        "authors": "Omer Shubi, Cfir Avraham Hadar, Yevgeni Berzak",
        "abstract": "Readers can have different goals with respect to the text that they are reading. Can these goals be decoded from their eye movements over the text? In this work, we examine for the first time whether it is possible to distinguish between two types of common reading goals: information seeking and ordinary reading for comprehension. Using large-scale eye tracking data, we address this task with a wide range of models that cover different architectural and data representation strategies, and further introduce a new model ensemble. We find that transformer-based models with scanpath representations coupled with language modeling solve it most successfully, and that accurate predictions can be made in \\emph{real time}, shortly after the participant started reading the text. We further introduce a new method for model performance analysis based on mixed effect modeling. Combining this method with rich textual annotations reveals key properties of textual items and participants that contribute to the difficulty of the task, and improves our understanding of the variability in eye movement patterns across the two reading regimes.",
        "timestamp": "2025-11-17T10:55:59.899Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "cognitive modeling",
          "computational psycholinguistics"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 727,
        "object_id": "paper:openreview.JTayo79NT3",
        "created_at": "2025-11-17T10:56:00+00:00",
        "updated_at": "2025-11-17T10:56:23+00:00",
        "version": 1
      }
    },
    "interactions:openreview.MFlx7Tt6Dk": {
      "data": {
        "sourceId": "openreview",
        "paperId": "MFlx7Tt6Dk",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T10:55:39.013Z",
            "data": {
              "session_id": "session_1763376938958_iik3uko",
              "source_id": "openreview",
              "paper_id": "MFlx7Tt6Dk",
              "start_time": "2025-11-17T10:55:26.206Z",
              "end_time": "2025-11-17T10:55:38.958Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "issue_number": 726,
        "object_id": "interactions:openreview.MFlx7Tt6Dk",
        "created_at": "2025-11-17T10:55:39+00:00",
        "updated_at": "2025-11-17T10:55:58+00:00",
        "version": 1
      }
    },
    "paper:openreview.MFlx7Tt6Dk": {
      "data": {
        "sourceId": "openreview",
        "paperId": "MFlx7Tt6Dk",
        "url": "https://openreview.net/forum?id=MFlx7Tt6Dk&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Automatic biblical authorship attribution",
        "authors": "Shira Faigenbaum-Golovin, Alon Kipnis, Axel B\u00fchler, Eliezer Piasetzky, Thomas R\u00f6mer, Israel Finkelstein",
        "abstract": "This study employs computational linguistic techniques to analyze the authorship of biblical texts, providing a data-driven perspective on one of the most extensively studied corpora in human history. We develop a statistical model that is sensitive to deviations in word and n-gram frequencies to differentiate among three major scribal traditions traditionally identified by biblical scholars: the Deuteronomic (D), Deuteronomistic History (DtrH), and Priestly (P) corpora. Using entire chapters from the first nine books of the Hebrew Bible, we show that linguistic signals alone can effectively cluster these corpora, with D and DtrH exhibiting greater similarity to each other than to P, a finding consistent with long-standing philological hypotheses. We further use these corpora as ground truth to evaluate authorship in additional, disputed texts. For instance, when we compared the authorship of the two Ark narratives, a surprising pattern emerged. Our results demonstrate that interpretable, statistically robust linguistic features can capture stylistic and compositional distinctions across ancient textual traditions, highlighting the potential of such methodologies for authorship attribution and historical text analysis in low-resource.",
        "timestamp": "2025-11-17T10:55:20.108Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "authorship attribution",
          "word frequency",
          "statistical analysis",
          "bible"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 725,
        "object_id": "paper:openreview.MFlx7Tt6Dk",
        "created_at": "2025-11-17T10:55:20+00:00",
        "updated_at": "2025-11-17T10:55:47+00:00",
        "version": 1
      }
    },
    "interactions:openreview.vxyYhY6chZ": {
      "data": {
        "sourceId": "openreview",
        "paperId": "vxyYhY6chZ",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T10:54:48.643Z",
            "data": {
              "session_id": "session_1763376887460_urw4epv",
              "source_id": "openreview",
              "paper_id": "vxyYhY6chZ",
              "start_time": "2025-11-17T10:53:48.310Z",
              "end_time": "2025-11-17T10:54:47.460Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 4,
              "total_elapsed_seconds": 59
            }
          }
        ]
      },
      "meta": {
        "issue_number": 724,
        "object_id": "interactions:openreview.vxyYhY6chZ",
        "created_at": "2025-11-17T10:54:49+00:00",
        "updated_at": "2025-11-17T10:55:13+00:00",
        "version": 1
      }
    },
    "paper:openreview.S4fteIdF0h": {
      "data": {
        "sourceId": "openreview",
        "paperId": "S4fteIdF0h",
        "url": "https://openreview.net/forum?id=S4fteIdF0h&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Out-of-Context Reasoning in Large Language Models",
        "authors": "Jonathan Shaki, Emanuele La Malfa, Michael J. Wooldridge, Sarit Kraus",
        "abstract": "We study how large language models (LLMs) reason about memorized knowledge through simple binary relations such as equality ($=$), inequality ($<$), and inclusion ($\\subset$). Unlike in-context reasoning, the axioms (e.g., $a < b, b < c$) are only seen during training and not provided in the task prompt (e.g., evaluating $a < c$). The tasks require one or more reasoning steps, and data aggregation from one or more sources, showing performance change with task complexity. We introduce a lightweight technique, out-of-context representation learning, which trains only new token embeddings on axioms and evaluates them on unseen tasks. Across reflexivity, symmetry, and transitivity tests, LLMs mostly perform statistically significant better than chance, making the correct answer extractable when testing multiple phrasing variations, but still fall short of consistent reasoning on every single query. Analysis shows that the learned embeddings are organized in structured ways, suggesting real relational understanding. Surprisingly, it also indicates that the core reasoning happens during the training, not inference.\n\nAccepted to EMNLP 2025 findings",
        "timestamp": "2025-11-17T10:57:01.936Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "LLMs",
          "interpretability",
          "reasoning"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 729,
        "object_id": "paper:openreview.S4fteIdF0h",
        "created_at": "2025-11-17T10:57:02+00:00",
        "updated_at": "2025-11-17T10:57:24+00:00",
        "version": 1
      }
    },
    "paper:openreview.QDeQxSwLzT": {
      "data": {
        "sourceId": "openreview",
        "paperId": "QDeQxSwLzT",
        "url": "https://openreview.net/pdf?id=QDeQxSwLzT",
        "title": "QDeQxSwLzT",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-17T10:59:06.337Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 731,
        "object_id": "paper:openreview.QDeQxSwLzT",
        "created_at": "2025-11-17T10:59:06+00:00",
        "updated_at": "2025-11-17T10:59:32+00:00",
        "version": 1
      }
    },
    "paper:openreview.K8cmGgth4O": {
      "data": {
        "sourceId": "openreview",
        "paperId": "K8cmGgth4O",
        "url": "https://openreview.net/forum?id=K8cmGgth4O&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Differences in Input and Output Quality of LLMs Across Age Groups",
        "authors": "Shira Darchi, Yuval Pinter",
        "abstract": "This work-in-progress examines how younger and older adults create prompts for large language models (LLMs) and whether variations in input quality affect the quality of the generated outputs. Building on evidence that aging influences linguistic style and digital communication habits, the study investigates differences in prompt length, linguistic complexity, and structural clarity across age groups. Using a mixed within- and between-subject design, participants complete two prompting tasks\u2014trip planning and greeting writing\u2014and provide self-assessments of the corresponding model responses. Output quality is evaluated through subjective ratings, error coding, and external assessments from both human raters and other LLMs. The ongoing analysis aims to determine whether age-related differences in prompt construction are associated with systematic variations in the quality of LLM responses. This research aims to enhance our understanding of how demographic factors and linguistic patterns shape interactions with AI systems and the resulting user experience.",
        "timestamp": "2025-11-17T10:59:55.120Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "LLMs",
          "NLP",
          "prompting",
          "aging",
          "input quality",
          "output quality"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 732,
        "object_id": "paper:openreview.K8cmGgth4O",
        "created_at": "2025-11-17T10:59:55+00:00",
        "updated_at": "2025-11-17T11:00:14+00:00",
        "version": 1
      }
    },
    "paper:openreview.VAWMM8oG9x": {
      "data": {
        "sourceId": "openreview",
        "paperId": "VAWMM8oG9x",
        "url": "https://openreview.net/forum?id=VAWMM8oG9x&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Pixels at BAREC Shared Task 2025: Visual Arabic Readability Assessment",
        "authors": "Ben Sapirstein",
        "abstract": "We present a visual-language approach to Arabic readability assessment using the PIXEL Vision Transformer, which processes rendered text as images to bypass tokenization challenges. Our system participated in the BAREC 2025 Shared Task (Sentence-level Strict track). We evaluate orthographic variants (normalization, diacritization, transliteration) and morphological segmentation with different visual boundary markers. Results show that diacritization provides useful visual cues for disambiguation, morphological segmentation improves over word-level processing, and transliterated scripts outperform native Arabic script. Our approach demonstrates the potential of visual processing for readability assessment in complex languages and writing systems.",
        "timestamp": "2025-11-17T11:02:12.152Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Arabic readability",
          "BAREC shared task",
          "Visual language models",
          "Orthographic variation",
          "Morphological segmentation"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 739,
        "object_id": "paper:openreview.VAWMM8oG9x",
        "created_at": "2025-11-17T11:02:13+00:00",
        "updated_at": "2025-11-17T11:02:34+00:00",
        "version": 1
      }
    },
    "interactions:openreview.RO9sAL6RWp": {
      "data": {
        "sourceId": "openreview",
        "paperId": "RO9sAL6RWp",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T11:01:20.107Z",
            "data": {
              "session_id": "session_1763377280068_1uevt54",
              "source_id": "openreview",
              "paper_id": "RO9sAL6RWp",
              "start_time": "2025-11-17T11:01:08.104Z",
              "end_time": "2025-11-17T11:01:20.068Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "issue_number": 737,
        "object_id": "interactions:openreview.RO9sAL6RWp",
        "created_at": "2025-11-17T11:01:20+00:00",
        "updated_at": "2025-11-17T11:01:44+00:00",
        "version": 1
      }
    },
    "paper:openreview.RO9sAL6RWp": {
      "data": {
        "sourceId": "openreview",
        "paperId": "RO9sAL6RWp",
        "url": "https://openreview.net/forum?id=RO9sAL6RWp&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Semi-synthetic parallel data for translation quality estimation: A case study of English\u2013Hebrew",
        "authors": "Assaf Siani, Ilan Kernerman, Anna Kernerman",
        "abstract": "Quality Estimation (QE) constitutes a vital component of machine translation (MT) workflows, enabling the automatic assessment of translation quality without reliance on human reference translations (Specia et al. 2018, Kepler et al. 2019). Nevertheless, developing accurate and adaptable QE models for under-resourced and morphologically rich languages remains a big challenge, primarily due to the scarcity of high-quality parallel data and the intricate linguistic phenomena involved (More & Tsarfaty 2016). This study introduces a semi-synthetic English\u2013Hebrew dataset designed for training neural QE models, while outlining its transition from research to real-world implementation within Lexicala\u2019s AI-driven translation infrastructure.\n\nUnder-resourced languages are particularly affected by the Anglo-centric bias pervasive in most MT training data, which limits both linguistic diversity and cultural representation. Recognizing this imbalance, we sought to develop a QE framework that unites human linguistic validation with scalable AI-based modeling, an approach encapsulated in our vision of achieving human quality at AI scale. The objective was to establish a system capable of accurately evaluating machine-generated translations and adapting dynamically to specific language pairs, client domains, and stylistic conventions.\n\nFollowing established methodologies in semi-synthetic data creation (Edunov et al. 2018, Fadaee et al. 2017), we compiled an English\u2013Hebrew parallel corpus by prompting ChatGPT to generate English sentences modeled on usage examples illustrating typical linguistic patterns extracted from learner\u2019s dictionaries. These sentences were translated into Hebrew using multiple MT engines (Google, Microsoft,  Yandex), after which an algorithmic selection process attempted to identify the optimal translation for each source sentence. Each English\u2013Hebrew pair was subsequently evaluated and ranked by an expert using a 5-point scale assessing accuracy, fluency, style, and completeness, resulting in a curated set of approximately 12,000 manually scored segments.\n\nTo enlarge and balance the corpus, we applied a series of controlled perturbations simulating recurrent morphosyntactic challenges in Hebrew, notably gender and number agreement mismatches and word-order variations (Belinkov & Bisk 2018, Habash & Roth 2009). Each intentional error reduced the corresponding quality score by one or two points, thereby generating 200K labeled segments encompassing a continuous range of translation quality. Additional adversarial augmentation, pairing unrelated English and Hebrew sentences and assigning them a score of zero, produced a comprehensive dataset of roughly 300K bilingual instances. Through further scaling and automation, the dataset was later expanded to 10M semi-synthetic segments for large-scale model training.\n\nFor QE modeling, we fine-tuned pre-trained BERT architectures (Devlin et al. 2019) using low-rank adaptation (LoRA; Hu et al. 2022). Experiments across multiple data sampling strategies showed a strong link between corpus characteristics and model performance. Training on a 500K normally distributed sample yielded a Pearson correlation of 0.65, which rose to 0.88 with a uniformly distributed 430K dataset. Scaling to 1M and 4M segments achieved 0.92, illustrating both the gains and the diminishing returns of increased data volume. These findings underscore that, beyond size, balanced score distribution and linguistic diversity are essential for effective and generalizable QE learning.\n\nBeyond research, these insights were operationalized within Lexicala\u2019s commercial localization pipelines. Our industry deployment experience revealed that universal QE models are inherently limited: optimal results require per-client customization. By integrating client-specific content subsets into the fine-tuning process, we ensure that models internalize domain-relevant terminology, stylistic nuances, and contextual expectations. This workflow, comprising data sampling from client materials, MT engines or LLMs, expert ranking, and subsequent fine-tuning, proved most effective in achieving reliable, domain-tailored predictions. The approach exemplifies Lexicala\u2019s broader strategy of harmonizing linguistic expertise with GenAI efficiency to produce outputs that are not only quantitatively robust but also qualitatively meaningful.\n\n[References included only in the PDF file.",
        "timestamp": "2025-11-17T11:01:03.807Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Quality Estimation",
          "Machine Translation",
          "English-Hebrew",
          "Under-Resourced Languages",
          "Neural Models"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 734,
        "object_id": "paper:openreview.RO9sAL6RWp",
        "created_at": "2025-11-17T11:01:04+00:00",
        "updated_at": "2025-11-17T11:01:23+00:00",
        "version": 1
      }
    },
    "interactions:openreview.qUtxOc1Akd": {
      "data": {
        "sourceId": "openreview",
        "paperId": "qUtxOc1Akd",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T11:00:34.244Z",
            "data": {
              "session_id": "session_1763377234185_r10ithi",
              "source_id": "openreview",
              "paper_id": "qUtxOc1Akd",
              "start_time": "2025-11-17T11:00:19.463Z",
              "end_time": "2025-11-17T11:00:34.185Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 733,
        "object_id": "interactions:openreview.qUtxOc1Akd",
        "created_at": "2025-11-17T11:00:34+00:00",
        "updated_at": "2025-11-17T11:00:55+00:00",
        "version": 1
      }
    },
    "interactions:openreview.46zUH6zQWS": {
      "data": {
        "sourceId": "openreview",
        "paperId": "46zUH6zQWS",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T11:02:56.564Z",
            "data": {
              "session_id": "session_1763377376507_dkjq381",
              "source_id": "openreview",
              "paper_id": "46zUH6zQWS",
              "start_time": "2025-11-17T11:02:48.675Z",
              "end_time": "2025-11-17T11:02:56.507Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 742,
        "object_id": "interactions:openreview.46zUH6zQWS",
        "created_at": "2025-11-17T11:02:57+00:00",
        "updated_at": "2025-11-17T11:03:22+00:00",
        "version": 1
      }
    },
    "paper:openreview.46zUH6zQWS": {
      "data": {
        "sourceId": "openreview",
        "paperId": "46zUH6zQWS",
        "url": "https://openreview.net/forum?id=46zUH6zQWS&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "MRLEval: A Benchmark for LLM Evaluation in Hebrew, Modern Standard Arabic and Levantine Arabic",
        "authors": "Guy Mor-Lan, Reut Tsarfaty",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has created a significant need for robust evaluation benchmarks, particularly for languages beyond English. Morphologically rich and diglossic languages like Hebrew and Arabic present unique challenges that are often overlooked by global benchmarks. We introduce MRLEval, an open-source, model-agnostic evaluation framework. MRLEval is a comprehensive suite of evaluation tasks for Hebrew, Modern Standard Arabic (MSA and Levantine Arabic. The framework includes data downloading, processing, fine-tuning scripts (for Huggingface and T5X), and evaluation for over 20 distinct tasks across these three languages, covering question answering, summarization, NER, sentiment analysis, and more.",
        "timestamp": "2025-11-17T11:02:45.688Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Evaluation",
          "LLMs",
          "MRLs",
          "Hebrew",
          "Arabic",
          "Levantine Arabic"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 741,
        "object_id": "paper:openreview.46zUH6zQWS",
        "created_at": "2025-11-17T11:02:46+00:00",
        "updated_at": "2025-11-17T11:03:09+00:00",
        "version": 1
      }
    },
    "paper:openreview.S2sXLNNbXx": {
      "data": {
        "sourceId": "openreview",
        "paperId": "S2sXLNNbXx",
        "url": "https://openreview.net/forum?id=S2sXLNNbXx&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs%23submission-status)",
        "title": "Detecting Conspiracies in Hebrew Twitter with LLM-GNN Fusion",
        "authors": "Lior Biton, Oren Tsur",
        "abstract": "Detecting online conspiracy narratives presents a significant challenge due to the short, implicit, and context-dependent nature of social media content. This challenge is further amplified in low-resource, morphologically rich languages such as Hebrew. This study proposes and evaluates multimodal fusion models that combine textual and structural information to improve conspiracy detection. We compare three approaches: a text-only (TO) baseline leveraging Large Language Model (LLM) embeddings, a naive fusion (NF) model that combines tweet embeddings with user-level representations, and a trainable multi-task GNN (TF-GNN) that jointly models tweet- and user-level signals over the retweet network. Experiments on a labeled dataset of 3,720 Hebrew-language tweets show that incorporating network structure enhances performance: the naive fusion model outperforms the text-only baseline with an F1 score of 0.539 compared to 0.518, while the TF-GNN achieves the highest score (F1 = 0.557), demonstrating the value of deep fusion.",
        "timestamp": "2025-11-17T11:02:26.289Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "conspiracy detection",
          "Hebrew Twitter",
          "social network analysis",
          "LLM",
          "GNN",
          "multimodal fusion"
        ],
        "doi": "",
        "journalName": "ISCOL 2025 Seminar Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 740,
        "object_id": "paper:openreview.S2sXLNNbXx",
        "created_at": "2025-11-17T11:02:26+00:00",
        "updated_at": "2025-11-17T11:02:47+00:00",
        "version": 1
      }
    },
    "paper:openreview.z8pUJASaP0": {
      "data": {
        "sourceId": "openreview",
        "paperId": "z8pUJASaP0",
        "url": "https://openreview.net/pdf?id=z8pUJASaP0",
        "title": "z8pUJASaP0",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-17T11:09:59.461Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 743,
        "object_id": "paper:openreview.z8pUJASaP0",
        "created_at": "2025-11-17T11:10:00+00:00",
        "updated_at": "2025-11-17T11:10:20+00:00",
        "version": 1
      }
    },
    "paper:openreview.8NQPcdw7ja": {
      "data": {
        "sourceId": "openreview",
        "paperId": "8NQPcdw7ja",
        "url": "https://openreview.net/forum?id=8NQPcdw7ja",
        "title": "Do Language Models Robustly Acquire New Knowledge?",
        "authors": "Harshay Shah, Badih Ghazi, Yangsibo Huang, Ravi Kumar, Da Yu, Chiyuan Zhang",
        "abstract": "Language models acquire vast knowledge during pretraining, but adding new knowledge to pre-trained models often lacks robustness\u2014models can retrieve individual facts but struggle with multi-hop reasoning over newly acquired knowledge and its implications. To systematically study this robustness gap, we introduce RANK (Robust Acquisition of New Knowledge), a testbed using synthetic knowledge graphs to evaluate knowledge acquisition via $k$-hop reasoning tasks of increasing complexity. Our evaluation of supervised fine-tuning (SFT) and in-context learning (ICL) using RANK reveals that ICL performance degrades with reasoning complexity and knowledge scale, while SFT trained on simple facts fails completely at multi-hop reasoning. However, we find that increasing training data diversity induces a sharp phase transition of fine-tuned models from memorization to out-of-distribution generalization. More generally, RANK enables controlled experiments that reveal insights into knowledge acquisition robustness.",
        "timestamp": "2025-11-17T16:51:16.078Z",
        "rating": "novote",
        "publishedDate": "23 Sept 2025",
        "tags": [
          "robustness",
          "knowledge acquisition",
          "knowledge graphs",
          "factual retrieval",
          "multi-hop reasoning"
        ],
        "doi": "",
        "journalName": "CCFM Oral",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 744,
        "object_id": "paper:openreview.8NQPcdw7ja",
        "created_at": "2025-11-17T16:51:16+00:00",
        "updated_at": "2025-11-17T16:51:39+00:00",
        "version": 1
      }
    },
    "interactions:openreview.8NQPcdw7ja": {
      "data": {
        "sourceId": "openreview",
        "paperId": "8NQPcdw7ja",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T16:52:10.398Z",
            "data": {
              "session_id": "session_1763398330385_hx22u1w",
              "source_id": "openreview",
              "paper_id": "8NQPcdw7ja",
              "start_time": "2025-11-17T16:51:26.467Z",
              "end_time": "2025-11-17T16:52:10.385Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 4,
              "total_elapsed_seconds": 44
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T17:13:57.433Z",
            "data": {
              "session_id": "session_1763399637136_m8o60mh",
              "source_id": "openreview",
              "paper_id": "8NQPcdw7ja",
              "start_time": "2025-11-17T17:13:37.587Z",
              "end_time": "2025-11-17T17:13:57.136Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T19:17:27.210Z",
            "data": {
              "session_id": "session_1763407046500_cl9hkyv",
              "source_id": "openreview",
              "paper_id": "8NQPcdw7ja",
              "start_time": "2025-11-17T17:14:00.733Z",
              "end_time": "2025-11-17T19:17:26.500Z",
              "heartbeat_count": 1481,
              "duration_seconds": 7405,
              "idle_seconds": 1,
              "total_elapsed_seconds": 7406
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-18T06:16:44.368Z",
            "data": {
              "session_id": "session_1763446604046_h2k4mk8",
              "source_id": "openreview",
              "paper_id": "8NQPcdw7ja",
              "start_time": "2025-11-18T06:16:38.125Z",
              "end_time": "2025-11-18T06:16:44.046Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T12:57:24.377Z",
            "data": {
              "session_id": "session_1763643444364_mrdajct",
              "source_id": "openreview",
              "paper_id": "8NQPcdw7ja",
              "start_time": "2025-11-20T12:57:10.000Z",
              "end_time": "2025-11-20T12:57:24.364Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "issue_number": 745,
        "object_id": "interactions:openreview.8NQPcdw7ja",
        "created_at": "2025-11-17T16:52:11+00:00",
        "updated_at": "2025-11-20T12:57:51+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.17010": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.17010",
        "url": "https://arxiv.org/pdf/2402.17010",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-17T20:10:47.479Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 746,
        "object_id": "paper:arxiv.2402.17010",
        "created_at": "2025-11-17T20:10:47+00:00",
        "updated_at": "2025-11-17T20:11:13+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2402.17010": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.17010",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-17T20:14:20.626Z",
            "data": {
              "session_id": "session_1763410459991_8cwg9lr",
              "source_id": "arxiv",
              "paper_id": "2402.17010",
              "start_time": "2025-11-17T20:10:47.074Z",
              "end_time": "2025-11-17T20:14:19.991Z",
              "heartbeat_count": 42,
              "duration_seconds": 210,
              "idle_seconds": 3,
              "total_elapsed_seconds": 213
            }
          }
        ]
      },
      "meta": {
        "issue_number": 747,
        "object_id": "interactions:arxiv.2402.17010",
        "created_at": "2025-11-17T20:14:21+00:00",
        "updated_at": "2025-11-17T20:14:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.13121": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.13121",
        "url": "https://arxiv.org/abs/2406.13121",
        "title": "Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?",
        "authors": "Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, S\u00e9bastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, Kelvin Guu",
        "abstract": "Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs' ability to natively ingest and process entire corpora of information offers numerous advantages. It enhances user-friendliness by eliminating the need for specialized knowledge of tools, provides robust end-to-end modeling that minimizes cascading errors in complex pipelines, and allows for the application of sophisticated prompting techniques across the entire system. To assess this paradigm shift, we introduce LOFT, a benchmark of real-world tasks requiring context up to millions of tokens designed to evaluate LCLMs' performance on in-context retrieval and reasoning. Our findings reveal LCLMs' surprising ability to rival state-of-the-art retrieval and RAG systems, despite never having been explicitly trained for these tasks. However, LCLMs still face challenges in areas like compositional reasoning that are required in SQL-like tasks. Notably, prompting strategies significantly influence performance, emphasizing the need for continued research as context lengths grow. Overall, LOFT provides a rigorous testing ground for LCLMs, showcasing their potential to supplant existing paradigms and tackle novel tasks as model capabilities scale.",
        "timestamp": "2025-11-18T06:24:50.278Z",
        "rating": "novote",
        "publishedDate": "2024/06/19",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Information Retrieval (cs.IR)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 752,
        "object_id": "paper:arxiv.2406.13121",
        "created_at": "2025-11-18T06:24:50+00:00",
        "updated_at": "2025-11-18T06:25:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.11018": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.11018",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-18T06:24:42.196Z",
            "data": {
              "session_id": "session_1763447082113_qwga4bl",
              "source_id": "arxiv",
              "paper_id": "2404.11018",
              "start_time": "2025-11-18T06:24:33.223Z",
              "end_time": "2025-11-18T06:24:42.113Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 751,
        "object_id": "interactions:arxiv.2404.11018",
        "created_at": "2025-11-18T06:24:43+00:00",
        "updated_at": "2025-11-18T06:25:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.11018": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.11018",
        "url": "https://arxiv.org/abs/2404.11018",
        "title": "Many-Shot In-Context Learning",
        "authors": "Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, Hugo Larochelle",
        "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. We also find that inference cost increases linearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL to varying degrees. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.",
        "timestamp": "2025-11-18T06:24:28.749Z",
        "rating": "novote",
        "publishedDate": "2024/04/17",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 750,
        "object_id": "paper:arxiv.2404.11018",
        "created_at": "2025-11-18T06:24:29+00:00",
        "updated_at": "2025-11-18T06:24:52+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.01574": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.01574",
        "url": "https://arxiv.org/abs/2406.01574",
        "title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark",
        "authors": "Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen",
        "abstract": "In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.",
        "timestamp": "2025-11-18T06:24:00.821Z",
        "rating": "novote",
        "publishedDate": "2024/06/03",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 749,
        "object_id": "paper:arxiv.2406.01574",
        "created_at": "2025-11-18T06:24:01+00:00",
        "updated_at": "2025-11-18T06:24:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.05195": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.05195",
        "url": "https://arxiv.org/abs/2507.05195",
        "title": "Train-before-Test Harmonizes Language Model Rankings",
        "authors": "Guanhua Zhang, Ricardo Dominguez-Olmedo, Moritz Hardt",
        "abstract": "Existing language model benchmarks provide contradictory model rankings, even for benchmarks that aim to capture similar skills. This dilemma of conflicting rankings hampers model selection, clouds model comparisons, and adds confusion to a growing ecosystem of competing models. In this paper, we take a different perspective on model comparison: instead of relying on out-of-the-box performance via direct evaluation, we compare model potential by providing each model with identical benchmark-specific fine-tuning before evaluation. We call this approach train-before-test. Our primary contribution is a comprehensive empirical evaluation of model potential across 24 benchmarks and 61 models. First, we demonstrate that model potential rankings obtained through train-before-test exhibit remarkable consistency across all benchmarks. Whereas traditional rankings demonstrate little external validity under direct evaluation, they enjoy a significant degree of external validity when applying train-before-test: model potential rankings transfer gracefully from one benchmark to another. Second, train-before-test restores the connection between perplexity and downstream task performance, lost under direct evaluation. Remarkably, even pre-finetuning perplexity of a base model predicts post-finetuning downstream performance, suggesting that ranking consistency reflects inherent model potential rather than fine-tuning artifacts. Finally, train-before-test reduces the model-score matrix to essentially rank one, indicating that model potential is dominated by one latent factor, uncovered by train-before-test. Our work supports the recommendation to make train-before-test a default component of LLM benchmarking.",
        "timestamp": "2025-11-18T06:23:52.810Z",
        "rating": "novote",
        "publishedDate": "2025/07/07",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 748,
        "object_id": "paper:arxiv.2507.05195",
        "created_at": "2025-11-18T06:23:53+00:00",
        "updated_at": "2025-11-18T06:24:13+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2511.11810": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.11810",
        "interactions": []
      },
      "meta": {
        "issue_number": 754,
        "object_id": "interactions:arxiv.2511.11810",
        "created_at": "2025-11-18T06:56:26+00:00",
        "updated_at": "2025-11-18T06:56:28+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2511.11810": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.11810",
        "url": "https://arxiv.org/abs/2511.11810",
        "title": "On the Notion that Language Models Reason",
        "authors": "Bertram H\u00f8jer",
        "abstract": "Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \\textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are \"statistical pattern matchers\"\" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.",
        "timestamp": "2025-11-18T06:56:19.575Z",
        "rating": "novote",
        "publishedDate": "2025/11/14",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 753,
        "object_id": "paper:arxiv.2511.11810",
        "created_at": "2025-11-18T06:56:20+00:00",
        "updated_at": "2025-11-18T06:56:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.06261": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.06261",
        "url": "https://arxiv.org/abs/2507.06261",
        "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities",
        "authors": "Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, Nan-Jiang Jiang, Krishna Haridasan, Ahmed Omran, Nikunj Saunshi, Dara Bahri, Gaurav Mishra, Eric Chu, Toby Boyd, Brad Hekman, Aaron Parisi, Chaoyi Zhang, Kornraphop Kawintiranon, Tania Bedrax-Weiss, Oliver Wang, Ya Xu, Ollie Purkiss, Uri Mendlovic, Ila\u00ef Deutel, Nam Nguyen, Adam Langley, Flip Korn, Lucia Rossazza, Alexandre Ram\u00e9, Sagar Waghmare, Helen Miller, Nathan Byrd, Ashrith Sheshan, Raia Hadsell, Sangnie Bhardwaj, Pawel Janus, Tero Rissa, Dan Horgan, Alvin Abdagic, Lior Belenki, James Allingham, Anima Singh, Theo Guidroz, Srivatsan Srinivasan, Herman Schmit, Kristen Chiafullo, Andre Elisseeff, Nilpa Jha, Prateek Kolhar, Leonard Berrada, Frank Ding, Xiance Si, Shrestha Basu Mallick, Franz Och, Sofia Erell, Eric Ni, Tejasi Latkar, Sherry Yang, Petar Sirkovic, Ziqiang Feng, Robert Leland, Rachel Hornung, Gang Wu, Charles Blundell, Hamidreza Alvari, Po-Sen Huang, Cathy Yip, Sanja Deur, Li Liu, Gabriela Surita, Pablo Duque, Dima Damen, Johnson Jia, Arthur Guez, Markus Mircea, Animesh Sinha, Alberto Magni, Pawe\u0142 Stradomski, Tal Marian, Vlado Gali\u0107, Wenhu Chen, Hisham Husain, Achintya Singhal, Dominik Grewe, Fran\u00e7ois-Xavier Aubet, Shuang Song, Lorenzo Blanco, Leland Rechis, Lewis Ho, Rich Munoz, Kelvin Zheng, Jessica Hamrick, Kevin Mather, Hagai Taitelbaum, Eliza Rutherford, Yun Lei, Kuangyuan Chen, Anand Shukla, Erica Moreira, Eric Doi, Berivan Isik, Nir Shabat, Dominika Rogozi\u0144ska, Kashyap Kolipaka, Jason Chang, Eugen Vu\u0161ak, Srinivasan Venkatachary, Shadi Noghabi, Tarun Bharti, Younghoon Jun, Aleksandr Zaks, Simon Green, Jeshwanth Challagundla, William Wong, Muqthar Mohammad, Dean Hirsch, Yong Cheng, Iftekhar Naim, Lev Proleev, Damien Vincent, Aayush Singh, Maxim Krikun, Dilip Krishnan, Zoubin Ghahramani, Aviel Atias, Rajeev Aggarwal, Christo Kirov, Dimitrios Vytiniotis, Christy Koh, Alexandra Chronopoulou, Pawan Dogra, Vlad-Doru Ion, Gladys Tyen, Jason Lee, Felix Weissenberger, Trevor Strohman, Ashwin Balakrishna, Jack Rae, Marko Velic, Raoul de Liedekerke, Oded Elyada, Wentao Yuan, Canoee Liu, Lior Shani, Sergey Kishchenko, Bea Alessio, Yandong Li, Richard Song, Sam Kwei, Orion Jankowski, Aneesh Pappu, Youhei Namiki, Yenai Ma, Nilesh Tripuraneni, Colin Cherry, Marissa Ikonomidis, Yu-Cheng Ling, Colin Ji, Beka Westberg, Auriel Wright, Da Yu, David Parkinson, Swaroop Ramaswamy, Jerome Connor, Soheil Hassas Yeganeh, Snchit Grover, George Kenwright, Lubo Litchev, Chris Apps, Alex Tomala, Felix Halim, Alex Castro-Ros, Zefei Li, Anudhyan Boral, Pauline Sho, Michal Yarom, Eric Malmi, David Klinghoffer, Rebecca Lin, Alan Ansell, Pradeep Kumar S, Shubin Zhao, Siqi Zuo, Adam Santoro, Heng-Tze Cheng, Solomon Demmessie, Yuchi Liu, Nicole Brichtova, Allie Culp, Nathaniel Braun, Dan Graur, Will Ng, Nikhil Mehta, Aaron Phillips, Patrik Sundberg, Varun Godbole, Fangyu Liu, Yash Katariya, David Rim, Mojtaba Seyedhosseini, Sean Ammirati, Jonas Valfridsson, Mahan Malihi, Timothy Knight, Andeep Toor, Thomas Lampe, Abe Ittycheriah, Lewis Chiang, Chak Yeung, Alexandre Fr\u00e9chette, Jinmeng Rao, Huisheng Wang, Himanshu Srivastava, Richard Zhang, Rocky Rhodes, Ariel Brand, Dean Weesner, Ilya Figotin, Felix Gimeno, Rachana Fellinger, Pierre Marcenac, Jos\u00e9 Leal, Eyal Marcus, Victor Cotruta, Rodrigo Cabrera, Sheryl Luo, Dan Garrette, Vera Axelrod, Sorin Baltateanu, David Barker, Dongkai Chen, Horia Toma, Ben Ingram, Jason Riesa, Chinmay Kulkarni, Yujing Zhang, Hongbin Liu, Chao Wang, Martin Polacek, Will Wu, Kai Hui, Adrian N Reyes, Yi Su, Megan Barnes, Ishaan Malhi, Anfal Siddiqui, Qixuan Feng, Mihai Damaschin, Daniele Pighin, Andreas Steiner, Samuel Yang, Ramya Sree Boppana, Simeon Ivanov, Arun Kandoor, Aditya Shah, Asier Mujika, Da Huang, Christopher A. Choquette-Choo, Mohak Patel, Tianhe Yu, Toni Creswell, Jerry, Catarina Barros, Yasaman Razeghi, Aurko Roy, Phil Culliton, Binbin Xiong, Jiaqi Pan, Thomas Strohmann, Tolly Powell, Babi Seal, Doug DeCarlo, Pranav Shyam, Kaan Katircioglu, Xuezhi Wang, Cassidy Hardin, Immanuel Odisho, Josef Broder, Oscar Chang, Arun Nair, Artem Shtefan, Maura O'Brien, Manu Agarwal, Sahitya Potluri, Siddharth Goyal, Amit Jhindal, Saksham Thakur, Yury Stuken, James Lyon, Kristina Toutanova, Fangxiaoyu Feng, Austin Wu, Ben Horn, Alek Wang, Alex Cullum, Gabe Taubman, Disha Shrivastava, Chongyang Shi, Hamish Tomlinson, Roma Patel, Tao Tu, Ada Maksutaj Oflazer, Francesco Pongetti, Mingyao Yang, Adrien Ali Ta\u00efga, Vincent Perot, Nuo Wang Pierse, Feng Han, Yoel Drori, I\u00f1aki Iturrate, Ayan Chakrabarti, Legg Yeung, Dave Dopson, Yi-ting Chen, Apoorv Kulshreshtha, Tongfei Guo, Philip Pham, Tal Schuster, Junquan Chen, Alex Polozov, Jinwei Xing, Huanjie Zhou, Praneeth Kacham, Doron Kukliansky, Antoine Miech, Sergey Yaroshenko, Ed Chi, Sholto Douglas, Hongliang Fei, Mathieu Blondel, Preethi Myla, Lior Madmoni, Xing Wu, Daniel Keysers, Kristian Kjems, Isabela Albuquerque, Lijun Yu, Joel D'sa, Michelle Plantan, Vlad Ionescu, Jaume Sanchez Elias, Abhirut Gupta, Manish Reddy Vuyyuru, Fred Alcober, Tong Zhou, Kaiyang Ji, Florian Hartmann, Subha Puttagunta, Hugo Song, Ehsan Amid, Anca Stefanoiu, Andrew Lee, Paul Pucciarelli, Emma Wang, Amit Raul, Slav Petrov, Isaac Tian, Valentin Anklin, Nana Nti, Victor Gomes, Max Schumacher, Grace Vesom, Alex Panagopoulos, Konstantinos Bousmalis, Daniel Andor, Josh Jacob, Yuan Zhang, Bill Rosgen, Matija Kecman, Matthew Tung, Alexandra Belias, Noah Goodman, Paul Covington, Brian Wieder, Nikita Saxena, Elnaz Davoodi, Muhuan Huang, Sharath Maddineni, Vincent Roulet, Folawiyo Campbell-Ajala, Pier Giuseppe Sessa, Xintian, Guangda Lai, Paul Collins, Alex Haig, Vytenis Sakenas, Xiaowei Xu, Marissa Giustina, Laurent El Shafey, Pichi Charoenpanit, Shefali Garg, Joshua Ainslie, Boone Severson, Montse Gonzalez Arenas, Shreya Pathak, Sujee Rajayogam, Jie Feng, Michiel Bakker, Sheng Li, Nevan Wichers, Jamie Rogers, Xinyang Geng, Yeqing Li, Rolf Jagerman, Chao Jia, Nadav Olmert, David Sharon, Matthew Mauger, Sandeep Mariserla, Hongxu Ma, Megha Mohabey, Kyuyeun Kim, Alek Andreev, Scott Pollom, Juliette Love, Vihan Jain, Priyanka Agrawal, Yannick Schroecker, Alisa Fortin, Manfred Warmuth, Ji Liu, Andrew Leach, Irina Blok, Ganesh Poomal Girirajan, Roee Aharoni, Benigno Uria, Andrei Sozanschi, Dan Goldberg, Lucian Ionita, Marco Tulio Ribeiro, Martin Zlocha, Vighnesh Birodkar, Sami Lachgar, Liangzhe Yuan, Himadri Choudhury, Matt Ginsberg, Fei Zheng, Gregory Dibb, Emily Graves, Swachhand Lokhande, Gabriel Rasskin, George-Cristian Muraru, Corbin Quick, Sandeep Tata, Pierre Sermanet, Aditya Chawla, Itay Karo, Yan Wang, Susan Zhang, Orgad Keller, Anca Dragan, Guolong Su, Ian Chou, Xi Liu, Yiqing Tao, Shruthi Prabhakara, Marc Wilson, Ruibo Liu, Shibo Wang, Georgie Evans, David Du, Alfonso Casta\u00f1o, Gautam Prasad, Mona El Mahdy, Sebastian Gerlach, Machel Reid, Jarrod Kahn, Amir Zait, Thanumalayan Sankaranarayana Pillai, Thatcher Ulrich, Guanyu Wang, Jan Wassenberg, Efrat Farkash, Kiran Yalasangi, Congchao Wang, Maria Bauza, Simon Bucher, Ting Liu, Jun Yan, Gary Leung, Vikas Sindhwani, Parker Barnes, Avi Singh, Ivan Jurin, Jichuan Chang, Niket Kumar Bhumihar, Sivan Eiger, Gui Citovsky, Ben Withbroe, Zhang Li, Siyang Xue, Niccol\u00f2 Dal Santo, Georgi Stoyanov, Yves Raimond, Steven Zheng, Yilin Gao, V\u00edt List\u00edk, S\u0142awek Kwasiborski, Rachel Saputro, Adnan Ozturel, Ganesh Mallya, Kushal Majmundar, Ross West, Paul Caron, Jinliang Wei, Lluis Castrejon, Sharad Vikram, Deepak Ramachandran, Nikhil Dhawan, Jiho Park, Sara Smoot, George van den Driessche, Yochai Blau, Chase Malik, Wei Liang, Roy Hirsch, Cicero Nogueira dos Santos, Eugene Weinstein, A\u00e4ron van den Oord, Sid Lall, Nicholas FitzGerald, Zixuan Jiang, Xuan Yang, Dale Webster, Ali Elqursh, Aedan Pope, Georges Rotival, David Raposo, Wanzheng Zhu, Jeff Dean, Sami Alabed, Dustin Tran, Arushi Gupta, Zach Gleicher, Jessica Austin, Edouard Rosseel, Megh Umekar, Dipanjan Das, Yinghao Sun, Kai Chen, Karolis Misiunas, Xiang Zhou, Yixian Di, Alyssa Loo, Josh Newlan, Bo Li, Vinay Ramasesh, Ying Xu, Alex Chen, Sudeep Gandhe, Radu Soricut, Nikita Gupta, Shuguang Hu, Seliem El-Sayed, Xavier Garcia, Idan Brusilovsky, Pu-Chin Chen, Andrew Bolt, Lu Huang, Alex Gurney, Zhiying Zhang, Alexander Pritzel, Jarek Wilkiewicz, Bryan Seybold, Bhargav Kanagal Shamanna, Felix Fischer, Josef Dean, Karan Gill, Ross Mcilroy, Abhishek Bhowmick, Jeremy Selier, Antoine Yang, Derek Cheng, Vladimir Magay, Jie Tan, Dhriti Varma, Christian Walder, Tomas Kocisky, Ryo Nakashima, Paul Natsev, Mike Kwong, Ionel Gog, Chiyuan Zhang, Sander Dieleman, Thomas Jimma, Andrey Ryabtsev, Siddhartha Brahma, David Steiner, Dayou Du, Ante \u017du\u017eul, Mislav \u017dani\u0107, Mukund Raghavachari, Willi Gierke, Zeyu Zheng, Dessie Petrova, Yann Dauphin, Yuchuan Liu, Ido Kessler, Steven Hand, Chris Duvarney, Seokhwan Kim, Hyo Lee, L\u00e9onard Hussenot, Jeffrey Hui, Josh Smith, Deepali Jain, Jiawei Xia, Gaurav Singh Tomar, Keyvan Amiri, Du Phan, Fabian Fuchs, Tobias Weyand, Nenad Tomasev, Alexandra Cordell, Xin Liu, Jonathan Mallinson, Pankaj Joshi, Andy Crawford, Arun Suggala, Steve Chien, Nick Fernando, Mariella Sanchez-Vargas, Duncan Williams, Phil Crone, Xiyang Luo, Igor Karpov, Jyn Shan, Terry Thurk, Robin Strudel, Paul Voigtlaender, Piyush Patil, Tim Dozat, Ali Khodaei, Sahil Singla, Piotr Ambroszczyk, Qiyin Wu, Yifan Chang, Brian Roark, Chaitra Hegde, Tianli Ding, Angelos Filos, Zhongru Wu, Andr\u00e9 Susano Pinto, Shuang Liu, Saarthak Khanna, Aditya Pandey, Siobhan Mcloughlin, Qiujia Li, Sam Haves, Allan Zhou, Elena Buchatskaya, Isabel Leal, Peter de Boursac, Nami Akazawa, Nina Anderson, Terry Chen, Krishna Somandepalli, Chen Liang, Sheela Goenka, Stephanie Winkler, Alexander Grushetsky, Yifan Ding, Jamie Smith, Fan Ye, Jordi Pont-Tuset, Eric Li, Ruichao Li, Tomer Golany, Dawid Wegner, Tao Jiang, Omer Barak, Yuan Shangguan, Eszter V\u00e9rtes, Renee Wong, J\u00f6rg Bornschein, Alex Tudor, Michele Bevilacqua, Tom Schaul, Ankit Singh Rawat, Yang Zhao, Kyriakos Axiotis, Lei Meng, Cory McLean, Jonathan Lai, Jennifer Beattie, Nate Kushman, Yaxin Liu, Blair Kutzman, Fiona Lang, Jingchen Ye, Praneeth Netrapalli, Pushkar Mishra, Myriam Khan, Megha Goel, Rob Willoughby, David Tian, Honglei Zhuang, JD Chen, Zak Tsai, Tasos Kementsietsidis, Arjun Khare, James Keeling, Keyang Xu, Nathan Waters, Florent Altch\u00e9, Ashok Popat, Bhavishya Mittal, David Saxton, Dalia El Badawy, Michael Mathieu, Zheng Zheng, Hao Zhou, Nishant Ranka, Richard Shin, Qingnan Duan, Tim Salimans, Ioana Mihailescu, Uri Shaham, Ming-Wei Chang, Yannis Assael, Nishanth Dikkala, Martin Izzard, Vincent Cohen-Addad, Cat Graves, Vlad Feinberg, Grace Chung, DJ Strouse, Danny Karmon, Sahand Sharifzadeh, Zoe Ashwood, Khiem Pham, Jon Blanton, Alex Vasiloff, Jarred Barber, Mark Geller, Aurick Zhou, Fedir Zubach, Tzu-Kuo Huang, Lei Zhang, Himanshu Gupta, Matt Young, Julia Proskurnia, Ronny Votel, Valentin Gabeur, Gabriel Barcik, Aditya Tripathi, Hongkun Yu, Geng Yan, Beer Changpinyo, Filip Paveti\u0107, Amy Coyle, Yasuhisa Fujii, Jorge Gonzalez Mendez, Tianhao Zhou, Harish Rajamani, Blake Hechtman, Eddie Cao, Da-Cheng Juan, Yi-Xuan Tan, Valentin Dalibard, Yilun Du, Natalie Clay, Kaisheng Yao, Wenhao Jia, Dimple Vijaykumar, Yuxiang Zhou, Xinyi Bai, Wei-Chih Hung, Steven Pecht, Georgi Todorov, Nikhil Khadke, Pramod Gupta, Preethi Lahoti, Arnaud Autef, Karthik Duddu, James Lee-Thorp, Alexander Bykovsky, Tautvydas Misiunas, Sebastian Flennerhag, Santhosh Thangaraj, Jed McGiffin, Zack Nado, Markus Kunesch, Andreas Noever, Amir Hertz, Marco Liang, Victor Stone, Evan Palmer, Samira Daruki, Arijit Pramanik, Siim P\u00f5der, Austin Kyker, Mina Khan, Evgeny Sluzhaev, Marvin Ritter, Avraham Ruderman, Wenlei Zhou, Chirag Nagpal, Kiran Vodrahalli, George Necula, Paul Barham, Ellie Pavlick, Jay Hartford, Izhak Shafran, Long Zhao, Maciej Miku\u0142a, Tom Eccles, Hidetoshi Shimokawa, Kanav Garg, Luke Vilnis, Hanwen Chen, Ilia Shumailov, Kuang-Huei Lee, Abdelrahman Abdelhamed, Meiyan Xie, Vered Cohen, Ester Hlavnova, Dan Malkin, Chawin Sitawarin, James Lottes, Pauline Coquinot, Tianli Yu, Sandeep Kumar, Jingwei Zhang, Aroma Mahendru, Zafarali Ahmed, James Martens, Tao Chen, Aviel Boag, Daiyi Peng, Coline Devin, Arseniy Klimovskiy, Mary Phuong, Danny Vainstein, Jin Xie, Bhuvana Ramabhadran, Nathan Howard, Xinxin Yu, Gitartha Goswami, Jingyu Cui, Sam Shleifer, Mario Pinto, Chih-Kuan Yeh, Ming-Hsuan Yang, Sara Javanmardi, Dan Ethier, Chace Lee, Jordi Orbay, Suyog Kotecha, Carla Bromberg, Pete Shaw, James Thornton, Adi Gerzi Rosenthal, Shane Gu, Matt Thomas, Ian Gemp, Aditya Ayyar, Asahi Ushio, Aarush Selvan, Joel Wee, Chenxi Liu, Maryam Majzoubi, Weiren Yu, Jake Abernethy, Tyler Liechty, Renke Pan, Hoang Nguyen, Qiong, Sarah Perrin, Abhinav Arora, Emily Pitler, Weiyi Wang, Kaushik Shivakumar, Flavien Prost, Ben Limonchik, Jing Wang, Yi Gao, Timothee Cour, Shyamal Buch, Huan Gui, Maria Ivanova, Philipp Neubeck, Kelvin Chan, Lucy Kim, Huizhong Chen, Naman Goyal, Da-Woon Chung, Lu Liu, Yao Su, Anastasia Petrushkina, Jiajun Shen, Armand Joulin, Yuanzhong Xu, Stein Xudong Lin, Yana Kulizhskaya, Ciprian Chelba, Shobha Vasudevan, Eli Collins, Vasilisa Bashlovkina, Tony Lu, Doug Fritz, Jongbin Park, Yanqi Zhou, Chen Su, Richard Tanburn, Mikhail Sushkov, Mitchelle Rasquinha, Jinning Li, Jennifer Prendki, Yiming Li, Pallavi LV, Shriya Sharma, Hen Fitoussi, Hui Huang, Andrew Dai, Phuong Dao, Mike Burrows, Henry Prior, Danfeng Qin, Golan Pundak, Lars Lowe Sjoesund, Art Khurshudov, Zhenkai Zhu, Albert Webson, Elizabeth Kemp, Tat Tan, Saurabh Agrawal, Susie Sargsyan, Liqun Cheng, Jim Stephan, Tom Kwiatkowski, David Reid, Arunkumar Byravan, Assaf Hurwitz Michaely, Nicolas Heess, Luowei Zhou, Sonam Goenka, Viral Carpenter, Anselm Levskaya, Bo Wang, Reed Roberts, R\u00e9mi Leblond, Sharat Chikkerur, Stav Ginzburg, Max Chang, Robert Riachi, Chuqiao, Zal\u00e1n Borsos, Michael Pliskin, Julia Pawar, Morgane Lustman, Hannah Kirkwood, Ankit Anand, Aditi Chaudhary, Norbert Kalb, Kieran Milan, Sean Augenstein, Anna Goldie, Laurel Prince, Karthik Raman, Yanhua Sun, Vivian Xia, Aaron Cohen, Zhouyuan Huo, Josh Camp, Seher Ellis, Lukas Zilka, David Vilar Torres, Lisa Patel, Sho Arora, Betty Chan, Jonas Adler, Kareem Ayoub, Jacky Liang, Fayaz Jamil, Jiepu Jiang, Simon Baumgartner, Haitian Sun, Yael Karov, Yaroslav Akulov, Hui Zheng, Irene Cai, Claudio Fantacci, James Rubin, Alex Rav Acha, Mengchao Wang, Nina D'Souza, Rohit Sathyanarayana, Shengyang Dai, Simon Rowe, Andrey Simanovsky, Omer Goldman, Yuheng Kuang, Xiaoyue Pan, Andrew Rosenberg, Tania Rojas-Esponda, Praneet Dutta, Amy Zeng, Irina Jurenka, Greg Farquhar, Yamini Bansal, Shariq Iqbal, Becca Roelofs, Ga-Young Joung, Parker Beak, Changwan Ryu, Ryan Poplin, Yan Wu, Jean-Baptiste Alayrac, Senaka Buthpitiya, Olaf Ronneberger, Caleb Habtegebriel, Wei Li, Paul Cavallaro, Aurora Wei, Guy Bensky, Timo Denk, Harish Ganapathy, Jeff Stanway, Pratik Joshi, Francesco Bertolini, Jessica Lo, Olivia Ma, Zachary Charles, Geta Sampemane, Himanshu Sahni, Xu Chen, Harry Askham, David Gaddy, Peter Young, Jiewen Tan, Matan Eyal, Arthur Bra\u017einskas, Li Zhong, Zhichun Wu, Mark Epstein, Kai Bailey, Andrew Hard, Kamyu Lee, Sasha Goldshtein, Alex Ruiz, Mohammed Badawi, Matthias Lochbrunner, JK Kearns, Ashley Brown, Fabio Pardo, Theophane Weber, Haichuan Yang, Pan-Pan Jiang, Berkin Akin, Zhao Fu, Marcus Wainwright, Chi Zou, Meenu Gaba, Pierre-Antoine Manzagol, Wendy Kan, Yang Song, Karina Zainullina, Rui Lin, Jeongwoo Ko, Salil Deshmukh, Apoorv Jindal, James Svensson, Divya Tyam, Heri Zhao, Christine Kaeser-Chen, Scott Baird, Pooya Moradi, Jamie Hall, Qiuchen Guo, Vincent Tsang, Bowen Liang, Fernando Pereira, Suhas Ganesh, Ivan Korotkov, Jakub Adamek, Sridhar Thiagarajan, Vinh Tran, Charles Chen, Chris Tar, Sanil Jain, Ishita Dasgupta, Taylan Bilal, David Reitter, Kai Zhao, Giulia Vezzani, Yasmin Gehman, Pulkit Mehta, Lauren Beltrone, Xerxes Dotiwalla, Sergio Guadarrama, Zaheer Abbas, Stefani Karp, Petko Georgiev, Chun-Sung Ferng, Marc Brockschmidt, Liqian Peng, Christoph Hirnschall, Vikas Verma, Yingying Bi, Ying Xiao, Avigail Dabush, Kelvin Xu, Phil Wallis, Randall Parker, Qifei Wang, Yang Xu, Ilkin Safarli, Dinesh Tewari, Yin Zhang, Seungyeon Kim, Andrea Gesmundo, Mackenzie Thomas, Sergey Levi, Ahmed Chowdhury, Kanishka Rao, Peter Garst, Sam Conway-Rahman, Helen Ran, Kay McKinney, Zhisheng Xiao, Wenhao Yu, Rohan Agrawal, Axel Stjerngren, Catalin Ionescu, Jingjing Chen, Vivek Sharma, Justin Chiu, Fei Liu, Ken Franko, Clayton Sanford, Xingyu Cai, Paul Michel, Sanjay Ganapathy, Jane Labanowski, Zachary Garrett, Ben Vargas, Sean Sun, Bryan Gale, Thomas Buschmann, Guillaume Desjardins, Nimesh Ghelani, Palak Jain, Mudit Verma, Chulayuth Asawaroengchai, Julian Eisenschlos, Jitendra Harlalka, Hideto Kazawa, Don Metzler, Joshua Howland, Ying Jian, Jake Ades, Viral Shah, Tynan Gangwani, Seungji Lee, Roman Ring, Steven M. Hernandez, Dean Reich, Amer Sinha, Ashutosh Sathe, Joe Kovac, Ashleah Gill, Ajay Kannan, Andrea D'olimpio, Martin Sevenich, Jay Whang, Been Kim, Khe Chai Sim, Jilin Chen, Jiageng Zhang, Shuba Lall, Yossi Matias, Bill Jia, Abe Friesen, Sara Nasso, Ashish Thapliyal, Bryan Perozzi, Ting Yu, Anna Shekhawat, Safeen Huda, Peter Grabowski, Eric Wang, Ashwin Sreevatsa, Hilal Dib, Mehadi Hassen, Parker Schuh, Vedrana Milutinovic, Chris Welty, Michael Quinn, Ali Shah, Bangju Wang, Gabe Barth-Maron, Justin Frye, Natalie Axelsson, Tao Zhu, Yukun Ma, Irene Giannoumis, Hanie Sedghi, Chang Ye, Yi Luan, Kevin Aydin, Bilva Chandra, Vivek Sampathkumar, Ronny Huang, Victor Lavrenko, Ahmed Eleryan, Zhi Hong, Steven Hansen, Sara Mc Carthy, Bidisha Samanta, Domagoj \u0106evid, Xin Wang, Fangtao Li, Michael Voznesensky, Matt Hoffman, Andreas Terzis, Vikash Sehwag, Gil Fidel, Luheng He, Mu Cai, Yanzhang He, Alex Feng, Martin Nikoltchev, Samrat Phatale, Jason Chase, Rory Lawton, Ming Zhang, Tom Ouyang, Manuel Tragut, Mehdi Hafezi Manshadi, Arjun Narayanan, Jiaming Shen, Xu Gao, Tolga Bolukbasi, Nick Roy, Xin Li, Daniel Golovin, Liviu Panait, Zhen Qin, Guangxing Han, Thomas Anthony, Sneha Kudugunta, Viorica Patraucean, Aniket Ray, Xinyun Chen, Xiaochen Yang, Tanuj Bhatia, Pranav Talluri, Alex Morris, Andrija Ra\u017enatovi\u0107, Bethanie Brownfield, James An, Sheng Peng, Patrick Kane, Ce Zheng, Nico Duduta, Joshua Kessinger, James Noraky, Siqi Liu, Keran Rong, Petar Veli\u010dkovi\u0107, Keith Rush, Alex Goldin, Fanny Wei, Shiva Mohan Reddy Garlapati, Caroline Pantofaru, Okwan Kwon, Jianmo Ni, Eric Noland, Julia Di Trapani, Fran\u00e7oise Beaufays, Abhijit Guha Roy, Yinlam Chow, Aybuke Turker, Geoffrey Cideron, Lantao Mei, Jon Clark, Qingyun Dou, Matko Bo\u0161njak, Ralph Leith, Yuqing Du, Amir Yazdanbakhsh, Milad Nasr, Chester Kwak, Suraj Satishkumar Sheth, Alex Kaskasoli, Ankesh Anand, Balaji Lakshminarayanan, Sammy Jerome, David Bieber, Chun-Te Chu, Alexandre Senges, Tianxiao Shen, Mukund Sridhar, Ndaba Ndebele, Benjamin Beyret, Shakir Mohamed, Mia Chen, Markus Freitag, Jiaxian Guo, Luyang Liu, Paul Roit, Heng Chen, Shen Yan, Tom Stone, JD Co-Reyes, Jeremy Cole, Salvatore Scellato, Shekoofeh Azizi, Hadi Hashemi, Alicia Jin, Anand Iyer, Marcella Valentine, Andr\u00e1s Gy\u00f6rgy, Arun Ahuja, Daniel Hernandez Diaz, Chen-Yu Lee, Nathan Clement, Weize Kong, Drew Garmon, Ishaan Watts, Kush Bhatia, Khyatti Gupta, Matt Miecnikowski, Hugo Vallet, Ankur Taly, Edward Loper, Saket Joshi, James Atwood, Jo Chick, Mark Collier, Fotis Iliopoulos, Ryan Trostle, Beliz Gunel, Ramiro Leal-Cavazos, Arnar Mar Hrafnkelsson, Michael Guzman, Xiaoen Ju, Andy Forbes, Jesse Emond, Kushal Chauhan, Ben Caine, Li Xiao, Wenjun Zeng, Alexandre Moufarek, Daniel Murphy, Maya Meng, Nitish Gupta, Felix Riedel, Anil Das, Elijah Lawal, Shashi Narayan, Tiberiu Sosea, James Swirhun, Linda Friso, Behnam Neyshabur, Jing Lu, Sertan Girgin, Michael Wunder, Edouard Yvinec, Aroonalok Pyne, Victor Carbune, Shruti Rijhwani, Yang Guo, Tulsee Doshi, Anton Briukhov, Max Bain, Ayal Hitron, Xuanhui Wang, Ashish Gupta, Ke Chen, Cosmo Du, Weiyang Zhang, Dhruv Shah, Arjun Akula, Max Dylla, Ashyana Kachra, Weicheng Kuo, Tingting Zou, Lily Wang, Luyao Xu, Jifan Zhu, Justin Snyder, Sachit Menon, Orhan Firat, Igor Mordatch, Yuan Yuan, Natalia Ponomareva, Rory Blevins, Lawrence Moore, Weijun Wang, Phil Chen, Martin Scholz, Artur Dwornik, Jason Lin, Sicheng Li, Diego Antognini, Te I, Xiaodan Song, Matt Miller, Uday Kalra, Adam Raveret, Oscar Akerlund, Felix Wu, Andrew Nystrom, Namrata Godbole, Tianqi Liu, Hannah DeBalsi, Jewel Zhao, Buhuang Liu, Avi Caciularu, Lauren Lax, Urvashi Khandelwal, Victoria Langston, Eric Bailey, Silvio Lattanzi, Yufei Wang, Neel Kovelamudi, Sneha Mondal, Guru Guruganesh, Nan Hua, Ofir Roval, Pawe\u0142 Weso\u0142owski, Rishikesh Ingale, Jonathan Halcrow, Tim Sohn, Christof Angermueller, Bahram Raad, Eli Stickgold, Eva Lu, Alec Kosik, Jing Xie, Timothy Lillicrap, Austin Huang, Lydia Lihui Zhang, Dominik Paulus, Clement Farabet, Alex Wertheim, Bing Wang, Rishabh Joshi, Chu-ling Ko, Yonghui Wu, Shubham Agrawal, Lily Lin, XiangHai Sheng, Peter Sung, Tyler Breland-King, Christina Butterfield, Swapnil Gawde, Sumeet Singh, Qiao Zhang, Raj Apte, Shilpa Shetty, Adrian Hutter, Tao Li, Elizabeth Salesky, Federico Lebron, Jonni Kanerva, Michela Paganini, Arthur Nguyen, Rohith Vallu, Jan-Thorsten Peter, Sarmishta Velury, David Kao, Jay Hoover, Anna Bortsova, Colton Bishop, Shoshana Jakobovits, Alessandro Agostini, Alekh Agarwal, Chang Liu, Charles Kwong, Sasan Tavakkol, Ioana Bica, Alex Greve, Anirudh GP, Jake Marcus, Le Hou, Tom Duerig, Rivka Moroshko, Dave Lacey, Andy Davis, Julien Amelot, Guohui Wang, Frank Kim, Theofilos Strinopoulos, Hui Wan, Charline Le Lan, Shankar Krishnan, Haotian Tang, Peter Humphreys, Junwen Bai, Idan Heimlich Shtacher, Diego Machado, Chenxi Pang, Ken Burke, Dangyi Liu, Renga Aravamudhan, Yue Song, Ed Hirst, Abhimanyu Singh, Brendan Jou, Liang Bai, Francesco Piccinno, Chuyuan Kelly Fu, Robin Alazard, Barak Meiri, Daniel Winter, Charlie Chen, Mingda Zhang, Jens Heitkaemper, John Lambert, Jinhyuk Lee, Alexander Fr\u00f6mmgen, Sergey Rogulenko, Pranav Nair, Paul Niemczyk, Anton Bulyenov, Bibo Xu, Hadar Shemtov, Morteza Zadimoghaddam, Serge Toropov, Mateo Wirth, Hanjun Dai, Sreenivas Gollapudi, Daniel Zheng, Alex Kurakin, Chansoo Lee, Kalesha Bullard, Nicolas Serrano, Ivana Balazevic, Yang Li, Johan Schalkwyk, Mark Murphy, Mingyang Zhang, Kevin Sequeira, Romina Datta, Nishant Agrawal, Charles Sutton, Nithya Attaluri, Mencher Chiang, Wael Farhan, Gregory Thornton, Kate Lin, Travis Choma, Hung Nguyen, Kingshuk Dasgupta, Dirk Robinson, Iulia Com\u015fa, Michael Riley, Arjun Pillai, Basil Mustafa, Ben Golan, Amir Zandieh, Jean-Baptiste Lespiau, Billy Porter, David Ross, Sujeevan Rajayogam, Mohit Agarwal, Subhashini Venugopalan, Bobak Shahriari, Qiqi Yan, Hao Xu, Taylor Tobin, Pavel Dubov, Hongzhi Shi, Adri\u00e0 Recasens, Anton Kovsharov, Sebastian Borgeaud, Lucio Dery, Shanthal Vasanth, Elena Gribovskaya, Linhai Qiu, Mahdis Mahdieh, Wojtek Skut, Elizabeth Nielsen, CJ Zheng, Adams Yu, Carrie Grimes Bostock, Shaleen Gupta, Aaron Archer, Chris Rawles, Elinor Davies, Alexey Svyatkovskiy, Tomy Tsai, Yoni Halpern, Christian Reisswig, Bartek Wydrowski, Bo Chang, Joan Puigcerver, Mor Hazan Taege, Jian Li, Eva Schnider, Xinjian Li, Dragos Dena, Yunhan Xu, Umesh Telang, Tianze Shi, Heiga Zen, Kyle Kastner, Yeongil Ko, Neesha Subramaniam, Aviral Kumar, Pete Blois, Zhuyun Dai, John Wieting, Yifeng Lu, Yoel Zeldes, Tian Xie, Anja Hauth, Alexandru \u0162ifrea, Yuqi Li, Sam El-Husseini, Dan Abolafia, Howard Zhou, Wen Ding, Sahra Ghalebikesabi, Carlos Gu\u00eda, Andrii Maksai, \u00c1goston Weisz, Sercan Arik, Nick Sukhanov, Aga \u015awietlik, Xuhui Jia, Luo Yu, Weiyue Wang, Mark Brand, Dawn Bloxwich, Sean Kirmani, Zhe Chen, Alec Go, Pablo Sprechmann, Nithish Kannen, Alen Carin, Paramjit Sandhu, Isabel Edkins, Leslie Nooteboom, Jai Gupta, Loren Maggiore, Javad Azizi, Yael Pritch, Pengcheng Yin, Mansi Gupta, Danny Tarlow, Duncan Smith, Desi Ivanov, Mohammad Babaeizadeh, Ankita Goel, Satish Kambala, Grace Chu, Matej Kastelic, Michelle Liu, Hagen Soltau, Austin Stone, Shivani Agrawal, Min Kim, Kedar Soparkar, Srinivas Tadepalli, Oskar Bunyan, Rachel Soh, Arvind Kannan, DY Kim, Blake JianHang Chen, Afief Halumi, Sudeshna Roy, Yulong Wang, Olcan Sercinoglu, Gena Gibson, Sijal Bhatnagar, Motoki Sano, Daniel von Dincklage, Qingchun Ren, Blagoj Mitrevski, Mirek Ol\u0161\u00e1k, Jennifer She, Carl Doersch, Jilei, Wang, Bingyuan Liu, Qijun Tan, Tamar Yakar, Tris Warkentin, Alex Ramirez, Carl Lebsack, Josh Dillon, Rajiv Mathews, Tom Cobley, Zelin Wu, Zhuoyuan Chen, Jon Simon, Swaroop Nath, Tara Sainath, Alexei Bendebury, Ryan Julian, Bharath Mankalale, Daria \u0106urko, Paulo Zacchello, Adam R. Brown, Kiranbir Sodhia, Heidi Howard, Sergi Caelles, Abhinav Gupta, Gareth Evans, Anna Bulanova, Lesley Katzen, Roman Goldenberg, Anton Tsitsulin, Joe Stanton, Benoit Schillings, Vitaly Kovalev, Corey Fry, Rushin Shah, Kuo Lin, Shyam Upadhyay, Cheng Li, Soroush Radpour, Marcello Maggioni, Jing Xiong, Lukas Haas, Jenny Brennan, Aishwarya Kamath, Nikolay Savinov, Arsha Nagrani, Trevor Yacovone, Ryan Kappedal, Kostas Andriopoulos, Li Lao, YaGuang Li, Grigory Rozhdestvenskiy, Kazuma Hashimoto, Andrew Audibert, Sophia Austin, Daniel Rodriguez, Anian Ruoss, Garrett Honke, Deep Karkhanis, Xi Xiong, Qing Wei, James Huang, Zhaoqi Leng, Vittal Premachandran, Stan Bileschi, Georgios Evangelopoulos, Thomas Mensink, Jay Pavagadhi, Denis Teplyashin, Paul Chang, Linting Xue, Garrett Tanzer, Sally Goldman, Kaushal Patel, Shixin Li, Jeremy Wiesner, Ivy Zheng, Ian Stewart-Binks, Jie Han, Zhi Li, Liangchen Luo, Karel Lenc, Mario Lu\u010di\u0107, Fuzhao Xue, Ryan Mullins, Alexey Guseynov, Chung-Ching Chang, Isaac Galatzer-Levy, Adam Zhang, Garrett Bingham, Grace Hu, Ale Hartman, Yue Ma, Jordan Griffith, Alex Irpan, Carey Radebaugh, Summer Yue, Lijie Fan, Victor Ungureanu, Christina Sorokin, Hannah Teufel, Peiran Li, Rohan Anil, Dimitris Paparas, Todd Wang, Chu-Cheng Lin, Hui Peng, Megan Shum, Goran Petrovic, Demetra Brady, Richard Nguyen, Klaus Macherey, Zhihao Li, Harman Singh, Madhavi Yenugula, Mariko Iinuma, Xinyi Chen, Kavya Kopparapu, Alexey Stern, Shachi Dave, Chandu Thekkath, Florence Perot, Anurag Kumar, Fangda Li, Yang Xiao, Matthew Bilotti, Mohammad Hossein Bateni, Isaac Noble, Lisa Lee, Amelio V\u00e1zquez-Reina, Julian Salazar, Xiaomeng Yang, Boyu Wang, Ela Gruzewska, Anand Rao, Sindhu Raghuram, Zheng Xu, Eyal Ben-David, Jieru Mei, Sid Dalmia, Zhaoyi Zhang, Yuchen Liu, Gagan Bansal, Helena Pankov, Steven Schwarcz, Andrea Burns, Christine Chan, Sumit Sanghai, Ricky Liang, Ethan Liang, Antoine He, Amy Stuart, Arun Narayanan, Yukun Zhu, Christian Frank, Bahar Fatemi, Amit Sabne, Oran Lang, Indro Bhattacharya, Shane Settle, Maria Wang, Brendan McMahan, Andrea Tacchetti, Livio Baldini Soares, Majid Hadian, Serkan Cabi, Timothy Chung, Nikita Putikhin, Gang Li, Jeremy Chen, Austin Tarango, Henryk Michalewski, Mehran Kazemi, Hussain Masoom, Hila Sheftel, Rakesh Shivanna, Archita Vadali, Ramona Comanescu, Doug Reid, Joss Moore, Arvind Neelakantan, Micha\u00ebl Sander, Jonathan Herzig, Aviv Rosenberg, Mostafa Dehghani, JD Choi, Michael Fink, Reid Hayes, Eric Ge, Shitao Weng, Chia-Hua Ho, John Karro, Kalpesh Krishna, Lam Nguyen Thiet, Amy Skerry-Ryan, Daniel Eppens, Marco Andreetto, Navin Sarma, Silvano Bonacina, Burcu Karagol Ayan, Megha Nawhal, Zhihao Shan, Mike Dusenberry, Shantanu Thakoor, Sagar Gubbi, Duc Dung Nguyen, Reut Tsarfaty, Samuel Albanie, Jovana Mitrovi\u0107, Meet Gandhi, Bo-Juen Chen, Alessandro Epasto, Georgi Stephanov, Ye Jin, Samuel Gehman, Aida Amini, Jack Weber, Feryal Behbahani, Shawn Xu, Miltos Allamanis, Xi Chen, Myle Ott, Claire Sha, Michal Jastrzebski, Hang Qi, David Greene, Xinyi Wu, Abodunrinwa Toki, Daniel Vlasic, Jane Shapiro, Ragha Kotikalapudi, Zhe Shen, Takaaki Saeki, Sirui Xie, Albin Cassirer, Shikhar Bharadwaj, Tatsuya Kiyono, Srinadh Bhojanapalli, Elan Rosenfeld, Sam Ritter, Jieming Mao, Jo\u00e3o Gabriel Oliveira, Zoltan Egyed, Bernd Bandemer, Emilio Parisotto, Keisuke Kinoshita, Juliette Pluto, Petros Maniatis, Steve Li, Yaohui Guo, Golnaz Ghiasi, Jean Tarbouriech, Srimon Chatterjee, Julie Jin, Katrina, Jennimaria Palomaki, S\u00e9b Arnold, Madhavi Sewak, Federico Piccinini, Mohit Sharma, Ben Albrecht, Sean Purser-haskell, Ashwin Vaswani, Chongyan Chen, Matheus Wisniewski, Qin Cao, John Aslanides, Nguyet Minh Phu, Maximilian Sieb, Lauren Agubuzu, Anne Zheng, Daniel Sohn, Marco Selvi, Anders Andreassen, Krishan Subudhi, Prem Eruvbetine, Oliver Woodman, Tomas Mery, Sebastian Krause, Xiaoqi Ren, Xiao Ma, Jincheng Luo, Dawn Chen, Wei Fan, Henry Griffiths, Christian Schuler, Alice Li, Shujian Zhang, Jean-Michel Sarr, Shixin Luo, Riccardo Patana, Matthew Watson, Dani Naboulsi, Michael Collins, Sailesh Sidhwani, Emiel Hoogeboom, Sharon Silver, Emily Caveness, Xiaokai Zhao, Mikel Rodriguez, Maxine Deines, Libin Bai, Patrick Griffin, Marco Tagliasacchi, Emily Xue, Spandana Raj Babbula, Bo Pang, Nan Ding, Gloria Shen, Elijah Peake, Remi Crocker, Shubha Srinivas Raghvendra, Danny Swisher, Woohyun Han, Richa Singh, Ling Wu, Vladimir Pchelin, Tsendsuren Munkhdalai, Dana Alon, Geoff Bacon, Efren Robles, Jannis Bulian, Melvin Johnson, George Powell, Felipe Tiengo Ferreira, Yaoyiran Li, Frederik Benzing, Mihajlo Velimirovi\u0107, Hubert Soyer, William Kong, Tony, Nguy\u00ean, Zhen Yang, Jeremiah Liu, Joost van Amersfoort, Daniel Gillick, Baochen Sun, Nathalie Rauschmayr, Katie Zhang, Serena Zhan, Tao Zhou, Alexey Frolov, Chengrun Yang, Denis Vnukov, Louis Rouillard, Hongji Li, Amol Mandhane, Nova Fallen, Rajesh Venkataraman, Clara Huiyi Hu, Jennifer Brennan, Jenny Lee, Jerry Chang, Martin Sundermeyer, Zhufeng Pan, Rosemary Ke, Simon Tong, Alex Fabrikant, William Bono, Jindong Gu, Ryan Foley, Yiran Mao, Manolis Delakis, Dhruva Bhaswar, Roy Frostig, Nick Li, Avital Zipori, Cath Hope, Olga Kozlova, Swaroop Mishra, Josip Djolonga, Craig Schiff, Majd Al Merey, Eleftheria Briakou, Peter Morgan, Andy Wan, Avinatan Hassidim, RJ Skerry-Ryan, Kuntal Sengupta, Mary Jasarevic, Praveen Kallakuri, Paige Kunkle, Hannah Brennan, Tom Lieber, Hassan Mansoor, Julian Walker, Bing Zhang, Annie Xie, Goran \u017du\u017ei\u0107, Adaeze Chukwuka, Alex Druinsky, Donghyun Cho, Rui Yao, Ferjad Naeem, Shiraz Butt, Eunyoung Kim, Zhipeng Jia, Mandy Jordan, Adam Lelkes, Mark Kurzeja, Sophie Wang, James Zhao, Andrew Over, Abhishek Chakladar, Marcel Prasetya, Neha Jha, Sriram Ganapathy, Yale Cong, Prakash Shroff, Carl Saroufim, Sobhan Miryoosefi, Mohamed Hammad, Tajwar Nasir, Weijuan Xi, Yang Gao, Young Maeng, Ben Hora, Chin-Yi Cheng, Parisa Haghani, Yoad Lewenberg, Caden Lu, Martin Matysiak, Naina Raisinghani, Huiyu Wang, Lexi Baugher, Rahul Sukthankar, Minh Giang, John Schultz, Noah Fiedel, Minmin Chen, Cheng-Chun Lee, Tapomay Dey, Hao Zheng, Shachi Paul, Celine Smith, Andy Ly, Yicheng Wang, Rishabh Bansal, Bartek Perz, Susanna Ricco, Stasha Blank, Vaishakh Keshava, Deepak Sharma, Marvin Chow, Kunal Lad, Komal Jalan, Simon Osindero, Craig Swanson, Jacob Scott, Anastasija Ili\u0107, Xiaowei Li, Siddhartha Reddy Jonnalagadda, Afzal Shama Soudagar, Yan Xiong, Bat-Orgil Batsaikhan, Daniel Jarrett, Naveen Kumar, Maulik Shah, Matt Lawlor, Austin Waters, Mark Graham, Rhys May, Sabela Ramos, Sandra Lefdal, Zeynep Cankara, Nacho Cano, Brendan O'Donoghue, Jed Borovik, Frederick Liu, Jordan Grimstad, Mahmoud Alnahlawi, Katerina Tsihlas, Tom Hudson, Nikolai Grigorev, Yiling Jia, Terry Huang, Tobenna Peter Igwe, Sergei Lebedev, Xiaodan Tang, Igor Krivokon, Frankie Garcia, Melissa Tan, Eric Jia, Peter Stys, Shikhar Vashishth, Yu Liang, Balaji Venkatraman, Chenjie Gu, Anastasios Kementsietsidis, Chen Zhu, Junehyuk Jung, Yunfei Bai, Mohammad Javad Hosseini, Faruk Ahmed, Aditya Gupta, Xin Yuan, Shereen Ashraf, Shitij Nigam, Gautam Vasudevan, Pranjal Awasthi, Adi Mayrav Gilady, Zelda Mariet, Ramy Eskander, Haiguang Li, Hexiang Hu, Guillermo Garrido, Philippe Schlattner, George Zhang, Rohun Saxena, Petar Devi\u0107, Kritika Muralidharan, Ashwin Murthy, Yiqian Zhou, Min Choi, Arissa Wongpanich, Zhengdong Wang, Premal Shah, Yuntao Xu, Yiling Huang, Stephen Spencer, Alice Chen, James Cohan, Junjie Wang, Jonathan Tompson, Junru Wu, Ruba Haroun, Haiqiong Li, Blanca Huergo, Fan Yang, Tongxin Yin, James Wendt, Michael Bendersky, Rahma Chaabouni, Javier Snaider, Johan Ferret, Abhishek Jindal, Tara Thompson, Andrew Xue, Will Bishop, Shubham Milind Phal, Archit Sharma, Yunhsuan Sung, Prabakar Radhakrishnan, Mo Shomrat, Reeve Ingle, Roopali Vij, Justin Gilmer, Mihai Dorin Istin, Sam Sobell, Yang Lu, Emily Nottage, Dorsa Sadigh, Jeremiah Willcock, Tingnan Zhang, Steve Xu, Sasha Brown, Katherine Lee, Gary Wang, Yun Zhu, Yi Tay, Cheolmin Kim, Audrey Gutierrez, Abhanshu Sharma, Yongqin Xian, Sungyong Seo, Claire Cui, Elena Pochernina, Cip Baetu, Krzysztof Jastrz\u0119bski, Mimi Ly, Mohamed Elhawaty, Dan Suh, Eren Sezener, Pidong Wang, Nancy Yuen, George Tucker, Jiahao Cai, Zuguang Yang, Cindy Wang, Alex Muzio, Hai Qian, Jae Yoo, Derek Lockhart, Kevin R. McKee, Mandy Guo, Malika Mehrotra, Artur Mendon\u00e7a, Sanket Vaibhav Mehta, Sherry Ben, Chetan Tekur, Jiaqi Mu, Muye Zhu, Victoria Krakovna, Hongrae Lee, AJ Maschinot, S\u00e9bastien Cevey, HyunJeong Choe, Aijun Bai, Hansa Srinivasan, Derek Gasaway, Nick Young, Patrick Siegler, Dan Holtmann-Rice, Vihari Piratla, Kate Baumli, Roey Yogev, Alex Hofer, Hado van Hasselt, Svetlana Grant, Yuri Chervonyi, David Silver, Andrew Hogue, Ayushi Agarwal, Kathie Wang, Preeti Singh, Four Flynn, Josh Lipschultz, Robert David, Lizzetth Bellot, Yao-Yuan Yang, Long Le, Filippo Graziano, Kate Olszewska, Kevin Hui, Akanksha Maurya, Nikos Parotsidis, Weijie Chen, Tayo Oguntebi, Joe Kelley, Anirudh Baddepudi, Johannes Mauerer, Gregory Shaw, Alex Siegman, Lin Yang, Shravya Shetty, Subhrajit Roy, Yunting Song, Wojciech Stokowiec, Ryan Burnell, Omkar Savant, Robert Busa-Fekete, Jin Miao, Samrat Ghosh, Liam MacDermed, Phillip Lippe, Mikhail Dektiarev, Zach Behrman, Fabian Mentzer, Kelvin Nguyen, Meng Wei, Siddharth Verma, Chris Knutsen, Sudeep Dasari, Zhipeng Yan, Petr Mitrichev, Xingyu Wang, Virat Shejwalkar, Jacob Austin, Srinivas Sunkara, Navneet Potti, Yan Virin, Christian Wright, Ga\u00ebl Liu, Oriana Riva, Etienne Pot, Greg Kochanski, Quoc Le, Gargi Balasubramaniam, Arka Dhar, Yuguo Liao, Adam Bloniarz, Divyansh Shukla, Elizabeth Cole, Jong Lee, Sheng Zhang, Sushant Kafle, Siddharth Vashishtha, Parsa Mahmoudieh, Grace Chen, Raphael Hoffmann, Pranesh Srinivasan, Agustin Dal Lago, Yoav Ben Shalom, Zi Wang, Michael Elabd, Anuj Sharma, Junhyuk Oh, Suraj Kothawade, Maigo Le, Marianne Monteiro, Shentao Yang, Kaiz Alarakyia, Robert Geirhos, Diana Mincu, H\u00e5vard Garnes, Hayato Kobayashi, Soroosh Mariooryad, Kacper Krasowiak, Zhixin, Shibl Mourad, Mingqiu Wang, Fan Bu, Ophir Aharoni, Guanjie Chen, Abhimanyu Goyal, Vadim Zubov, Ankur Bapna, Elahe Dabir, Nisarg Kothari, Kay Lamerigts, Nicola De Cao, Jeremy Shar, Christopher Yew, Nitish Kulkarni, Dre Mahaarachchi, Mandar Joshi, Zhenhai Zhu, Jared Lichtarge, Yichao Zhou, Hannah Muckenhirn, Vittorio Selo, Oriol Vinyals, Peter Chen, Anthony Brohan, Vaibhav Mehta, Sarah Cogan, Ruth Wang, Ty Geri, Wei-Jen Ko, Wei Chen, Fabio Viola, Keshav Shivam, Lisa Wang, Madeleine Clare Elish, Raluca Ada Popa, S\u00e9bastien Pereira, Jianqiao Liu, Raphael Koster, Donnie Kim, Gufeng Zhang, Sayna Ebrahimi, Partha Talukdar, Yanyan Zheng, Petra Poklukar, Ales Mikhalap, Dale Johnson, Anitha Vijayakumar, Mark Omernick, Matt Dibb, Ayush Dubey, Qiong Hu, Apurv Suman, Vaibhav Aggarwal, Ilya Kornakov, Fei Xia, Wing Lowe, Alexey Kolganov, Ted Xiao, Vitaly Nikolaev, Steven Hemingray, Bonnie Li, Joana Iljazi, Miko\u0142aj Rybi\u0144ski, Ballie Sandhu, Peggy Lu, Thang Luong, Rodolphe Jenatton, Vineetha Govindaraj, Gabriel Dulac-Arnold, Wonpyo Park, Henry Wang, Abhinit Modi, Jean Pouget-Abadie, Kristina Greller, Rahul Gupta, Robert Berry, Prajit Ramachandran, Jinyu Xie, Liam McCafferty, Jianling Wang, Kilol Gupta, Hyeontaek Lim, Bla\u017e Bratani\u010d, Andy Brock, Ilia Akolzin, Jim Sproch, Dan Karliner, Duhyeon Kim, Adrian Goedeckemeyer, Noam Shazeer, Cordelia Schmid, Daniele Calandriello, Parul Bhatia, Krzysztof Choromanski, Ceslee Montgomery, Dheeru Dua, Ana Ramalho, Helen King, Yue Gao, Lynn Nguyen, David Lindner, Divya Pitta, Oleaser Johnson, Khalid Salama, Diego Ardila, Michael Han, Erin Farnese, Seth Odoom, Ziyue Wang, Xiangzhuo Ding, Norman Rink, Ray Smith, Harshal Tushar Lehri, Eden Cohen, Neera Vats, Tong He, Parthasarathy Gopavarapu, Adam Paszke, Miteyan Patel, Wouter Van Gansbeke, Lucia Loher, Luis Castro, Maria Voitovich, Tamara von Glehn, Nelson George, Simon Niklaus, Zach Eaton-Rosen, Nemanja Raki\u0107evi\u0107, Erik Jue, Sagi Perel, Carrie Zhang, Yuval Bahat, Ang\u00e9line Pouget, Zhi Xing, Fantine Huot, Ashish Shenoy, Taylor Bos, Vincent Coriou, Bryan Richter, Natasha Noy, Yaqing Wang, Santiago Ontanon, Siyang Qin, Gleb Makarchuk, Demis Hassabis, Zhuowan Li, Mandar Sharma, Kumaran Venkatesan, Iurii Kemaev, Roxanne Daniel, Shiyu Huang, Saloni Shah, Octavio Ponce, Warren, Chen, Manaal Faruqui, Jialin Wu, Slavica Anda\u010di\u0107, Szabolcs Payrits, Daniel McDuff, Tom Hume, Yuan Cao, MH Tessler, Qingze Wang, Yinan Wang, Ivor Rendulic, Eirikur Agustsson, Matthew Johnson, Tanya Lando, Andrew Howard, Sri Gayatri Sundara Padmanabhan, Mayank Daswani, Andrea Banino, Michael Kilgore, Jonathan Heek, Ziwei Ji, Alvaro Caceres, Conglong Li, Nora Kassner, Alexey Vlaskin, Zeyu Liu, Alex Grills, Yanhan Hou, Roykrong Sukkerd, Gowoon Cheon, Nishita Shetty, Larisa Markeeva, Piotr Stanczyk, Tejas Iyer, Yuan Gong, Shawn Gao, Keerthana Gopalakrishnan, Tim Blyth, Malcolm Reynolds, Avishkar Bhoopchand, Misha Bilenko, Dero Gharibian, Vicky Zayats, Aleksandra Faust, Abhinav Singh, Min Ma, Hongyang Jiao, Sudheendra Vijayanarasimhan, Lora Aroyo, Vikas Yadav, Sarah Chakera, Ashwin Kakarla, Vilobh Meshram, Karol Gregor, Gabriela Botea, Evan Senter, Dawei Jia, Geza Kovacs, Neha Sharma, Sebastien Baur, Kai Kang, Yifan He, Lin Zhuo, Marija Kostelac, Itay Laish, Songyou Peng, Louis O'Bryan, Daniel Kasenberg, Girish Ramchandra Rao, Edouard Leurent, Biao Zhang, Sage Stevens, Ana Salazar, Ye Zhang, Ivan Lobov, Jake Walker, Allen Porter, Morgan Redshaw, Han Ke, Abhishek Rao, Alex Lee, Hoi Lam, Michael Moffitt, Jaeyoun Kim, Siyuan Qiao, Terry Koo, Robert Dadashi, Xinying Song, Mukund Sundararajan, Peng Xu, Chizu Kawamoto, Yan Zhong, Clara Barbu, Apoorv Reddy, Mauro Verzetti, Leon Li, George Papamakarios, Hanna Klimczak-Pluci\u0144ska, Mary Cassin, Koray Kavukcuoglu, Rigel Swavely, Alain Vaucher, Jeffrey Zhao, Ross Hemsley, Michael Tschannen, Heming Ge, Gaurav Menghani, Yang Yu, Natalie Ha, Wei He, Xiao Wu, Maggie Song, Rachel Sterneck, Stefan Zinke, Dan A. Calian, Annie Marsden, Alejandro Cruzado Ruiz, Matteo Hessel, Almog Gueta, Benjamin Lee, Brian Farris, Manish Gupta, Yunjie Li, Mohammad Saleh, Vedant Misra, Kefan Xiao, Piermaria Mendolicchio, Gavin Buttimore, Varvara Krayvanova, Nigamaa Nayakanti, Matthew Wiethoff, Yash Pande, Azalia Mirhoseini, Ni Lao, Jasmine Liu, Yiqing Hua, Angie Chen, Yury Malkov, Dmitry Kalashnikov, Shubham Gupta, Kartik Audhkhasi, Yuexiang Zhai, Sudhindra Kopalle, Prateek Jain, Eran Ofek, Clemens Meyer, Khuslen Baatarsukh, Hana Strej\u010dek, Jun Qian, James Freedman, Ricardo Figueira, Michal Sokolik, Olivier Bachem, Raymond Lin, Dia Kharrat, Chris Hidey, Pingmei Xu, Dennis Duan, Yin Li, Muge Ersoy, Richard Everett, Kevin Cen, Rebeca Santamaria-Fernandez, Amir Taubenfeld, Ian Mackinnon, Linda Deng, Polina Zablotskaia, Shashank Viswanadha, Shivanker Goel, Damion Yates, Yunxiao Deng, Peter Choy, Mingqing Chen, Abhishek Sinha, Alex Mossin, Yiming Wang, Arthur Szlam, Susan Hao, Paul Kishan Rubenstein, Metin Toksoz-Exley, Miranda Aperghis, Yin Zhong, Junwhan Ahn, Michael Isard, Olivier Lacombe, Florian Luisier, Chrysovalantis Anastasiou, Yogesh Kalley, Utsav Prabhu, Emma Dunleavy, Shaan Bijwadia, Justin Mao-Jones, Kelly Chen, Rama Pasumarthi, Emily Wood, Adil Dostmohamed, Nate Hurley, Jiri Simsa, Alicia Parrish, Mantas Pajarskas, Matt Harvey, Ondrej Skopek, Yony Kochinski, Javier Rey, Verena Rieser, Denny Zhou, Sun Jae Lee, Trilok Acharya, Guowang Li, Joe Jiang, Xiaofan Zhang, Bryant Gipson, Ethan Mahintorabi, Marco Gelmi, Nima Khajehnouri, Angel Yeh, Kayi Lee, Loic Matthey, Leslie Baker, Trang Pham, Han Fu, Alex Pak, Prakhar Gupta, Cristina Vasconcelos, Adam Sadovsky, Brian Walker, Sissie Hsiao, Patrik Zochbauer, Andreea Marzoca, Noam Velan, Junhao Zeng, Gilles Baechler, Danny Driess, Divya Jain, Yanping Huang, Lizzie Tao, John Maggs, Nir Levine, Jon Schneider, Erika Gemzer, Samuel Petit, Shan Han, Zach Fisher, Dustin Zelle, Courtney Biles, Eugene Ie, Asya Fadeeva, Casper Liu, Juliana Vicente Franco, Adrian Collister, Hao Zhang, Renshen Wang, Ruizhe Zhao, Leandro Kieliger, Kurt Shuster, Rui Zhu, Boqing Gong, Lawrence Chan, Ruoxi Sun, Sujoy Basu, Roland Zimmermann, Jamie Hayes, Abhishek Bapna, Jasper Snoek, Weel Yang, Puranjay Datta, Jad Al Abdallah, Kevin Kilgour, Lu Li, SQ Mah, Yennie Jun, Morgane Rivi\u00e8re, Abhijit Karmarkar, Tammo Spalink, Tao Huang, Lucas Gonzalez, Duc-Hieu Tran, Averi Nowak, John Palowitch, Martin Chadwick, Ellie Talius, Harsh Mehta, Thibault Sellam, Philipp Fr\u00e4nken, Massimo Nicosia, Kyle He, Aditya Kini, David Amos, Sugato Basu, Harrison Jobe, Eleni Shaw, Qiantong Xu, Colin Evans, Daisuke Ikeda, Chaochao Yan, Larry Jin, Lun Wang, Sachin Yadav, Ilia Labzovsky, Ramesh Sampath, Ada Ma, Candice Schumann, Aditya Siddhant, Rohin Shah, John Youssef, Rishabh Agarwal, Natalie Dabney, Alessio Tonioni, Moran Ambar, Jing Li, Isabelle Guyon, Benny Li, David Soergel, Boya Fang, Georgi Karadzhov, Cristian Udrescu, Trieu Trinh, Vikas Raunak, Seb Noury, Dee Guo, Sonal Gupta, Mara Finkelstein, Denis Petek, Lihao Liang, Greg Billock, Pei Sun, David Wood, Yiwen Song, Xiaobin Yu, Tatiana Matejovicova, Regev Cohen, Kalyan Andra, David D'Ambrosio, Zhiwei Deng, Vincent Nallatamby, Ebrahim Songhori, Rumen Dangovski, Andrew Lampinen, Pankil Botadra, Adam Hillier, Jiawei Cao, Nagabhushan Baddi, Adhi Kuncoro, Toshihiro Yoshino, Ankit Bhagatwala, Marc\u00e1urelio Ranzato, Rylan Schaeffer, Tianlin Liu, Shuai Ye, Obaid Sarvana, John Nham, Chenkai Kuang, Isabel Gao, Jinoo Baek, Shubham Mittal, Ayzaan Wahid, Anita Gergely, Bin Ni, Josh Feldman, Carrie Muir, Pascal Lamblin, Wolfgang Macherey, Ethan Dyer, Logan Kilpatrick, V\u00edctor Campos, Mukul Bhutani, Stanislav Fort, Yanif Ahmad, Aliaksei Severyn, Kleopatra Chatziprimou, Oleksandr Ferludin, Mason Dimarco, Aditya Kusupati, Joe Heyward, Dan Bahir, Kevin Villela, Katie Millican, Dror Marcus, Sanaz Bahargam, Caglar Unlu, Nicholas Roth, Zichuan Wei, Siddharth Gopal, Deepanway Ghoshal, Edward Lee, Sharon Lin, Jennie Lees, Dayeong Lee, Anahita Hosseini, Connie Fan, Seth Neel, Marcus Wu, Yasemin Altun, Honglong Cai, Enrique Piqueras, Josh Woodward, Alessandro Bissacco, Salem Haykal, Mahyar Bordbar, Prasha Sundaram, Sarah Hodkinson, Daniel Toyama, George Polovets, Austin Myers, Anu Sinha, Tomer Levinboim, Kashyap Krishnakumar, Rachita Chhaparia, Tatiana Sholokhova, Nitesh Bharadwaj Gundavarapu, Ganesh Jawahar, Haroon Qureshi, Jieru Hu, Nikola Momchev, Matthew Rahtz, Renjie Wu, Aishwarya P S, Kedar Dhamdhere, Meiqi Guo, Umang Gupta, Ali Eslami, Mariano Schain, Michiel Blokzijl, David Welling, Dave Orr, Levent Bolelli, Nicolas Perez-Nieves, Mikhail Sirotenko, Aman Prasad, Arjun Kar, Borja De Balle Pigem, Tayfun Terzi, Gell\u00e9rt Weisz, Dipankar Ghosh, Aditi Mavalankar, Dhruv Madeka, Kaspar Daugaard, Hartwig Adam, Viraj Shah, Dana Berman, Maggie Tran, Steven Baker, Ewa Andrejczuk, Grishma Chole, Ganna Raboshchuk, Mahdi Mirzazadeh, Thais Kagohara, Shimu Wu, Christian Schallhart, Bernett Orlando, Chen Wang, Alban Rrustemi, Hao Xiong, Hao Liu, Arpi Vezer, Nolan Ramsden, Shuo-yiin Chang, Sidharth Mudgal, Yan Li, Nino Vieillard, Yedid Hoshen, Farooq Ahmad, Ambrose Slone, Amy Hua, Natan Potikha, Mirko Rossini, Jon Stritar, Sushant Prakash, Zifeng Wang, Xuanyi Dong, Alireza Nazari, Efrat Nehoran, Kaan Tekelioglu, Yinxiao Li, Kartikeya Badola, Tom Funkhouser, Yuanzhen Li, Varun Yerram, Ramya Ganeshan, Daniel Formoso, Karol Langner, Tian Shi, Huijian Li, Yumeya Yamamori, Amayika Panda, Alaa Saade, Angelo Scorza Scarpati, Chris Breaux, CJ Carey, Zongwei Zhou, Cho-Jui Hsieh, Sophie Bridgers, Alena Butryna, Nishesh Gupta, Vaibhav Tulsyan, Sanghyun Woo, Evgenii Eltyshev, Will Grathwohl, Chanel Parks, Seth Benjamin, Rina Panigrahy, Shenil Dodhia, Daniel De Freitas, Chris Sauer, Will Song, Ferran Alet, Jackson Tolins, Cosmin Paduraru, Xingyi Zhou, Brian Albert, Zizhao Zhang, Lei Shu, Mudit Bansal, Sarah Nguyen, Amir Globerson, Owen Xiao, James Manyika, Tom Hennigan, Rong Rong, Josip Matak, Anton Bakalov, Ankur Sharma, Danila Sinopalnikov, Andrew Pierson, Stephen Roller, Geoff Brown, Mingcen Gao, Toshiyuki Fukuzawa, Amin Ghafouri, Kenny Vassigh, Iain Barr, Zhicheng Wang, Anna Korsun, Rajesh Jayaram, Lijie Ren, Tim Zaman, Samira Khan, Yana Lunts, Dan Deutsch, Dave Uthus, Nitzan Katz, Masha Samsikova, Amr Khalifa, Nikhil Sethi, Jiao Sun, Luming Tang, Uri Alon, Xianghong Luo, Dian Yu, Abhishek Nayyar, Bryce Petrini, Will Truong, Vincent Hellendoorn, Nikolai Chinaev, Chris Alberti, Wei Wang, Jingcao Hu, Vahab Mirrokni, Ananth Balashankar, Avia Aharon, Aahil Mehta, Ahmet Iscen, Joseph Kready, Lucas Manning, Anhad Mohananey, Yuankai Chen, Anshuman Tripathi, Allen Wu, Igor Petrovski, Dawsen Hwang, Martin Baeuml, Shreyas Chandrakaladharan, Yuan Liu, Rey Coaguila, Maxwell Chen, Sally Ma, Pouya Tafti, Susheel Tatineni, Terry Spitz, Jiayu Ye, Paul Vicol, Mihaela Rosca, Adri\u00e0 Puigdom\u00e8nech, Zohar Yahav, Sanjay Ghemawat, Hanzhao Lin, Phoebe Kirk, Zaid Nabulsi, Sergey Brin, Bernd Bohnet, Ken Caluwaerts, Aditya Srikanth Veerubhotla, Dan Zheng, Zihang Dai, Petre Petrov, Yichong Xu, Ramin Mehran, Zhuo Xu, Luisa Zintgraf, Jiho Choi, Spurthi Amba Hombaiah, Romal Thoppilan, Sashank Reddi, Lukasz Lew, Li Li, Kellie Webster, KP Sawhney, Lampros Lamprou, Siamak Shakeri, Mayank Lunayach, Jianmin Chen, Sumit Bagri, Alex Salcianu, Ying Chen, Yani Donchev, Charlotte Magister, Signe N\u00f8rly, Vitor Rodrigues, Tomas Izo, Hila Noga, Joe Zou, Thomas K\u00f6ppe, Wenxuan Zhou, Kenton Lee, Xiangzhu Long, Danielle Eisenbud, Anthony Chen, Connor Schenck, Chi Ming To, Peilin Zhong, Emanuel Taropa, Minh Truong, Omer Levy, Danilo Martins, Zhiyuan Zhang, Christopher Semturs, Kelvin Zhang, Alex Yakubovich, Pol Moreno, Lara McConnaughey, Di Lu, Sam Redmond, Lotte Weerts, Yonatan Bitton, Tiziana Refice, Nicolas Lacasse, Arthur Conmy, Corentin Tallec, Julian Odell, Hannah Forbes-Pollard, Arkadiusz Socala, Jonathan Hoech, Pushmeet Kohli, Alanna Walton, Rui Wang, Mikita Sazanovich, Kexin Zhu, Andrei Kapishnikov, Rich Galt, Matthew Denton, Ben Murdoch, Caitlin Sikora, Kareem Mohamed, Wei Wei, Uri First, Tim McConnell, Luis C. Cobo, James Qin, Thi Avrahami, Daniel Balle, Yu Watanabe, Annie Louis, Adam Kraft, Setareh Ariafar, Yiming Gu, Eug\u00e9nie Rives, Charles Yoon, Andrei Rusu, James Cobon-Kerr, Chris Hahn, Jiaming Luo, Yuvein, Niharika Ahuja, Rodrigo Benenson, Rapha\u00ebl Lopez Kaufman, Honglin Yu, Lloyd Hightower, Junlin Zhang, Darren Ni, Lisa Anne Hendricks, Gabby Wang, Gal Yona, Lalit Jain, Pablo Barrio, Surya Bhupatiraju, Siva Velusamy, Allan Dafoe, Sebastian Riedel, Tara Thomas, Zhe Yuan, Mathias Bellaiche, Sheena Panthaplackel, Klemen Kloboves, Sarthak Jauhari, Canfer Akbulut, Todor Davchev, Evgeny Gladchenko, David Madras, Aleksandr Chuklin, Tyrone Hill, Quan Yuan, Mukundan Madhavan, Luke Leonhard, Dylan Scandinaro, Qihang Chen, Ning Niu, Arthur Douillard, Bogdan Damoc, Yasumasa Onoe, Fabian Pedregosa, Fred Bertsch, Chas Leichner, Joseph Pagadora, Jonathan Malmaud, Sameera Ponda, Andy Twigg, Oleksii Duzhyi, Jingwei Shen, Miaosen Wang, Roopal Garg, Jing Chen, Utku Evci, Jonathan Lee, Leon Liu, Koji Kojima, Masa Yamaguchi, Arunkumar Rajendran, AJ Piergiovanni, Vinodh Kumar Rajendran, Marco Fornoni, Gabriel Ibagon, Harry Ragan, Sadh MNM Khan, John Blitzer, Andrew Bunner, Guan Sun, Takahiro Kosakai, Scott Lundberg, Ndidi Elue, Kelvin Guu, SK Park, Jane Park, Arunachalam Narayanaswamy, Chengda Wu, Jayaram Mudigonda, Trevor Cohn, Hairong Mu, Ravi Kumar, Laura Graesser, Yichi Zhang, Richard Killam, Vincent Zhuang, Mai Gim\u00e9nez, Wael Al Jishi, Ruy Ley-Wild, Alex Zhai, Kazuki Osawa, Diego Cedillo, Jialu Liu, Mayank Upadhyay, Marcin Sieniek, Roshan Sharma, Tom Paine, Anelia Angelova, Sravanti Addepalli, Carolina Parada, Kingshuk Majumder, Avery Lamp, Sanjiv Kumar, Xiang Deng, Artiom Myaskovsky, Tea Saboli\u0107, Jeffrey Dudek, Sarah York, F\u00e9lix de Chaumont Quitry, Jiazhong Nie, Dee Cattle, Alok Gunjan, Bilal Piot, Waleed Khawaja, Seojin Bang, Simon Wang, Siavash Khodadadeh, Raghavender R, Praynaa Rawlani, Richard Powell, Kevin Lee, Johannes Griesser, GS Oh, Cesar Magalhaes, Yujia Li, Simon Tokumine, Hadas Natalie Vogel, Dennis Hsu, Arturo BC, Disha Jindal, Matan Cohen, Zi Yang, Junwei Yuan, Dario de Cesare, Tony Bruguier, Jun Xu, Monica Roy, Alon Jacovi, Dan Belov, Rahul Arya, Phoenix Meadowlark, Shlomi Cohen-Ganor, Wenting Ye, Patrick Morris-Suzuki, Praseem Banzal, Gan Song, Pranavaraj Ponnuramu, Fred Zhang, George Scrivener, Salah Zaiem, Alif Raditya Rochman, Kehang Han, Badih Ghazi, Kate Lee, Shahar Drath, Daniel Suo, Antonious Girgis, Pradeep Shenoy, Duy Nguyen, Douglas Eck, Somit Gupta, Le Yan, Joao Carreira, Anmol Gulati, Ruoxin Sang, Daniil Mirylenka, Emma Cooney, Edward Chou, Mingyang Ling, Cindy Fan, Ben Coleman, Guilherme Tubone, Ravin Kumar, Jason Baldridge, Felix Hernandez-Campos, Angeliki Lazaridou, James Besley, Itay Yona, Neslihan Bulut, Quentin Wellens, AJ Pierigiovanni, Jasmine George, Richard Green, Pu Han, Connie Tao, Geoff Clark, Chong You, Abbas Abdolmaleki, Justin Fu, Tongzhou Chen, Ashwin Chaugule, Angad Chandorkar, Altaf Rahman, Will Thompson, Penporn Koanantakool, Mike Bernico, Jie Ren, Andrey Vlasov, Sergei Vassilvitskii, Maciej Kula, Yizhong Liang, Dahun Kim, Yangsibo Huang, Chengxi Ye, Dmitry Lepikhin, Wesley Helmholz, et al. (3326 additional authors not shown)",
        "abstract": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.",
        "timestamp": "2025-11-18T07:01:50.377Z",
        "rating": "novote",
        "publishedDate": "2025/07/07",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 755,
        "object_id": "paper:arxiv.2507.06261",
        "created_at": "2025-11-18T07:01:51+00:00",
        "updated_at": "2025-11-18T07:02:19+00:00",
        "version": 1
      }
    },
    "paper:openreview.d10LM5LIsF": {
      "data": {
        "sourceId": "openreview",
        "paperId": "d10LM5LIsF",
        "url": "https://openreview.net/pdf?id=d10LM5LIsF",
        "title": "d10LM5LIsF",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-18T13:40:42.515Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 756,
        "object_id": "paper:openreview.d10LM5LIsF",
        "created_at": "2025-11-18T13:40:42+00:00",
        "updated_at": "2025-11-18T13:41:09+00:00",
        "version": 1
      }
    },
    "interactions:openreview.d10LM5LIsF": {
      "data": {
        "sourceId": "openreview",
        "paperId": "d10LM5LIsF",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-18T13:42:33.330Z",
            "data": {
              "session_id": "session_1763473352914_v0fw6ah",
              "source_id": "openreview",
              "paper_id": "d10LM5LIsF",
              "start_time": "2025-11-18T13:42:20.586Z",
              "end_time": "2025-11-18T13:42:32.914Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "issue_number": 757,
        "object_id": "interactions:openreview.d10LM5LIsF",
        "created_at": "2025-11-18T13:42:34+00:00",
        "updated_at": "2025-11-18T13:42:56+00:00",
        "version": 1
      }
    },
    "paper:openreview.YjbbkBJpvD": {
      "data": {
        "sourceId": "openreview",
        "paperId": "YjbbkBJpvD",
        "url": "https://openreview.net/pdf?id=YjbbkBJpvD",
        "title": "YjbbkBJpvD",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-18T13:45:32.019Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 759,
        "object_id": "paper:openreview.YjbbkBJpvD",
        "created_at": "2025-11-18T13:45:32+00:00",
        "updated_at": "2025-11-18T13:45:56+00:00",
        "version": 1
      }
    },
    "interactions:openreview.DqdngIu8Dp": {
      "data": {
        "sourceId": "openreview",
        "paperId": "DqdngIu8Dp",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-18T13:51:55.766Z",
            "data": {
              "session_id": "session_1763473915191_q8a94rj",
              "source_id": "openreview",
              "paper_id": "DqdngIu8Dp",
              "start_time": "2025-11-18T13:51:49.510Z",
              "end_time": "2025-11-18T13:51:55.191Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 762,
        "object_id": "interactions:openreview.DqdngIu8Dp",
        "created_at": "2025-11-18T13:51:56+00:00",
        "updated_at": "2025-11-18T13:52:19+00:00",
        "version": 1
      }
    },
    "paper:openreview.DqdngIu8Dp": {
      "data": {
        "sourceId": "openreview",
        "paperId": "DqdngIu8Dp",
        "url": "https://openreview.net/pdf?id=DqdngIu8Dp",
        "title": "DqdngIu8Dp",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-18T13:51:49.800Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 761,
        "object_id": "paper:openreview.DqdngIu8Dp",
        "created_at": "2025-11-18T13:51:50+00:00",
        "updated_at": "2025-11-18T13:52:11+00:00",
        "version": 1
      }
    },
    "paper:openreview.9piEl9fU9Y": {
      "data": {
        "sourceId": "openreview",
        "paperId": "9piEl9fU9Y",
        "url": "https://openreview.net/pdf?id=9piEl9fU9Y",
        "title": "9piEl9fU9Y",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-18T13:51:46.408Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 760,
        "object_id": "paper:openreview.9piEl9fU9Y",
        "created_at": "2025-11-18T13:51:46+00:00",
        "updated_at": "2025-11-18T13:52:07+00:00",
        "version": 1
      }
    },
    "paper:openreview.9lLZy0YNrN": {
      "data": {
        "sourceId": "openreview",
        "paperId": "9lLZy0YNrN",
        "url": "https://openreview.net/forum?id=9lLZy0YNrN&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Daclweb.org%2FACL%2FARR%2F2025%2FOctober%2FAuthors%23your-submissions)",
        "title": "GRADE: Quantifying Sample Diversity in Text-to-Image Models",
        "authors": "Royi Rassin, Aviv Slobodkin, Shauli Ravfogel, Yanai Elazar, Yoav Goldberg",
        "abstract": "Text-to-image models are mainly judged by prompt adherence and realism. Yet, prompts are inherently underspecified, leaving ample room to realize them in diverse ways. For instance, ``a princess at a children's birthday party'' does not detail the princess' outfit, or demographic traits like skin color, age, etc. Yet, models consistently depict her as the birthday girl with similar skin tone and a pink dress. In those common, underspecified cases, we would expect models to cover the full spectrum of the concept. Measuring and quantifying this ability has been mostly unexplored. In this work, we present GRADE, a natural-language driven method to quantify sample diversity across concept-specific attributes. We use GRADE to measure diversity in 12 models and reveal all models collapse to default behaviors, a phenomenon where a model consistently generates concepts with the same attributes (e.g., 98% of dresses are pink). Notably, we find that diversity often worsens with stronger prompt adherence and larger models. Lastly, we attribute underspecified captions in the training data for models' low diversity, where omitted attributes correlate with default behaviors.",
        "timestamp": "2025-11-18T14:28:17.324Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "text-to-image",
          "diversity"
        ],
        "doi": "",
        "journalName": "ACL ARR 2025 October Submission",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 763,
        "object_id": "paper:openreview.9lLZy0YNrN",
        "created_at": "2025-11-18T14:28:17+00:00",
        "updated_at": "2025-11-18T14:28:41+00:00",
        "version": 1
      }
    },
    "interactions:openreview.9lLZy0YNrN": {
      "data": {
        "sourceId": "openreview",
        "paperId": "9lLZy0YNrN",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-18T14:28:52.965Z",
            "data": {
              "session_id": "session_1763476131840_05kg0n0",
              "source_id": "openreview",
              "paper_id": "9lLZy0YNrN",
              "start_time": "2025-11-18T14:28:16.827Z",
              "end_time": "2025-11-18T14:28:51.840Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 0,
              "total_elapsed_seconds": 35
            }
          }
        ]
      },
      "meta": {
        "issue_number": 764,
        "object_id": "interactions:openreview.9lLZy0YNrN",
        "created_at": "2025-11-18T14:28:53+00:00",
        "updated_at": "2025-11-18T14:29:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2511.10643": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.10643",
        "url": "https://arxiv.org/pdf/2511.10643",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-18T18:22:45.896Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 766,
        "object_id": "paper:arxiv.2511.10643",
        "created_at": "2025-11-18T18:22:46+00:00",
        "updated_at": "2025-11-18T18:23:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2305.10601": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.10601",
        "url": "https://arxiv.org/abs/2305.10601",
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "authors": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan",
        "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: this https URL.",
        "timestamp": "2025-11-18T19:23:54.553Z",
        "rating": "novote",
        "publishedDate": "2023/05/17",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 768,
        "object_id": "paper:arxiv.2305.10601",
        "created_at": "2025-11-18T19:23:55+00:00",
        "updated_at": "2025-11-18T19:24:21+00:00",
        "version": 1
      }
    },
    "interactions:openreview.YjbbkBJpvD": {
      "data": {
        "sourceId": "openreview",
        "paperId": "YjbbkBJpvD",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-18T20:02:05.782Z",
            "data": {
              "session_id": "session_1763496125354_j9chrqh",
              "source_id": "openreview",
              "paper_id": "YjbbkBJpvD",
              "start_time": "2025-11-18T20:01:59.833Z",
              "end_time": "2025-11-18T20:02:05.354Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 770,
        "object_id": "interactions:openreview.YjbbkBJpvD",
        "created_at": "2025-11-18T20:02:06+00:00",
        "updated_at": "2025-11-18T20:02:30+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2305.10601": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.10601",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-18T20:01:56.378Z",
            "data": {
              "session_id": "session_1763496115527_qsq4eyw",
              "source_id": "arxiv",
              "paper_id": "2305.10601",
              "start_time": "2025-11-18T20:01:30.919Z",
              "end_time": "2025-11-18T20:01:55.527Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 5,
              "total_elapsed_seconds": 25
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-19T07:48:36.620Z",
            "data": {
              "session_id": "session_1763538516600_rfpiwlr",
              "source_id": "arxiv",
              "paper_id": "2305.10601",
              "start_time": "2025-11-19T07:48:25.635Z",
              "end_time": "2025-11-19T07:48:36.600Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-19T07:49:48.529Z",
            "data": {
              "session_id": "session_1763538587890_wmecv4v",
              "source_id": "arxiv",
              "paper_id": "2305.10601",
              "start_time": "2025-11-19T07:48:38.529Z",
              "end_time": "2025-11-19T07:49:47.890Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 4,
              "total_elapsed_seconds": 69
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-19T07:52:21.071Z",
            "data": {
              "session_id": "session_1763538740737_gwpfsxv",
              "source_id": "arxiv",
              "paper_id": "2305.10601",
              "start_time": "2025-11-19T07:51:25.346Z",
              "end_time": "2025-11-19T07:52:20.737Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 0,
              "total_elapsed_seconds": 55
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T08:29:27.681Z",
            "data": {
              "session_id": "session_1763627367257_5y03wkg",
              "source_id": "arxiv",
              "paper_id": "2305.10601",
              "start_time": "2025-11-20T08:29:10.467Z",
              "end_time": "2025-11-20T08:29:27.257Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T08:36:42.376Z",
            "data": {
              "session_id": "session_1763627802361_wtbgkbw",
              "source_id": "arxiv",
              "paper_id": "2305.10601",
              "start_time": "2025-11-20T08:36:28.515Z",
              "end_time": "2025-11-20T08:36:42.361Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T08:38:16.192Z",
            "data": {
              "session_id": "session_1763627896188_9bvbf0i",
              "source_id": "arxiv",
              "paper_id": "2305.10601",
              "start_time": "2025-11-20T08:38:03.027Z",
              "end_time": "2025-11-20T08:38:16.188Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T10:11:03.163Z",
            "data": {
              "session_id": "session_1763633462701_gzoysqq",
              "source_id": "arxiv",
              "paper_id": "2305.10601",
              "start_time": "2025-11-20T10:10:49.355Z",
              "end_time": "2025-11-20T10:11:02.701Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T11:02:54.861Z",
            "data": {
              "session_id": "session_1763636574844_vss3dy8",
              "source_id": "arxiv",
              "paper_id": "2305.10601",
              "start_time": "2025-11-20T11:02:49.696Z",
              "end_time": "2025-11-20T11:02:54.844Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T12:57:06.663Z",
            "data": {
              "session_id": "session_1763643426303_mnb6jmr",
              "source_id": "arxiv",
              "paper_id": "2305.10601",
              "start_time": "2025-11-20T12:56:56.759Z",
              "end_time": "2025-11-20T12:57:06.303Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 5,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T13:03:42.568Z",
            "data": {
              "session_id": "session_1763643822557_sdwnezn",
              "source_id": "arxiv",
              "paper_id": "2305.10601",
              "start_time": "2025-11-20T13:03:32.203Z",
              "end_time": "2025-11-20T13:03:42.557Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 0,
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T13:38:15.366Z",
            "data": {
              "session_id": "session_1763645895357_wsll9oq",
              "source_id": "arxiv",
              "paper_id": "2305.10601",
              "start_time": "2025-11-20T13:38:08.895Z",
              "end_time": "2025-11-20T13:38:15.357Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 769,
        "object_id": "interactions:arxiv.2305.10601",
        "created_at": "2025-11-18T20:01:57+00:00",
        "updated_at": "2025-11-20T13:38:42+00:00",
        "version": 1
      }
    },
    "paper:openreview.ev3kv63UZC": {
      "data": {
        "sourceId": "openreview",
        "paperId": "ev3kv63UZC",
        "url": "https://openreview.net/pdf?id=ev3kv63UZC",
        "title": "ev3kv63UZC",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-19T06:36:01.647Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 791,
        "object_id": "paper:openreview.ev3kv63UZC",
        "created_at": "2025-11-19T06:36:02+00:00",
        "updated_at": "2025-11-19T06:36:20+00:00",
        "version": 1
      }
    },
    "paper:openreview.Pvgpz0Ydbl": {
      "data": {
        "sourceId": "openreview",
        "paperId": "Pvgpz0Ydbl",
        "url": "https://openreview.net/pdf?id=Pvgpz0Ydbl",
        "title": "Pvgpz0Ydbl",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-19T06:35:59.348Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 790,
        "object_id": "paper:openreview.Pvgpz0Ydbl",
        "created_at": "2025-11-19T06:35:59+00:00",
        "updated_at": "2025-11-19T06:36:22+00:00",
        "version": 1
      }
    },
    "paper:openreview.zsfsP6zm2r": {
      "data": {
        "sourceId": "openreview",
        "paperId": "zsfsP6zm2r",
        "url": "https://openreview.net/pdf?id=zsfsP6zm2r",
        "title": "zsfsP6zm2r",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-19T06:35:59.029Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 789,
        "object_id": "paper:openreview.zsfsP6zm2r",
        "created_at": "2025-11-19T06:35:59+00:00",
        "updated_at": "2025-11-19T06:36:22+00:00",
        "version": 1
      }
    },
    "paper:openreview.SVIz8zbZtw": {
      "data": {
        "sourceId": "openreview",
        "paperId": "SVIz8zbZtw",
        "url": "https://openreview.net/pdf?id=SVIz8zbZtw",
        "title": "SVIz8zbZtw",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-19T06:35:58.447Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 788,
        "object_id": "paper:openreview.SVIz8zbZtw",
        "created_at": "2025-11-19T06:35:59+00:00",
        "updated_at": "2025-11-19T06:36:20+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.00656": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.00656",
        "url": "https://arxiv.org/abs/2501.00656",
        "title": "2 OLMo 2 Furious",
        "authors": "Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Allyson Ettinger, Michal Guerquin, David Heineman, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Jake Poznanski, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi",
        "abstract": "We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes a family of dense autoregressive language models at 7B, 13B and 32B scales with fully released artifacts -- model weights, full training data, training code and recipes, training logs and thousands of intermediate checkpoints. In this work, we describe our modified model architecture and training recipe, focusing on techniques for achieving better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from T\u00fclu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to training compute, often matching or outperforming open-weight only models like Llama 3.1, Qwen 2.5, and Gemma 2 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with open-weight only models of comparable size and even some proprietary models like GPT-3.5 Turbo and GPT 4o Mini.",
        "timestamp": "2025-11-19T12:11:22.466Z",
        "rating": "novote",
        "publishedDate": "2024/12/31",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 793,
        "object_id": "paper:arxiv.2501.00656",
        "created_at": "2025-11-19T12:11:23+00:00",
        "updated_at": "2025-11-19T12:11:44+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2511.08394": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.08394",
        "url": "https://arxiv.org/abs/2511.08394",
        "title": "Interaction Dynamics as a Reward Signal for LLMs",
        "authors": "Sian Gooding, Edward Grefenstette",
        "abstract": "The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.",
        "timestamp": "2025-11-20T16:26:19.948Z",
        "rating": "novote",
        "publishedDate": "2025/11/11",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Human-Computer Interaction (cs.HC)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 795,
        "object_id": "paper:arxiv.2511.08394",
        "created_at": "2025-11-20T16:26:20+00:00",
        "updated_at": "2025-11-20T16:26:45+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2511.08394": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.08394",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T16:41:32.600Z",
            "data": {
              "session_id": "session_1763656891942_1ry9gjq",
              "source_id": "arxiv",
              "paper_id": "2511.08394",
              "start_time": "2025-11-20T16:38:53.264Z",
              "end_time": "2025-11-20T16:41:31.942Z",
              "heartbeat_count": 31,
              "duration_seconds": 155,
              "idle_seconds": 4,
              "total_elapsed_seconds": 159
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T17:01:25.072Z",
            "data": {
              "session_id": "session_1763658085017_wnr6jp2",
              "source_id": "arxiv",
              "paper_id": "2511.08394",
              "start_time": "2025-11-20T17:01:14.030Z",
              "end_time": "2025-11-20T17:01:25.017Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-20T19:01:08.239Z",
            "data": {
              "session_id": "session_1763665267621_ide906y",
              "source_id": "arxiv",
              "paper_id": "2511.08394",
              "start_time": "2025-11-20T18:57:47.147Z",
              "end_time": "2025-11-20T19:01:07.621Z",
              "heartbeat_count": 40,
              "duration_seconds": 200,
              "idle_seconds": 0,
              "total_elapsed_seconds": 200
            }
          }
        ]
      },
      "meta": {
        "issue_number": 796,
        "object_id": "interactions:arxiv.2511.08394",
        "created_at": "2025-11-20T16:27:27+00:00",
        "updated_at": "2025-11-20T19:01:33+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2102.11903": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2102.11903",
        "url": "https://arxiv.org/pdf/2102.11903",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-20T16:46:23.626Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 800,
        "object_id": "paper:arxiv.2102.11903",
        "created_at": "2025-11-20T16:46:24+00:00",
        "updated_at": "2025-11-20T16:46:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1701.07795": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1701.07795",
        "url": "https://arxiv.org/pdf/1701.07795",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-20T16:46:19.404Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 797,
        "object_id": "paper:arxiv.1701.07795",
        "created_at": "2025-11-20T16:46:19+00:00",
        "updated_at": "2025-11-20T16:46:40+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.07096": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07096",
        "url": "https://arxiv.org/abs/2504.07096",
        "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens",
        "authors": "Jiacheng Liu, Taylor Blanton, Yanai Elazar, Sewon Min, YenSung Chen, Arnavi Chheda-Kothary, Huy Tran, Byron Bischoff, Eric Marsh, Michael Schmitz, Cassidy Trier, Aaron Sarnat, Jenna James, Jon Borchardt, Bailey Kuehl, Evie Cheng, Karen Farley, Sruthi Sreeram, Taira Anderson, David Albright, Carissa Schoenick, Luca Soldaini, Dirk Groeneveld, Rock Yuren Pang, Pang Wei Koh, Noah A. Smith, Sophie Lebrecht, Yejin Choi, Hannaneh Hajishirzi, Ali Farhadi, Jesse Dodge",
        "abstract": "We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.",
        "timestamp": "2025-11-20T20:47:12.735Z",
        "rating": "novote",
        "publishedDate": "2025/04/09",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 801,
        "object_id": "paper:arxiv.2504.07096",
        "created_at": "2025-11-20T20:47:13+00:00",
        "updated_at": "2025-11-20T20:47:36+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2401.01854": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.01854",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T14:05:57.509Z",
            "data": {
              "session_id": "session_1763733956625_e4m83vx",
              "source_id": "arxiv",
              "paper_id": "2401.01854",
              "start_time": "2025-11-21T13:58:21.458Z",
              "end_time": "2025-11-21T14:05:56.625Z",
              "heartbeat_count": 91,
              "duration_seconds": 455,
              "idle_seconds": 0,
              "total_elapsed_seconds": 455
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-21T20:22:28.557Z",
            "data": {
              "session_id": "session_1763756548201_ckgnaad",
              "source_id": "arxiv",
              "paper_id": "2401.01854",
              "start_time": "2025-11-21T20:22:19.363Z",
              "end_time": "2025-11-21T20:22:28.201Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 803,
        "object_id": "interactions:arxiv.2401.01854",
        "created_at": "2025-11-21T13:58:19+00:00",
        "updated_at": "2025-11-21T20:22:50+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2401.01854": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.01854",
        "url": "https://arxiv.org/pdf/2401.01854",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-21T13:58:12.495Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 802,
        "object_id": "paper:arxiv.2401.01854",
        "created_at": "2025-11-21T13:58:13+00:00",
        "updated_at": "2025-11-21T13:58:36+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1806.10869": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1806.10869",
        "url": "https://arxiv.org/pdf/1806.10869",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-22T07:15:22.079Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 804,
        "object_id": "paper:arxiv.1806.10869",
        "created_at": "2025-11-22T07:15:22+00:00",
        "updated_at": "2025-11-22T07:15:46+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1705.01509": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1705.01509",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-22T07:17:19.075Z",
            "data": {
              "session_id": "session_1763795838446_a59vcg4",
              "source_id": "arxiv",
              "paper_id": "1705.01509",
              "start_time": "2025-11-22T07:16:57.953Z",
              "end_time": "2025-11-22T07:17:18.446Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "issue_number": 808,
        "object_id": "interactions:arxiv.1705.01509",
        "created_at": "2025-11-22T07:17:19+00:00",
        "updated_at": "2025-11-22T07:17:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1705.01509": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1705.01509",
        "url": "https://arxiv.org/pdf/1705.01509",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-22T07:16:58.323Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 807,
        "object_id": "paper:arxiv.1705.01509",
        "created_at": "2025-11-22T07:16:58+00:00",
        "updated_at": "2025-11-22T07:17:23+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1806.10869": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1806.10869",
        "interactions": []
      },
      "meta": {
        "issue_number": 805,
        "object_id": "interactions:arxiv.1806.10869",
        "created_at": "2025-11-22T07:16:55+00:00",
        "updated_at": "2025-11-22T07:16:57+00:00",
        "version": 1
      }
    },
    "interactions:openreview.XfHWcNTSHp": {
      "data": {
        "sourceId": "openreview",
        "paperId": "XfHWcNTSHp",
        "interactions": []
      },
      "meta": {
        "issue_number": 810,
        "object_id": "interactions:openreview.XfHWcNTSHp",
        "created_at": "2025-11-22T07:22:05+00:00",
        "updated_at": "2025-11-22T07:22:07+00:00",
        "version": 1
      }
    },
    "paper:openreview.XfHWcNTSHp": {
      "data": {
        "sourceId": "openreview",
        "paperId": "XfHWcNTSHp",
        "url": "https://openreview.net/forum?id=XfHWcNTSHp&noteId=rqfpI8oy7d",
        "title": "A Survey on Data Selection for Language Models",
        "authors": "Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, William Yang Wang",
        "abstract": "A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required.\n\nData selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data selection practices has become concentrated within a few organizations, many of which do not openly share their findings and methodologies.\n\nTo narrow this gap in knowledge, we present a comprehensive review of existing literature on data selection methods and related research areas, providing a taxonomy of existing approaches. By describing the current landscape of research, this work aims to accelerate progress in data selection by establishing an entry point for new and established researchers. Additionally, throughout this review we draw attention to noticeable holes in the literature and conclude the paper by proposing promising avenues for future research.",
        "timestamp": "2025-11-22T07:21:46.993Z",
        "rating": "novote",
        "publishedDate": "12 Jul 2024",
        "tags": [],
        "doi": "",
        "journalName": "Accepted by TMLR",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 809,
        "object_id": "paper:openreview.XfHWcNTSHp",
        "created_at": "2025-11-22T07:21:47+00:00",
        "updated_at": "2025-11-22T07:22:05+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.07096": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.07096",
        "interactions": []
      },
      "meta": {
        "issue_number": 811,
        "object_id": "interactions:arxiv.2504.07096",
        "created_at": "2025-11-22T11:02:00+00:00",
        "updated_at": "2025-11-22T11:02:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2306.13891": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2306.13891",
        "url": "https://arxiv.org/abs/2306.13891",
        "title": "Estimating the Causal Effect of Early ArXiving on Paper Acceptance",
        "authors": "Yanai Elazar, Jiayao Zhang, David Wadden, Bo Zhang, Noah A. Smith",
        "abstract": "What is the effect of releasing a preprint of a paper before it is submitted for peer review? No randomized controlled trial has been conducted, so we turn to observational data to answer this question. We use data from the ICLR conference (2018--2022) and apply methods from causal inference to estimate the effect of arXiving a paper before the reviewing period (early arXiving) on its acceptance to the conference. Adjusting for confounders such as topic, authors, and quality, we may estimate the causal effect. However, since quality is a challenging construct to estimate, we use the negative outcome control method, using paper citation count as a control variable to debias the quality confounding effect. Our results suggest that early arXiving may have a small effect on a paper's chances of acceptance. However, this effect (when existing) does not differ significantly across different groups of authors, as grouped by author citation count and institute rank. This suggests that early arXiving does not provide an advantage to any particular group.",
        "timestamp": "2025-11-22T11:02:57.797Z",
        "rating": "novote",
        "publishedDate": "2023/06/24",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 812,
        "object_id": "paper:arxiv.2306.13891",
        "created_at": "2025-11-22T11:02:58+00:00",
        "updated_at": "2025-11-22T11:03:17+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2306.13891": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2306.13891",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T08:19:29.219Z",
            "data": {
              "session_id": "session_1764145168915_848ney3",
              "source_id": "arxiv",
              "paper_id": "2306.13891",
              "start_time": "2025-11-26T08:18:30.846Z",
              "end_time": "2025-11-26T08:19:28.915Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 3,
              "total_elapsed_seconds": 58
            }
          }
        ]
      },
      "meta": {
        "issue_number": 813,
        "object_id": "interactions:arxiv.2306.13891",
        "created_at": "2025-11-22T12:02:12+00:00",
        "updated_at": "2025-11-26T08:19:50+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2104.10706": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2104.10706",
        "url": "https://arxiv.org/pdf/2104.10706",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-23T08:23:51.189Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 814,
        "object_id": "paper:arxiv.2104.10706",
        "created_at": "2025-11-23T08:23:51+00:00",
        "updated_at": "2025-11-23T08:24:09+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2104.10706": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2104.10706",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-23T08:24:30.940Z",
            "data": {
              "session_id": "session_1763886270050_mtu52wj",
              "source_id": "arxiv",
              "paper_id": "2104.10706",
              "start_time": "2025-11-23T08:23:50.743Z",
              "end_time": "2025-11-23T08:24:30.050Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 4,
              "total_elapsed_seconds": 39
            }
          }
        ]
      },
      "meta": {
        "issue_number": 815,
        "object_id": "interactions:arxiv.2104.10706",
        "created_at": "2025-11-23T08:24:31+00:00",
        "updated_at": "2025-11-23T08:24:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2305.03350": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.03350",
        "url": "https://arxiv.org/abs/2305.03350",
        "title": "Reconstructing Training Data from Multiclass Neural Networks",
        "authors": "Gon Buzaglo, Niv Haim, Gilad Yehudai, Gal Vardi, Michal Irani",
        "abstract": "Reconstructing samples from the training set of trained neural networks is a major privacy concern. Haim et al. (2022) recently showed that it is possible to reconstruct training samples from neural network binary classifiers, based on theoretical results about the implicit bias of gradient methods. In this work, we present several improvements and new insights over this previous work. As our main improvement, we show that training-data reconstruction is possible in the multi-class setting and that the reconstruction quality is even higher than in the case of binary classification. Moreover, we show that using weight-decay during training increases the vulnerability to sample reconstruction. Finally, while in the previous work the training set was of size at most $1000$ from $10$ classes, we show preliminary evidence of the ability to reconstruct from a model trained on $5000$ samples from $100$ classes.",
        "timestamp": "2025-11-23T08:58:34.217Z",
        "rating": "novote",
        "publishedDate": "2023/05/05",
        "tags": [
          "Machine Learning (cs.LG)",
          "Cryptography and Security (cs.CR)",
          "Computer Vision and Pattern Recognition (cs.CV)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 816,
        "object_id": "paper:arxiv.2305.03350",
        "created_at": "2025-11-23T08:58:34+00:00",
        "updated_at": "2025-11-23T08:58:56+00:00",
        "version": 1
      }
    },
    "paper:openreview.oUOg50iFL1": {
      "data": {
        "sourceId": "openreview",
        "paperId": "oUOg50iFL1",
        "url": "https://openreview.net/pdf?id=oUOg50iFL1",
        "title": "oUOg50iFL1",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-23T08:59:09.701Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 817,
        "object_id": "paper:openreview.oUOg50iFL1",
        "created_at": "2025-11-23T08:59:10+00:00",
        "updated_at": "2025-11-23T08:59:30+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2403.09539": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.09539",
        "interactions": []
      },
      "meta": {
        "issue_number": 819,
        "object_id": "interactions:arxiv.2403.09539",
        "created_at": "2025-11-23T09:02:28+00:00",
        "updated_at": "2025-11-23T09:02:30+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.09539": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.09539",
        "url": "https://arxiv.org/pdf/2403.09539",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-23T09:02:04.266Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 818,
        "object_id": "paper:arxiv.2403.09539",
        "created_at": "2025-11-23T09:02:04+00:00",
        "updated_at": "2025-11-23T09:02:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2407.16607": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.16607",
        "interactions": []
      },
      "meta": {
        "issue_number": 821,
        "object_id": "interactions:arxiv.2407.16607",
        "created_at": "2025-11-23T09:05:00+00:00",
        "updated_at": "2025-11-23T09:05:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.16607": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.16607",
        "url": "https://arxiv.org/abs/2407.16607",
        "title": "Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?",
        "authors": "Jonathan Hayase, Alisa Liu, Yejin Choi, Sewoong Oh, Noah A. Smith",
        "abstract": "The pretraining data of today's strongest language models is opaque; in particular, little is known about the proportions of various domains or languages represented. In this work, we tackle a task which we call data mixture inference, which aims to uncover the distributional make-up of training data. We introduce a novel attack based on a previously overlooked source of information: byte-pair encoding (BPE) tokenizers, used by the vast majority of modern language models. Our key insight is that the ordered list of merge rules learned by a BPE tokenizer naturally reveals information about the token frequencies in its training data. Given a tokenizer's merge list along with example data for each category of interest, we formulate a linear program that solves for the proportion of each category in the tokenizer's training set. In controlled experiments, we show that our attack recovers mixture ratios with high precision for tokenizers trained on known mixtures of natural languages, programming languages, and data sources. We then apply our approach to off-the-shelf tokenizers released with recent LMs. We confirm much publicly disclosed information about these models, and also make several new inferences: GPT-4o and Mistral NeMo's tokenizers are much more multilingual than their predecessors, training on 39% and 47% non-English language data, respectively; Llama 3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). We hope our work sheds light on current design practices for pretraining data, and inspires continued research into data mixture inference for LMs.",
        "timestamp": "2025-11-23T09:04:45.846Z",
        "rating": "novote",
        "publishedDate": "2024/07/23",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 820,
        "object_id": "paper:arxiv.2407.16607",
        "created_at": "2025-11-23T09:04:46+00:00",
        "updated_at": "2025-11-23T09:05:08+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.12459": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.12459",
        "interactions": []
      },
      "meta": {
        "issue_number": 823,
        "object_id": "interactions:arxiv.2504.12459",
        "created_at": "2025-11-23T09:06:32+00:00",
        "updated_at": "2025-11-23T09:06:33+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.12459": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.12459",
        "url": "https://arxiv.org/pdf/2504.12459?",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-23T09:06:16.537Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 822,
        "object_id": "paper:arxiv.2504.12459",
        "created_at": "2025-11-23T09:06:16+00:00",
        "updated_at": "2025-11-23T09:06:37+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.1703.04730": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1703.04730",
        "interactions": []
      },
      "meta": {
        "issue_number": 825,
        "object_id": "interactions:arxiv.1703.04730",
        "created_at": "2025-11-24T06:24:30+00:00",
        "updated_at": "2025-11-24T06:24:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.1703.04730": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "1703.04730",
        "url": "https://arxiv.org/pdf/1703.04730",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-24T06:23:43.420Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 824,
        "object_id": "paper:arxiv.1703.04730",
        "created_at": "2025-11-24T06:23:43+00:00",
        "updated_at": "2025-11-24T06:24:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2002.08484": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2002.08484",
        "url": "https://arxiv.org/abs/2002.08484",
        "title": "Estimating Training Data Influence by Tracing Gradient Descent",
        "authors": "Garima Pruthi, Frederick Liu, Mukund Sundararajan, Satyen Kale",
        "abstract": "We introduce a method called TracIn that computes the influence of a training example on a prediction made by the model. The idea is to trace how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TracIn via: (a) a first-order gradient approximation to the exact computation, (b) saved checkpoints of standard training procedures, and (c) cherry-picking layers of a deep neural network. In contrast with previously proposed methods, TracIn is simple to implement; all it needs is the ability to work with gradients, checkpoints, and loss functions. The method is general. It applies to any machine learning model trained using stochastic gradient descent or a variant of it, agnostic of architecture, domain and task. We expect the method to be widely useful within processes that study and improve training data.",
        "timestamp": "2025-11-24T12:54:07.690Z",
        "rating": "novote",
        "publishedDate": "2020/02/19",
        "tags": [
          "Machine Learning (cs.LG)",
          "Machine Learning (stat.ML)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 826,
        "object_id": "paper:arxiv.2002.08484",
        "created_at": "2025-11-24T12:54:08+00:00",
        "updated_at": "2025-11-24T12:54:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.10341": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.10341",
        "url": "https://arxiv.org/abs/2502.10341",
        "title": "Organize the Web: Constructing Domains Enhances Pre-Training Data Curation",
        "authors": "Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, Luca Soldaini",
        "abstract": "Modern language models are trained on large, unstructured datasets consisting of trillions of tokens and obtained by crawling the web. The unstructured nature makes it difficult to reason about their contents and develop systematic approaches to data curation. In this paper, we unpack monolithic web corpora by developing taxonomies of their contents and organizing them into domains. We introduce WebOrganizer, a framework for organizing web pages in terms of both their topic and format. Using these two complementary notions of domains, we automatically annotate pre-training data by distilling annotations from a large language model into efficient classifiers. This allows us to study how data from different domains should be mixed to improve models on downstream tasks, and we show that we can combine insights about effective topics and formats to further boost performance. We demonstrate that our domain mixing also improves existing methods that select data based on quality. Furthermore, we study and compare how quality-based methods will implicitly change the domain mixture. Overall, our work demonstrates that constructing and mixing domains provides a valuable complement to quality-based data curation methods, opening new avenues for effective and insightful pre-training data curation.",
        "timestamp": "2025-11-24T17:33:20.006Z",
        "rating": "novote",
        "publishedDate": "2025/02/14",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 827,
        "object_id": "paper:arxiv.2502.10341",
        "created_at": "2025-11-24T17:33:20+00:00",
        "updated_at": "2025-11-24T17:33:41+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.10341": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.10341",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-24T17:50:29.933Z",
            "data": {
              "session_id": "session_1764006629333_34it6ag",
              "source_id": "arxiv",
              "paper_id": "2502.10341",
              "start_time": "2025-11-24T17:33:24.551Z",
              "end_time": "2025-11-24T17:50:29.333Z",
              "heartbeat_count": 204,
              "duration_seconds": 1020,
              "idle_seconds": 5,
              "total_elapsed_seconds": 1025
            }
          }
        ]
      },
      "meta": {
        "issue_number": 828,
        "object_id": "interactions:arxiv.2502.10341",
        "created_at": "2025-11-24T17:50:30+00:00",
        "updated_at": "2025-11-24T17:50:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.23383": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.23383",
        "url": "https://arxiv.org/abs/2509.23383",
        "title": "Train Once, Answer All: Many Pretraining Experiments for the Cost of One",
        "authors": "Sebastian Bordt, Martin Pawelczyk",
        "abstract": "Recent work has demonstrated that controlled pretraining experiments are a powerful tool for understanding learning, reasoning, and memorization in large language models (LLMs). However, the computational cost of pretraining presents a significant constraint. To overcome this constraint, we propose to conduct multiple pretraining experiments simultaneously during a single training run. We demonstrate the feasibility of this approach by conducting ten experiments during the training of a 1.5B parameter model on 210B tokens. Although we only train a single model, we can replicate the results from multiple previous works on data contamination, poisoning, and memorization. We also conduct novel investigations into knowledge acquisition, mathematical reasoning, and watermarking. For example, we dynamically update the training data until the model acquires a particular piece of knowledge. Remarkably, the influence of the ten experiments on the model's training dynamics and overall performance is minimal. However, interactions between different experiments may act as a potential confounder in our approach. We propose to test for interactions with continual pretraining experiments, finding them to be negligible in our setup. Overall, our findings suggest that performing multiple pretraining experiments in a single training run can enable rigorous scientific experimentation with large models on a compute budget.",
        "timestamp": "2025-11-25T11:24:52.098Z",
        "rating": "novote",
        "publishedDate": "2025/09/27",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 829,
        "object_id": "paper:arxiv.2509.23383",
        "created_at": "2025-11-25T11:24:52+00:00",
        "updated_at": "2025-11-25T11:25:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.16948": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.16948",
        "url": "https://arxiv.org/pdf/2504.16948",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-25T11:47:55.292Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 830,
        "object_id": "paper:arxiv.2504.16948",
        "created_at": "2025-11-25T11:47:55+00:00",
        "updated_at": "2025-11-25T11:48:17+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2403.08946": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.08946",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T11:51:11.271Z",
            "data": {
              "session_id": "session_1764071469895_js2enyy",
              "source_id": "arxiv",
              "paper_id": "2403.08946",
              "start_time": "2025-11-25T11:50:02.329Z",
              "end_time": "2025-11-25T11:51:09.895Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 3,
              "total_elapsed_seconds": 68
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-25T15:32:37.895Z",
            "data": {
              "session_id": "session_1764084757872_hdyi8xo",
              "source_id": "arxiv",
              "paper_id": "2403.08946",
              "start_time": "2025-11-25T15:32:26.928Z",
              "end_time": "2025-11-25T15:32:37.872Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 836,
        "object_id": "interactions:arxiv.2403.08946",
        "created_at": "2025-11-25T11:49:58+00:00",
        "updated_at": "2025-11-25T15:32:49+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.08946": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.08946",
        "url": "https://arxiv.org/pdf/2403.08946",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-25T11:49:51.882Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 833,
        "object_id": "paper:arxiv.2403.08946",
        "created_at": "2025-11-25T11:49:52+00:00",
        "updated_at": "2025-11-25T11:50:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.15287": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.15287",
        "url": "https://arxiv.org/pdf/2411.15287",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-25T11:49:45.328Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 831,
        "object_id": "paper:arxiv.2411.15287",
        "created_at": "2025-11-25T11:49:45+00:00",
        "updated_at": "2025-11-25T11:50:07+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.02279": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.02279",
        "url": "https://arxiv.org/html/2506.02279v1",
        "title": "Instructions for *ACL Proceedings",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-11-25T16:28:30.446Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 837,
        "object_id": "paper:arxiv.2506.02279",
        "created_at": "2025-11-25T16:28:30+00:00",
        "updated_at": "2025-11-25T16:28:51+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2411.15287": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.15287",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T10:02:30.200Z",
            "data": {
              "session_id": "session_1764151350192_cok6pnc",
              "source_id": "arxiv",
              "paper_id": "2411.15287",
              "start_time": "2025-11-26T10:02:16.555Z",
              "end_time": "2025-11-26T10:02:30.192Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T10:06:23.379Z",
            "data": {
              "session_id": "session_1764151583079_8s1bujl",
              "source_id": "arxiv",
              "paper_id": "2411.15287",
              "start_time": "2025-11-26T10:05:59.002Z",
              "end_time": "2025-11-26T10:06:23.079Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T10:31:23.653Z",
            "data": {
              "session_id": "session_1764153083636_avsm9g3",
              "source_id": "arxiv",
              "paper_id": "2411.15287",
              "start_time": "2025-11-26T10:31:00.775Z",
              "end_time": "2025-11-26T10:31:23.636Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T10:54:49.592Z",
            "data": {
              "session_id": "session_1764154489587_4ef4cmz",
              "source_id": "arxiv",
              "paper_id": "2411.15287",
              "start_time": "2025-11-26T10:54:43.274Z",
              "end_time": "2025-11-26T10:54:49.587Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T11:01:59.104Z",
            "data": {
              "session_id": "session_1764154919101_m82n7hk",
              "source_id": "arxiv",
              "paper_id": "2411.15287",
              "start_time": "2025-11-26T11:01:49.806Z",
              "end_time": "2025-11-26T11:01:59.101Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-11-26T12:28:17.921Z",
            "data": {
              "session_id": "session_1764160097522_8yo1qve",
              "source_id": "arxiv",
              "paper_id": "2411.15287",
              "start_time": "2025-11-26T12:27:25.930Z",
              "end_time": "2025-11-26T12:28:17.521Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 2,
              "total_elapsed_seconds": 52
            }
          }
        ]
      },
      "meta": {
        "issue_number": 838,
        "object_id": "interactions:arxiv.2411.15287",
        "created_at": "2025-11-26T07:49:37+00:00",
        "updated_at": "2025-11-26T12:28:36+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2306.17844": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2306.17844",
        "interactions": []
      },
      "meta": {
        "issue_number": 840,
        "object_id": "interactions:arxiv.2306.17844",
        "created_at": "2025-11-26T13:13:30+00:00",
        "updated_at": "2025-11-26T13:13:33+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2306.17844": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2306.17844",
        "url": "https://arxiv.org/abs/2306.17844",
        "title": "The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks",
        "authors": "Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas",
        "abstract": "Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms for solving those tasks? Several recent studies, on tasks ranging from group arithmetic to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex. Small changes to model hyperparameters and initializations can induce the discovery of qualitatively different algorithms from a fixed training set, and even parallel implementations of multiple such algorithms. Some networks trained to perform modular addition implement a familiar Clock algorithm; others implement a previously undescribed, less intuitive, but comprehensible procedure which we term the Pizza algorithm, or a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools for characterizing the behavior of neural networks across their algorithmic phase space.",
        "timestamp": "2025-11-26T13:12:58.137Z",
        "rating": "novote",
        "publishedDate": "2023/06/30",
        "tags": [
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 839,
        "object_id": "paper:arxiv.2306.17844",
        "created_at": "2025-11-26T13:12:58+00:00",
        "updated_at": "2025-11-26T13:13:18+00:00",
        "version": 1
      }
    }
  }
}