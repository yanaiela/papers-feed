{
  "snapshot_time": "2025-06-11T17:50:07.491862+00:00",
  "repository": "yanaiela/papers-feed",
  "objects": {
    "interactions:arxiv.2411.00640": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.00640",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-10T23:48:26.492Z",
            "data": {
              "session_id": "session_1749599306052_fap3gto",
              "source_id": "arxiv",
              "paper_id": "2411.00640",
              "start_time": "2025-06-10T23:48:18.158Z",
              "end_time": "2025-06-10T23:48:26.052Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 4,
        "object_id": "interactions:arxiv.2411.00640",
        "created_at": "2025-06-10T23:48:27+00:00",
        "updated_at": "2025-06-10T23:48:52+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.00640": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.00640",
        "url": "https://arxiv.org/pdf/2411.00640",
        "title": "Adding Error Bars to Evals: A Statistical Approach to Language Model\n  Evaluations",
        "authors": "Evan Miller",
        "abstract": "Evaluations are critical for understanding the capabilities of large language\nmodels (LLMs). Fundamentally, evaluations are experiments; but the literature\non evaluations has largely ignored the literature from other sciences on\nexperiment analysis and planning. This article shows researchers with some\ntraining in statistics how to think about and analyze data from language model\nevaluations. Conceptualizing evaluation questions as having been drawn from an\nunseen super-population, we present formulas for analyzing evaluation data,\nmeasuring differences between two models, and planning an evaluation\nexperiment. We make a number of specific recommendations for running language\nmodel evaluations and reporting experiment results in a way that minimizes\nstatistical noise and maximizes informativeness.",
        "timestamp": "2025-06-10T23:48:18.338Z",
        "rating": "novote",
        "publishedDate": "2024-11-01T14:57:16Z",
        "tags": [
          "stat.AP",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 3,
        "object_id": "paper:arxiv.2411.00640",
        "created_at": "2025-06-10T23:48:18+00:00",
        "updated_at": "2025-06-10T23:48:37+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.23501": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.23501",
        "url": "https://arxiv.org/pdf/2410.23501",
        "title": "All or None: Identifiable Linear Properties of Next-token Predictors in\n  Language Modeling",
        "authors": "Emanuele Marconato, S\u00e9bastien Lachapelle, Sebastian Weichwald, Luigi Gresele",
        "abstract": "We analyze identifiability as a possible explanation for the ubiquity of\nlinear properties across language models, such as the vector difference between\nthe representations of \"easy\" and \"easiest\" being parallel to that between\n\"lucky\" and \"luckiest\". For this, we ask whether finding a linear property in\none model implies that any model that induces the same distribution has that\nproperty, too. To answer that, we first prove an identifiability result to\ncharacterize distribution-equivalent next-token predictors, lifting a diversity\nrequirement of previous results. Second, based on a refinement of relational\nlinearity [Paccanaro and Hinton, 2001; Hernandez et al., 2024], we show how\nmany notions of linearity are amenable to our analysis. Finally, we show that\nunder suitable conditions, these linear properties either hold in all or none\ndistribution-equivalent next-token predictors.",
        "timestamp": "2025-06-10T23:46:40.077Z",
        "rating": "novote",
        "publishedDate": "2024-10-30T23:19:29Z",
        "tags": [
          "stat.ML",
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2,
        "object_id": "paper:arxiv.2410.23501",
        "created_at": "2025-06-10T23:46:40+00:00",
        "updated_at": "2025-06-10T23:47:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.08679": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08679",
        "url": "https://arxiv.org/pdf/2503.08679",
        "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
        "authors": "Iv\u00e1n Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy",
        "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful, i.e. CoT reasoning does not always reflect how models arrive\nat conclusions. So far, most of these studies have focused on unfaithfulness in\nunnatural contexts where an explicit bias has been introduced. In contrast, we\nshow that unfaithful CoT can occur on realistic prompts with no artificial\nbias. Our results reveal non-negligible rates of several forms of unfaithful\nreasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and\nChatGPT-4o (7.0%) all answer a notable proportion of question pairs\nunfaithfully. Specifically, we find that models rationalize their implicit\nbiases in answers to binary questions (\"implicit post-hoc rationalization\").\nFor example, when separately presented with the questions \"Is X bigger than Y?\"\nand \"Is Y bigger than X?\", models sometimes produce superficially coherent\narguments to justify answering Yes to both questions or No to both questions,\ndespite such responses being logically contradictory. We also investigate\nrestoration errors (Dziri et al., 2023), where models make and then silently\ncorrect errors in their reasoning, and unfaithful shortcuts, where models use\nclearly illogical reasoning to simplify solving problems in Putnam questions (a\nhard benchmark). Our findings raise challenges for AI safety work that relies\non monitoring CoT to detect undesired behavior.",
        "timestamp": "2025-06-10T23:57:09.917Z",
        "rating": "novote",
        "publishedDate": "2025-03-11T17:56:30Z",
        "tags": [
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 6,
        "object_id": "paper:arxiv.2503.08679",
        "created_at": "2025-06-10T23:57:10+00:00",
        "updated_at": "2025-06-10T23:57:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.04741": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.04741",
        "url": "https://arxiv.org/pdf/2505.04741",
        "title": "When Bad Data Leads to Good Models",
        "authors": "Kenneth Li, Yida Chen, Fernanda Vi\u00e9gas, Martin Wattenberg",
        "abstract": "In large language model (LLM) pretraining, data quality is believed to\ndetermine model quality. In this paper, we re-examine the notion of \"quality\"\nfrom the perspective of pre- and post-training co-design. Specifically, we\nexplore the possibility that pre-training on more toxic data can lead to better\ncontrol in post-training, ultimately decreasing a model's output toxicity.\nFirst, we use a toy experiment to study how data composition affects the\ngeometry of features in the representation space. Next, through controlled\nexperiments with Olmo-1B models trained on varying ratios of clean and toxic\ndata, we find that the concept of toxicity enjoys a less entangled linear\nrepresentation as the proportion of toxic data increases. Furthermore, we show\nthat although toxic data increases the generational toxicity of the base model,\nit also makes the toxicity easier to remove. Evaluations on Toxigen and Real\nToxicity Prompts demonstrate that models trained on toxic data achieve a better\ntrade-off between reducing generational toxicity and preserving general\ncapabilities when detoxifying techniques such as inference-time intervention\n(ITI) are applied. Our findings suggest that, with post-training taken into\naccount, bad data may lead to good models.",
        "timestamp": "2025-06-10T23:57:10.004Z",
        "rating": "novote",
        "publishedDate": "2025-05-07T19:17:49Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 5,
        "object_id": "paper:arxiv.2505.04741",
        "created_at": "2025-06-10T23:57:10+00:00",
        "updated_at": "2025-06-10T23:57:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.13775": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.13775",
        "url": "https://arxiv.org/pdf/2505.13775",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-11T00:00:45.778Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 8,
        "object_id": "paper:arxiv.2505.13775",
        "created_at": "2025-06-11T00:00:46+00:00",
        "updated_at": "2025-06-11T00:01:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.03714": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.03714",
        "url": "https://arxiv.org/pdf/2310.03714",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-11T00:00:36.952Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 7,
        "object_id": "paper:arxiv.2310.03714",
        "created_at": "2025-06-11T00:00:37+00:00",
        "updated_at": "2025-06-11T00:00:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.03001": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.03001",
        "interactions": []
      },
      "meta": {
        "issue_number": 10,
        "object_id": "interactions:arxiv.2410.03001",
        "created_at": "2025-06-11T00:03:29+00:00",
        "updated_at": "2025-06-11T00:03:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.03001": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.03001",
        "url": "https://arxiv.org/pdf/2410.03001",
        "title": "Can Transformers Learn $n$-gram Language Models?",
        "authors": "Anej Svete, Nadav Borenstein, Mike Zhou, Isabelle Augenstein, Ryan Cotterell",
        "abstract": "Much theoretical work has described the ability of transformers to represent\nformal languages. However, linking theoretical results to empirical performance\nis not straightforward due to the complex interplay between the architecture,\nthe learning algorithm, and training data. To test whether theoretical lower\nbounds imply \\emph{learnability} of formal languages, we turn to recent work\nrelating transformers to $n$-gram language models (LMs). We study transformers'\nability to learn random $n$-gram LMs of two kinds: ones with arbitrary\nnext-symbol probabilities and ones where those are defined with shared\nparameters. We find that classic estimation techniques for $n$-gram LMs such as\nadd-$\\lambda$ smoothing outperform transformers on the former, while\ntransformers perform better on the latter, outperforming methods specifically\ndesigned to learn $n$-gram LMs.",
        "timestamp": "2025-06-11T00:03:19.769Z",
        "rating": "novote",
        "publishedDate": "2024-10-03T21:21:02Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 9,
        "object_id": "paper:arxiv.2410.03001",
        "created_at": "2025-06-11T00:03:20+00:00",
        "updated_at": "2025-06-11T00:03:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.06264": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.06264",
        "url": "https://arxiv.org/abs/2412.06264",
        "title": "Flow Matching Guide and Code",
        "authors": "Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, Itai Gat",
        "abstract": "Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM.",
        "timestamp": "2025-06-11T13:52:37.558Z",
        "rating": "novote",
        "publishedDate": "2024/12/09",
        "tags": [
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 14,
        "object_id": "paper:arxiv.2412.06264",
        "created_at": "2025-06-11T13:52:37+00:00",
        "updated_at": "2025-06-11T13:53:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.02867": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.02867",
        "url": "https://arxiv.org/abs/2506.02867v1",
        "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning",
        "authors": "Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, Jing Shao",
        "abstract": "Large reasoning models (LRMs) have demonstrated impressive capabilities in complex problem-solving, yet their internal reasoning mechanisms remain poorly understood. In this paper, we investigate the reasoning trajectories of LRMs from an information-theoretic perspective. By tracking how mutual information (MI) between intermediate representations and the correct answer evolves during LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at specific generative steps exhibits a sudden and significant increase during LRM's reasoning process. We theoretically analyze such phenomenon and show that as MI increases, the probability of model's prediction error decreases. Furthermore, these MI peaks often correspond to tokens expressing reflection or transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the thinking tokens. We then demonstrate that these thinking tokens are crucial for LRM's reasoning performance, while other tokens has minimal impacts. Building on these analyses, we propose two simple yet effective methods to improve LRM's reasoning performance, by delicately leveraging these thinking tokens. Overall, our work provides novel insights into the reasoning mechanisms of LRMs and offers practical ways to improve their reasoning capabilities. The code is available at this https URL.",
        "timestamp": "2025-06-11T13:52:35.681Z",
        "rating": "novote",
        "publishedDate": "2025/06/03",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 13,
        "object_id": "paper:arxiv.2506.02867",
        "created_at": "2025-06-11T13:52:35+00:00",
        "updated_at": "2025-06-11T13:52:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.01939": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.01939",
        "url": "https://arxiv.org/abs/2506.01939",
        "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
        "authors": "Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, Junyang Lin",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.",
        "timestamp": "2025-06-11T13:52:34.922Z",
        "rating": "novote",
        "publishedDate": "2025/06/02",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 12,
        "object_id": "paper:arxiv.2506.01939",
        "created_at": "2025-06-11T13:52:35+00:00",
        "updated_at": "2025-06-11T13:52:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.18128": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.18128",
        "url": "https://arxiv.org/abs/2505.18128",
        "title": "Frankentext: Stitching random text fragments into long-form narratives",
        "authors": "Chau Minh Pham, Jenna Russell, Dzung Pham, Mohit Iyyer",
        "abstract": "We introduce Frankentexts, a new type of long-form narratives produced by LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from human writings. This task presents a challenging test of controllable generation, requiring models to satisfy a writing prompt, integrate disparate text fragments, and still produce a coherent narrative. To generate Frankentexts, we instruct the model to produce a draft by selecting and combining human-written passages, then iteratively revise the draft while maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts along three axes: writing quality, instruction adherence, and detectability. Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts are coherent and 100% relevant to the prompt. Notably, up to 59% of these outputs are misclassified as human-written by detectors like Pangram, revealing limitations in AI text detectors. Human annotators can sometimes identify Frankentexts through their abrupt tone shifts and inconsistent grammar between segments, especially in longer generations. Beyond presenting a challenging generation task, Frankentexts invite discussion on building effective detectors for this new grey zone of authorship, provide training data for mixed authorship detection, and serve as a sandbox for studying human-AI co-writing processes.",
        "timestamp": "2025-06-11T13:52:32.703Z",
        "rating": "novote",
        "publishedDate": "2025/05/23",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 11,
        "object_id": "paper:arxiv.2505.18128",
        "created_at": "2025-06-11T13:52:32+00:00",
        "updated_at": "2025-06-11T13:52:51+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.12821": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.12821",
        "url": "https://arxiv.org/pdf/2502.12821",
        "title": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in\n  Large Language Models",
        "authors": "Elena Stringli, Maria Lymperaiou, Giorgos Filandrianos, Athanasios Voulodimos, Giorgos Stamou",
        "abstract": "Inverse tasks can uncover potential reasoning gaps as Large Language Models\n(LLMs) scale up. In this work, we explore the redefinition task, in which we\nassign alternative values to well-known physical constants and units of\nmeasure, prompting LLMs to respond accordingly. Our findings show that not only\ndoes model performance degrade with scale, but its false confidence also rises.\nMoreover, while factors such as prompting strategies or response formatting are\ninfluential, they do not preclude LLMs from anchoring to memorized values.",
        "timestamp": "2025-06-11T14:53:10.606Z",
        "rating": "novote",
        "publishedDate": "2025-02-18T12:32:11Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 15,
        "object_id": "paper:arxiv.2502.12821",
        "created_at": "2025-06-11T14:53:10+00:00",
        "updated_at": "2025-06-11T14:53:29+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.07830": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.07830",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-11T17:44:34.634Z",
            "data": {
              "session_id": "session_1749663874214_sl3ztf9",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-06-11T17:44:23.108Z",
              "end_time": "2025-06-11T17:44:34.214Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-11T17:49:17.585Z",
            "data": {
              "session_id": "session_1749664157581_9tp4gxw",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-06-11T17:49:09.817Z",
              "end_time": "2025-06-11T17:49:17.581Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 18,
        "object_id": "interactions:arxiv.2502.07830",
        "created_at": "2025-06-11T17:44:35+00:00",
        "updated_at": "2025-06-11T17:49:40+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.07830": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.07830",
        "url": "https://arxiv.org/pdf/2502.07830",
        "title": "Captured by Captions: On Memorization and its Mitigation in CLIP Models",
        "authors": "Wenhao Wang, Adam Dziedzic, Grace C. Kim, Michael Backes, Franziska Boenisch",
        "abstract": "Multi-modal models, such as CLIP, have demonstrated strong performance in\naligning visual and textual representations, excelling in tasks like image\nretrieval and zero-shot classification. Despite this success, the mechanisms by\nwhich these models utilize training data, particularly the role of\nmemorization, remain unclear. In uni-modal models, both supervised and\nself-supervised, memorization has been shown to be essential for\ngeneralization. However, it is not well understood how these findings would\napply to CLIP, which incorporates elements from both supervised learning via\ncaptions that provide a supervisory signal similar to labels, and from\nself-supervised learning via the contrastive objective. To bridge this gap in\nunderstanding, we propose a formal definition of memorization in CLIP (CLIPMem)\nand use it to quantify memorization in CLIP models. Our results indicate that\nCLIP's memorization behavior falls between the supervised and self-supervised\nparadigms, with \"mis-captioned\" samples exhibiting highest levels of\nmemorization. Additionally, we find that the text encoder contributes more to\nmemorization than the image encoder, suggesting that mitigation strategies\nshould focus on the text domain. Building on these insights, we propose\nmultiple strategies to reduce memorization while at the same time improving\nutility--something that had not been shown before for traditional learning\nparadigms where reducing memorization typically results in utility decrease.",
        "timestamp": "2025-06-11T17:44:23.430Z",
        "rating": "novote",
        "publishedDate": "2025-02-11T00:11:13Z",
        "tags": [
          "cs.CV",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 17,
        "object_id": "paper:arxiv.2502.07830",
        "created_at": "2025-06-11T17:44:23+00:00",
        "updated_at": "2025-06-11T17:44:44+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.04265": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.04265",
        "url": "https://arxiv.org/pdf/2410.04265",
        "title": "AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language\n  Models via Systematic Attribution of Machine Text against Web Text",
        "authors": "Ximing Lu, Melanie Sclar, Skyler Hallinan, Niloofar Mireshghallah, Jiacheng Liu, Seungju Han, Allyson Ettinger, Liwei Jiang, Khyathi Chandu, Nouha Dziri, Yejin Choi",
        "abstract": "Creativity has long been considered one of the most difficult aspect of human\nintelligence for AI to mimic. However, the rise of Large Language Models\n(LLMs), like ChatGPT, has raised questions about whether AI can match or even\nsurpass human creativity. We present CREATIVITY INDEX as the first step to\nquantify the linguistic creativity of a text by reconstructing it from existing\ntext snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that\nthe seemingly remarkable creativity of LLMs may be attributable in large part\nto the creativity of human-written texts on the web. To compute CREATIVITY\nINDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming\nalgorithm that can search verbatim and near-verbatim matches of text snippets\nfrom a given document against the web. Experiments reveal that the CREATIVITY\nINDEX of professional human authors is on average 66.2% higher than that of\nLLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of\n30.1%. In addition, we find that distinguished authors like Hemingway exhibit\nmeasurably higher CREATIVITY INDEX compared to other human writers. Finally, we\ndemonstrate that CREATIVITY INDEX can be used as a surprisingly effective\ncriterion for zero-shot machine text detection, surpassing the strongest\nexisting zero-shot system, DetectGPT, by a significant margin of 30.2%, and\neven outperforming the strongest supervised system, GhostBuster, in five out of\nsix domains.",
        "timestamp": "2025-06-11T17:44:07.863Z",
        "rating": "novote",
        "publishedDate": "2024-10-05T18:55:01Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 16,
        "object_id": "paper:arxiv.2410.04265",
        "created_at": "2025-06-11T17:44:08+00:00",
        "updated_at": "2025-06-11T17:44:27+00:00",
        "version": 1
      }
    }
  }
}