{
  "snapshot_time": "2025-10-17T17:25:02.023460+00:00",
  "repository": "yanaiela/papers-feed",
  "objects": {
    "interactions:arxiv.2411.00640": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.00640",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-10T23:48:26.492Z",
            "data": {
              "session_id": "session_1749599306052_fap3gto",
              "source_id": "arxiv",
              "paper_id": "2411.00640",
              "start_time": "2025-06-10T23:48:18.158Z",
              "end_time": "2025-06-10T23:48:26.052Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:33:25.086Z",
            "data": {
              "session_id": "session_1750977204673_9k6mbqz",
              "source_id": "arxiv",
              "paper_id": "2411.00640",
              "start_time": "2025-06-26T22:33:18.549Z",
              "end_time": "2025-06-26T22:33:24.673Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 4,
        "object_id": "interactions:arxiv.2411.00640",
        "created_at": "2025-06-10T23:48:27+00:00",
        "updated_at": "2025-06-26T22:33:51+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.00640": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.00640",
        "url": "https://arxiv.org/pdf/2411.00640",
        "title": "Adding Error Bars to Evals: A Statistical Approach to Language Model\n  Evaluations",
        "authors": "Evan Miller",
        "abstract": "Evaluations are critical for understanding the capabilities of large language\nmodels (LLMs). Fundamentally, evaluations are experiments; but the literature\non evaluations has largely ignored the literature from other sciences on\nexperiment analysis and planning. This article shows researchers with some\ntraining in statistics how to think about and analyze data from language model\nevaluations. Conceptualizing evaluation questions as having been drawn from an\nunseen super-population, we present formulas for analyzing evaluation data,\nmeasuring differences between two models, and planning an evaluation\nexperiment. We make a number of specific recommendations for running language\nmodel evaluations and reporting experiment results in a way that minimizes\nstatistical noise and maximizes informativeness.",
        "timestamp": "2025-06-10T23:48:18.338Z",
        "rating": "novote",
        "publishedDate": "2024-11-01T14:57:16Z",
        "tags": [
          "stat.AP",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 3,
        "object_id": "paper:arxiv.2411.00640",
        "created_at": "2025-06-10T23:48:18+00:00",
        "updated_at": "2025-06-10T23:48:37+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.23501": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.23501",
        "url": "https://arxiv.org/pdf/2410.23501",
        "title": "All or None: Identifiable Linear Properties of Next-token Predictors in\n  Language Modeling",
        "authors": "Emanuele Marconato, S\u00e9bastien Lachapelle, Sebastian Weichwald, Luigi Gresele",
        "abstract": "We analyze identifiability as a possible explanation for the ubiquity of\nlinear properties across language models, such as the vector difference between\nthe representations of \"easy\" and \"easiest\" being parallel to that between\n\"lucky\" and \"luckiest\". For this, we ask whether finding a linear property in\none model implies that any model that induces the same distribution has that\nproperty, too. To answer that, we first prove an identifiability result to\ncharacterize distribution-equivalent next-token predictors, lifting a diversity\nrequirement of previous results. Second, based on a refinement of relational\nlinearity [Paccanaro and Hinton, 2001; Hernandez et al., 2024], we show how\nmany notions of linearity are amenable to our analysis. Finally, we show that\nunder suitable conditions, these linear properties either hold in all or none\ndistribution-equivalent next-token predictors.",
        "timestamp": "2025-06-10T23:46:40.077Z",
        "rating": "novote",
        "publishedDate": "2024-10-30T23:19:29Z",
        "tags": [
          "stat.ML",
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 2,
        "object_id": "paper:arxiv.2410.23501",
        "created_at": "2025-06-10T23:46:40+00:00",
        "updated_at": "2025-06-10T23:47:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.08679": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08679",
        "url": "https://arxiv.org/pdf/2503.08679",
        "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
        "authors": "Iv\u00e1n Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy",
        "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful, i.e. CoT reasoning does not always reflect how models arrive\nat conclusions. So far, most of these studies have focused on unfaithfulness in\nunnatural contexts where an explicit bias has been introduced. In contrast, we\nshow that unfaithful CoT can occur on realistic prompts with no artificial\nbias. Our results reveal non-negligible rates of several forms of unfaithful\nreasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and\nChatGPT-4o (7.0%) all answer a notable proportion of question pairs\nunfaithfully. Specifically, we find that models rationalize their implicit\nbiases in answers to binary questions (\"implicit post-hoc rationalization\").\nFor example, when separately presented with the questions \"Is X bigger than Y?\"\nand \"Is Y bigger than X?\", models sometimes produce superficially coherent\narguments to justify answering Yes to both questions or No to both questions,\ndespite such responses being logically contradictory. We also investigate\nrestoration errors (Dziri et al., 2023), where models make and then silently\ncorrect errors in their reasoning, and unfaithful shortcuts, where models use\nclearly illogical reasoning to simplify solving problems in Putnam questions (a\nhard benchmark). Our findings raise challenges for AI safety work that relies\non monitoring CoT to detect undesired behavior.",
        "timestamp": "2025-06-10T23:57:09.917Z",
        "rating": "novote",
        "publishedDate": "2025-03-11T17:56:30Z",
        "tags": [
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 6,
        "object_id": "paper:arxiv.2503.08679",
        "created_at": "2025-06-10T23:57:10+00:00",
        "updated_at": "2025-06-10T23:57:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.04741": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.04741",
        "url": "https://arxiv.org/pdf/2505.04741",
        "title": "When Bad Data Leads to Good Models",
        "authors": "Kenneth Li, Yida Chen, Fernanda Vi\u00e9gas, Martin Wattenberg",
        "abstract": "In large language model (LLM) pretraining, data quality is believed to\ndetermine model quality. In this paper, we re-examine the notion of \"quality\"\nfrom the perspective of pre- and post-training co-design. Specifically, we\nexplore the possibility that pre-training on more toxic data can lead to better\ncontrol in post-training, ultimately decreasing a model's output toxicity.\nFirst, we use a toy experiment to study how data composition affects the\ngeometry of features in the representation space. Next, through controlled\nexperiments with Olmo-1B models trained on varying ratios of clean and toxic\ndata, we find that the concept of toxicity enjoys a less entangled linear\nrepresentation as the proportion of toxic data increases. Furthermore, we show\nthat although toxic data increases the generational toxicity of the base model,\nit also makes the toxicity easier to remove. Evaluations on Toxigen and Real\nToxicity Prompts demonstrate that models trained on toxic data achieve a better\ntrade-off between reducing generational toxicity and preserving general\ncapabilities when detoxifying techniques such as inference-time intervention\n(ITI) are applied. Our findings suggest that, with post-training taken into\naccount, bad data may lead to good models.",
        "timestamp": "2025-06-10T23:57:10.004Z",
        "rating": "novote",
        "publishedDate": "2025-05-07T19:17:49Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 5,
        "object_id": "paper:arxiv.2505.04741",
        "created_at": "2025-06-10T23:57:10+00:00",
        "updated_at": "2025-06-10T23:57:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.13775": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.13775",
        "url": "https://arxiv.org/pdf/2505.13775",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-11T00:00:45.778Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 8,
        "object_id": "paper:arxiv.2505.13775",
        "created_at": "2025-06-11T00:00:46+00:00",
        "updated_at": "2025-06-11T00:01:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.03714": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.03714",
        "url": "https://arxiv.org/pdf/2310.03714",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-11T00:00:36.952Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 7,
        "object_id": "paper:arxiv.2310.03714",
        "created_at": "2025-06-11T00:00:37+00:00",
        "updated_at": "2025-06-11T00:00:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.03001": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.03001",
        "interactions": []
      },
      "meta": {
        "issue_number": 10,
        "object_id": "interactions:arxiv.2410.03001",
        "created_at": "2025-06-11T00:03:29+00:00",
        "updated_at": "2025-06-11T00:03:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.03001": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.03001",
        "url": "https://arxiv.org/pdf/2410.03001",
        "title": "Can Transformers Learn $n$-gram Language Models?",
        "authors": "Anej Svete, Nadav Borenstein, Mike Zhou, Isabelle Augenstein, Ryan Cotterell",
        "abstract": "Much theoretical work has described the ability of transformers to represent\nformal languages. However, linking theoretical results to empirical performance\nis not straightforward due to the complex interplay between the architecture,\nthe learning algorithm, and training data. To test whether theoretical lower\nbounds imply \\emph{learnability} of formal languages, we turn to recent work\nrelating transformers to $n$-gram language models (LMs). We study transformers'\nability to learn random $n$-gram LMs of two kinds: ones with arbitrary\nnext-symbol probabilities and ones where those are defined with shared\nparameters. We find that classic estimation techniques for $n$-gram LMs such as\nadd-$\\lambda$ smoothing outperform transformers on the former, while\ntransformers perform better on the latter, outperforming methods specifically\ndesigned to learn $n$-gram LMs.",
        "timestamp": "2025-06-11T00:03:19.769Z",
        "rating": "novote",
        "publishedDate": "2024-10-03T21:21:02Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 9,
        "object_id": "paper:arxiv.2410.03001",
        "created_at": "2025-06-11T00:03:20+00:00",
        "updated_at": "2025-06-11T00:03:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.06264": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.06264",
        "url": "https://arxiv.org/abs/2412.06264",
        "title": "Flow Matching Guide and Code",
        "authors": "Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, Itai Gat",
        "abstract": "Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM.",
        "timestamp": "2025-06-11T13:52:37.558Z",
        "rating": "novote",
        "publishedDate": "2024/12/09",
        "tags": [
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 14,
        "object_id": "paper:arxiv.2412.06264",
        "created_at": "2025-06-11T13:52:37+00:00",
        "updated_at": "2025-06-11T13:53:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.02867": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.02867",
        "url": "https://arxiv.org/abs/2506.02867v1",
        "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning",
        "authors": "Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, Jing Shao",
        "abstract": "Large reasoning models (LRMs) have demonstrated impressive capabilities in complex problem-solving, yet their internal reasoning mechanisms remain poorly understood. In this paper, we investigate the reasoning trajectories of LRMs from an information-theoretic perspective. By tracking how mutual information (MI) between intermediate representations and the correct answer evolves during LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at specific generative steps exhibits a sudden and significant increase during LRM's reasoning process. We theoretically analyze such phenomenon and show that as MI increases, the probability of model's prediction error decreases. Furthermore, these MI peaks often correspond to tokens expressing reflection or transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the thinking tokens. We then demonstrate that these thinking tokens are crucial for LRM's reasoning performance, while other tokens has minimal impacts. Building on these analyses, we propose two simple yet effective methods to improve LRM's reasoning performance, by delicately leveraging these thinking tokens. Overall, our work provides novel insights into the reasoning mechanisms of LRMs and offers practical ways to improve their reasoning capabilities. The code is available at this https URL.",
        "timestamp": "2025-06-11T13:52:35.681Z",
        "rating": "novote",
        "publishedDate": "2025/06/03",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 13,
        "object_id": "paper:arxiv.2506.02867",
        "created_at": "2025-06-11T13:52:35+00:00",
        "updated_at": "2025-06-11T13:52:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.01939": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.01939",
        "url": "https://arxiv.org/abs/2506.01939",
        "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
        "authors": "Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, Junyang Lin",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.",
        "timestamp": "2025-06-11T13:52:34.922Z",
        "rating": "novote",
        "publishedDate": "2025/06/02",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 12,
        "object_id": "paper:arxiv.2506.01939",
        "created_at": "2025-06-11T13:52:35+00:00",
        "updated_at": "2025-06-11T13:52:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.18128": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.18128",
        "url": "https://arxiv.org/abs/2505.18128",
        "title": "Frankentext: Stitching random text fragments into long-form narratives",
        "authors": "Chau Minh Pham, Jenna Russell, Dzung Pham, Mohit Iyyer",
        "abstract": "We introduce Frankentexts, a new type of long-form narratives produced by LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from human writings. This task presents a challenging test of controllable generation, requiring models to satisfy a writing prompt, integrate disparate text fragments, and still produce a coherent narrative. To generate Frankentexts, we instruct the model to produce a draft by selecting and combining human-written passages, then iteratively revise the draft while maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts along three axes: writing quality, instruction adherence, and detectability. Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts are coherent and 100% relevant to the prompt. Notably, up to 59% of these outputs are misclassified as human-written by detectors like Pangram, revealing limitations in AI text detectors. Human annotators can sometimes identify Frankentexts through their abrupt tone shifts and inconsistent grammar between segments, especially in longer generations. Beyond presenting a challenging generation task, Frankentexts invite discussion on building effective detectors for this new grey zone of authorship, provide training data for mixed authorship detection, and serve as a sandbox for studying human-AI co-writing processes.",
        "timestamp": "2025-06-11T13:52:32.703Z",
        "rating": "novote",
        "publishedDate": "2025/05/23",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 11,
        "object_id": "paper:arxiv.2505.18128",
        "created_at": "2025-06-11T13:52:32+00:00",
        "updated_at": "2025-06-11T13:52:51+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.12821": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.12821",
        "url": "https://arxiv.org/pdf/2502.12821",
        "title": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in\n  Large Language Models",
        "authors": "Elena Stringli, Maria Lymperaiou, Giorgos Filandrianos, Athanasios Voulodimos, Giorgos Stamou",
        "abstract": "Inverse tasks can uncover potential reasoning gaps as Large Language Models\n(LLMs) scale up. In this work, we explore the redefinition task, in which we\nassign alternative values to well-known physical constants and units of\nmeasure, prompting LLMs to respond accordingly. Our findings show that not only\ndoes model performance degrade with scale, but its false confidence also rises.\nMoreover, while factors such as prompting strategies or response formatting are\ninfluential, they do not preclude LLMs from anchoring to memorized values.",
        "timestamp": "2025-06-11T14:53:10.606Z",
        "rating": "novote",
        "publishedDate": "2025-02-18T12:32:11Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 15,
        "object_id": "paper:arxiv.2502.12821",
        "created_at": "2025-06-11T14:53:10+00:00",
        "updated_at": "2025-06-11T14:53:29+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.07830": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.07830",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-11T17:44:34.634Z",
            "data": {
              "session_id": "session_1749663874214_sl3ztf9",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-06-11T17:44:23.108Z",
              "end_time": "2025-06-11T17:44:34.214Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-11T17:49:17.585Z",
            "data": {
              "session_id": "session_1749664157581_9tp4gxw",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-06-11T17:49:09.817Z",
              "end_time": "2025-06-11T17:49:17.581Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-16T21:05:46.244Z",
            "data": {
              "session_id": "session_1750107946228_u4xrade",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-06-16T21:05:29.068Z",
              "end_time": "2025-06-16T21:05:46.228Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-24T19:42:27.583Z",
            "data": {
              "session_id": "session_1750794147138_l03eij8",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-06-24T19:39:06.818Z",
              "end_time": "2025-06-24T19:42:27.138Z",
              "heartbeat_count": 40,
              "duration_seconds": 200,
              "idle_seconds": 0,
              "total_elapsed_seconds": 200
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T15:15:50.678Z",
            "data": {
              "session_id": "session_1751555750206_a7hcdap",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-07-03T15:15:38.078Z",
              "end_time": "2025-07-03T15:15:50.205Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T20:54:56.150Z",
            "data": {
              "session_id": "session_1751576095762_x6taswx",
              "source_id": "arxiv",
              "paper_id": "2502.07830",
              "start_time": "2025-07-03T20:53:56.309Z",
              "end_time": "2025-07-03T20:54:55.762Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 4,
              "total_elapsed_seconds": 59
            }
          }
        ]
      },
      "meta": {
        "issue_number": 18,
        "object_id": "interactions:arxiv.2502.07830",
        "created_at": "2025-06-11T17:44:35+00:00",
        "updated_at": "2025-07-03T20:55:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.07830": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.07830",
        "url": "https://arxiv.org/pdf/2502.07830",
        "title": "Captured by Captions: On Memorization and its Mitigation in CLIP Models",
        "authors": "Wenhao Wang, Adam Dziedzic, Grace C. Kim, Michael Backes, Franziska Boenisch",
        "abstract": "Multi-modal models, such as CLIP, have demonstrated strong performance in\naligning visual and textual representations, excelling in tasks like image\nretrieval and zero-shot classification. Despite this success, the mechanisms by\nwhich these models utilize training data, particularly the role of\nmemorization, remain unclear. In uni-modal models, both supervised and\nself-supervised, memorization has been shown to be essential for\ngeneralization. However, it is not well understood how these findings would\napply to CLIP, which incorporates elements from both supervised learning via\ncaptions that provide a supervisory signal similar to labels, and from\nself-supervised learning via the contrastive objective. To bridge this gap in\nunderstanding, we propose a formal definition of memorization in CLIP (CLIPMem)\nand use it to quantify memorization in CLIP models. Our results indicate that\nCLIP's memorization behavior falls between the supervised and self-supervised\nparadigms, with \"mis-captioned\" samples exhibiting highest levels of\nmemorization. Additionally, we find that the text encoder contributes more to\nmemorization than the image encoder, suggesting that mitigation strategies\nshould focus on the text domain. Building on these insights, we propose\nmultiple strategies to reduce memorization while at the same time improving\nutility--something that had not been shown before for traditional learning\nparadigms where reducing memorization typically results in utility decrease.",
        "timestamp": "2025-06-11T17:44:23.430Z",
        "rating": "novote",
        "publishedDate": "2025-02-11T00:11:13Z",
        "tags": [
          "cs.CV",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 17,
        "object_id": "paper:arxiv.2502.07830",
        "created_at": "2025-06-11T17:44:23+00:00",
        "updated_at": "2025-06-11T17:44:44+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.04265": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.04265",
        "url": "https://arxiv.org/pdf/2410.04265",
        "title": "AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language\n  Models via Systematic Attribution of Machine Text against Web Text",
        "authors": "Ximing Lu, Melanie Sclar, Skyler Hallinan, Niloofar Mireshghallah, Jiacheng Liu, Seungju Han, Allyson Ettinger, Liwei Jiang, Khyathi Chandu, Nouha Dziri, Yejin Choi",
        "abstract": "Creativity has long been considered one of the most difficult aspect of human\nintelligence for AI to mimic. However, the rise of Large Language Models\n(LLMs), like ChatGPT, has raised questions about whether AI can match or even\nsurpass human creativity. We present CREATIVITY INDEX as the first step to\nquantify the linguistic creativity of a text by reconstructing it from existing\ntext snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that\nthe seemingly remarkable creativity of LLMs may be attributable in large part\nto the creativity of human-written texts on the web. To compute CREATIVITY\nINDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming\nalgorithm that can search verbatim and near-verbatim matches of text snippets\nfrom a given document against the web. Experiments reveal that the CREATIVITY\nINDEX of professional human authors is on average 66.2% higher than that of\nLLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of\n30.1%. In addition, we find that distinguished authors like Hemingway exhibit\nmeasurably higher CREATIVITY INDEX compared to other human writers. Finally, we\ndemonstrate that CREATIVITY INDEX can be used as a surprisingly effective\ncriterion for zero-shot machine text detection, surpassing the strongest\nexisting zero-shot system, DetectGPT, by a significant margin of 30.2%, and\neven outperforming the strongest supervised system, GhostBuster, in five out of\nsix domains.",
        "timestamp": "2025-06-11T17:44:07.863Z",
        "rating": "novote",
        "publishedDate": "2024-10-05T18:55:01Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 16,
        "object_id": "paper:arxiv.2410.04265",
        "created_at": "2025-06-11T17:44:08+00:00",
        "updated_at": "2025-06-11T17:44:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.07684": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.07684",
        "url": "https://arxiv.org/pdf/2412.07684",
        "title": "The Pitfalls of Memorization: When Memorization Hurts Generalization",
        "authors": "Reza Bayat, Mohammad Pezeshki, Elvis Dohmatob, David Lopez-Paz, Pascal Vincent",
        "abstract": "Neural networks often learn simple explanations that fit the majority of the\ndata while memorizing exceptions that deviate from these explanations.This\nbehavior leads to poor generalization when the learned explanations rely on\nspurious correlations. In this work, we formalize the interplay between\nmemorization and generalization, showing that spurious correlations would\nparticularly lead to poor generalization when are combined with memorization.\nMemorization can reduce training loss to zero, leaving no incentive to learn\nrobust, generalizable patterns. To address this, we propose memorization-aware\ntraining (MAT), which uses held-out predictions as a signal of memorization to\nshift a model's logits. MAT encourages learning robust patterns invariant\nacross distributions, improving generalization under distribution shifts.",
        "timestamp": "2025-06-11T21:24:39.795Z",
        "rating": "novote",
        "publishedDate": "2024-12-10T17:18:33Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 21,
        "object_id": "paper:arxiv.2412.07684",
        "created_at": "2025-06-11T21:24:40+00:00",
        "updated_at": "2025-06-11T21:24:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.01769": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.01769",
        "url": "https://arxiv.org/pdf/2410.01769?",
        "title": "Quantifying Generalization Complexity for Large Language Models",
        "authors": "Zhenting Qi, Hongyin Luo, Xuliang Huang, Zhuokai Zhao, Yibo Jiang, Xiangjun Fan, Himabindu Lakkaraju, James Glass",
        "abstract": "While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities.",
        "timestamp": "2025-06-11T21:24:39.623Z",
        "rating": "novote",
        "publishedDate": "2024-10-02T17:25:37Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 20,
        "object_id": "paper:arxiv.2410.01769",
        "created_at": "2025-06-11T21:24:39+00:00",
        "updated_at": "2025-06-11T21:25:03+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.14985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.14985",
        "url": "https://arxiv.org/pdf/2407.14985",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-11T21:24:11.679Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 19,
        "object_id": "paper:arxiv.2407.14985",
        "created_at": "2025-06-11T21:24:11+00:00",
        "updated_at": "2025-06-11T21:24:36+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.21530": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.21530",
        "url": "https://arxiv.org/abs/2407.21530",
        "title": "Data Contamination Report from the 2024 CONDA Shared Task",
        "authors": "Oscar Sainz, Iker Garc\u00eda-Ferrero, Alon Jacovi, Jon Ander Campos, Yanai Elazar, Eneko Agirre, Yoav Goldberg, Wei-Lin Chen, Jenny Chim, Leshem Choshen, Luca D'Amico-Wong, Melissa Dell, Run-Ze Fan, Shahriar Golchin, Yucheng Li, Pengfei Liu, Bhavish Pahwa, Ameya Prabhu, Suryansh Sharma, Emily Silcock, Kateryna Solonko, David Stap, Mihai Surdeanu, Yu-Min Tseng, Vishaal Udandarao, Zengzhi Wang, Ruijie Xu, Jinglin Yang",
        "abstract": "The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant aspects of data contamination in natural language processing, where data contamination is understood as situations where evaluation data is included in pre-training corpora used to train large scale models, compromising evaluation results. The workshop fostered a shared task to collect evidence on data contamination in current available datasets and models. The goal of the shared task and associated database is to assist the community in understanding the extent of the problem and to assist researchers in avoiding reporting evaluation results on known contaminated resources. The shared task provides a structured, centralized public database for the collection of contamination evidence, open to contributions from the community via GitHub pool requests. This first compilation paper is based on 566 reported entries over 91 contaminated sources from a total of 23 contributors. The details of the individual contamination events are available in the platform. The platform continues to be online, open to contributions from the community.",
        "timestamp": "2025-06-11T21:26:27.743Z",
        "rating": "novote",
        "publishedDate": "2024/07/31",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 22,
        "object_id": "paper:arxiv.2407.21530",
        "created_at": "2025-06-11T21:26:28+00:00",
        "updated_at": "2025-06-11T21:26:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.02810": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.02810",
        "url": "https://arxiv.org/abs/2504.02810",
        "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
        "authors": "Haowei Lin, Xiangyu Wang, Ruilin Yan, Baizhou Huang, Haotian Ye, Jianhua Zhu, Zihao Wang, James Zou, Jianzhu Ma, Yitao Liang",
        "abstract": "With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.",
        "timestamp": "2025-06-11T21:54:40.717Z",
        "rating": "novote",
        "publishedDate": "2025/04/03",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 23,
        "object_id": "paper:arxiv.2504.02810",
        "created_at": "2025-06-11T21:54:40+00:00",
        "updated_at": "2025-06-11T21:54:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.02126": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.02126",
        "url": "https://arxiv.org/pdf/2506.02126",
        "title": "Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains",
        "authors": "Juncheng Wu, Sheng Liu, Haoqin Tu, Hang Yu, Xiaoke Huang, James Zou, Cihang Xie, Yuyin Zhou",
        "abstract": "Recent advances in reasoning-enhanced Large Language Models such as\nOpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex\ntasks. However, the quality and transparency of their internal reasoning\nprocesses remain underexplored. This work moves beyond the final-answer\naccuracy and investigates step-by-step reasoning in the medical and\nmathematical domains by explicitly decomposing the thinking trajectories into\ntwo parts: knowledge and reasoning. Specifically, we introduce a fine-grained\nevaluation framework that judges: (1) the correctness of knowledge used\n(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured\nby Information Gain (InfoGain)). Using this framework, we study R1-distilled\nand base Qwen models trained with supervised fine-tuning (SFT) and/or\nreinforcement learning (RL) in the medical and math domains. Three intriguing\nfindings emerge: (1) The general reasoning abilities in R1-distilled models do\nnot transfer effectively to the medical domain through either SFT or RL. (2)\nSFT raises final-answer accuracy in both domains, but often at the cost of\nreasoning quality: InfoGain drops by 38.9% on average compared with untrained\nmodels; In the medical domain, however, SFT remains crucial because domain\nknowledge is indispensable. (3) RL enhances medical reasoning by pruning\ninaccurate or irrelevant knowledge from reasoning paths, thereby improving both\nreasoning accuracy and knowledge correctness.",
        "timestamp": "2025-06-12T17:19:47.018Z",
        "rating": "novote",
        "publishedDate": "2025-06-02T18:01:00Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 63,
        "object_id": "paper:arxiv.2506.02126",
        "created_at": "2025-06-12T17:19:47+00:00",
        "updated_at": "2025-06-12T17:20:22+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.03247": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.03247",
        "url": "https://arxiv.org/pdf/2408.03247",
        "title": "Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons",
        "authors": "Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, Daniel Dajun Zeng",
        "abstract": "In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.",
        "timestamp": "2025-06-12T17:19:47.017Z",
        "rating": "novote",
        "publishedDate": "2024-08-06T15:07:08Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 62,
        "object_id": "paper:arxiv.2408.03247",
        "created_at": "2025-06-12T17:19:47+00:00",
        "updated_at": "2025-06-12T17:20:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.09522": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09522",
        "url": "https://arxiv.org/pdf/2504.09522",
        "title": "How new data permeates LLM knowledge and how to dilute it",
        "authors": "Chen Sun, Renat Aksitov, Andrey Zhmoginov, Nolan Andrew Miller, Max Vladymyrov, Ulrich Rueckert, Been Kim, Mark Sandler",
        "abstract": "Large language models learn and continually learn through the accumulation of\ngradient-based updates, but how individual pieces of new information affect\nexisting knowledge, leading to both beneficial generalization and problematic\nhallucination, remains poorly understood. We demonstrate that when learning new\ninformation, LLMs exhibit a \"priming\" effect: learning a new fact can cause the\nmodel to inappropriately apply that knowledge in unrelated contexts. To\nsystematically study this phenomenon, we introduce \"Outlandish,\" a carefully\ncurated dataset of 1320 diverse text samples designed to probe how new\nknowledge permeates through an LLM's existing knowledge base. Using this\ndataset, we show that the degree of priming after learning new information can\nbe predicted by measuring the token probability of key words before learning.\nThis relationship holds robustly across different model architectures (PALM-2,\nGemma, Llama), sizes, and training stages. Finally, we develop two novel\ntechniques to modulate how new knowledge affects existing model behavior: (1) a\n``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update\npruning method. These approaches reduce undesirable priming effects by 50-95\\%\nwhile preserving the model's ability to learn new information. Our findings\nprovide both empirical insights into how LLMs learn and practical tools for\nimproving the specificity of knowledge insertion in language models. Further\nmaterials: https://sunchipsster1.github.io/projects/outlandish/",
        "timestamp": "2025-06-12T17:19:46.979Z",
        "rating": "novote",
        "publishedDate": "2025-04-13T11:25:04Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 61,
        "object_id": "paper:arxiv.2504.09522",
        "created_at": "2025-06-12T17:19:47+00:00",
        "updated_at": "2025-06-12T17:20:18+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.14685": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.14685",
        "url": "https://arxiv.org/pdf/2505.14685",
        "title": "Language Models use Lookbacks to Track Beliefs",
        "authors": "Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma, Christoph Riedl, Yonatan Belinkov, Tamar Rott Shaham, David Bau, Atticus Geiger",
        "abstract": "How do language models (LMs) represent characters' beliefs, especially when\nthose beliefs may differ from reality? This question lies at the heart of\nunderstanding the Theory of Mind (ToM) capabilities of LMs. We analyze\nLlama-3-70B-Instruct's ability to reason about characters' beliefs using causal\nmediation and abstraction. We construct a dataset that consists of simple\nstories where two characters each separately change the state of two objects,\npotentially unaware of each other's actions. Our investigation uncovered a\npervasive algorithmic pattern that we call a lookback mechanism, which enables\nthe LM to recall important information when it becomes necessary. The LM binds\neach character-object-state triple together by co-locating reference\ninformation about them, represented as their Ordering IDs (OIs) in low rank\nsubspaces of the state token's residual stream. When asked about a character's\nbeliefs regarding the state of an object, the binding lookback retrieves the\ncorresponding state OI and then an answer lookback retrieves the state token.\nWhen we introduce text specifying that one character is (not) visible to the\nother, we find that the LM first generates a visibility ID encoding the\nrelation between the observing and the observed character OIs. In a visibility\nlookback, this ID is used to retrieve information about the observed character\nand update the observing character's beliefs. Our work provides insights into\nthe LM's belief tracking mechanisms, taking a step toward reverse-engineering\nToM reasoning in LMs.",
        "timestamp": "2025-06-12T17:19:46.992Z",
        "rating": "novote",
        "publishedDate": "2025-05-20T17:59:45Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 60,
        "object_id": "paper:arxiv.2505.14685",
        "created_at": "2025-06-12T17:19:47+00:00",
        "updated_at": "2025-06-12T17:20:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.20879": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.20879",
        "url": "https://arxiv.org/pdf/2504.20879",
        "title": "The Leaderboard Illusion",
        "authors": "Shivalika Singh, Yiyang Nan, Alex Wang, Daniel D'Souza, Sayash Kapoor, Ahmet \u00dcst\u00fcn, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah A. Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker",
        "abstract": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field",
        "timestamp": "2025-06-12T17:19:46.991Z",
        "rating": "novote",
        "publishedDate": "2025-04-29T15:48:49Z",
        "tags": [
          "cs.AI",
          "cs.CL",
          "cs.LG",
          "stat.ME"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 59,
        "object_id": "paper:arxiv.2504.20879",
        "created_at": "2025-06-12T17:19:47+00:00",
        "updated_at": "2025-06-12T17:20:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.13121": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.13121",
        "url": "https://arxiv.org/pdf/2310.13121",
        "title": "Understanding Addition in Transformers",
        "authors": "Philip Quirke, Fazl Barez",
        "abstract": "Understanding the inner workings of machine learning models like Transformers\nis vital for their safe and ethical use. This paper provides a comprehensive\nanalysis of a one-layer Transformer model trained to perform n-digit integer\naddition. Our findings suggest that the model dissects the task into parallel\nstreams dedicated to individual digits, employing varied algorithms tailored to\ndifferent positions within the digits. Furthermore, we identify a rare scenario\ncharacterized by high loss, which we explain. By thoroughly elucidating the\nmodel's algorithm, we provide new insights into its functioning. These findings\nare validated through rigorous testing and mathematical modeling, thereby\ncontributing to the broader fields of model understanding and interpretability.\nOur approach opens the door for analyzing more complex tasks and multi-layer\nTransformer models.",
        "timestamp": "2025-06-12T17:19:43.181Z",
        "rating": "novote",
        "publishedDate": "2023-10-19T19:34:42Z",
        "tags": [
          "cs.LG",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 58,
        "object_id": "paper:arxiv.2310.13121",
        "created_at": "2025-06-12T17:19:43+00:00",
        "updated_at": "2025-06-12T17:20:20+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.00985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.00985",
        "url": "https://arxiv.org/pdf/2505.00985",
        "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
        "authors": "Yash Goel, Ayan Sengupta, Tanmoy Chakraborty",
        "abstract": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development.",
        "timestamp": "2025-06-12T17:19:43.183Z",
        "rating": "novote",
        "publishedDate": "2025-05-02T04:13:27Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 57,
        "object_id": "paper:arxiv.2505.00985",
        "created_at": "2025-06-12T17:19:43+00:00",
        "updated_at": "2025-06-12T17:20:18+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.17148": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.17148",
        "url": "https://arxiv.org/abs/2501.17148",
        "title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders",
        "authors": "Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, Christopher Potts",
        "abstract": "Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering, we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.",
        "timestamp": "2025-06-12T17:19:43.193Z",
        "rating": "novote",
        "publishedDate": "2025/01/28",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 56,
        "object_id": "paper:arxiv.2501.17148",
        "created_at": "2025-06-12T17:19:43+00:00",
        "updated_at": "2025-06-12T17:20:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2312.01552": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2312.01552",
        "url": "https://arxiv.org/pdf/2312.01552",
        "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context\n  Learning",
        "authors": "Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, Yejin Choi",
        "abstract": "The alignment tuning process of large language models (LLMs) typically\ninvolves instruction learning through supervised fine-tuning (SFT) and\npreference tuning via reinforcement learning from human feedback (RLHF). A\nrecent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for\nSFT can achieve significant alignment performance as well, suggesting that the\neffect of alignment tuning might be \"superficial.\" This raises questions about\nhow exactly the alignment tuning transforms a base LLM.\n  We analyze the effect of alignment tuning by examining the token distribution\nshift between base LLMs and their aligned counterpart. Our findings reveal that\nbase LLMs and their alignment-tuned versions perform nearly identically in\ndecoding on the majority of token positions. Most distribution shifts occur\nwith stylistic tokens. These direct evidence strongly supports the Superficial\nAlignment Hypothesis suggested by LIMA.\n  Based on these findings, we rethink the alignment of LLMs by posing the\nresearch question: how effectively can we align base LLMs without SFT or RLHF?\nTo address this, we introduce a simple, tuning-free alignment method, URIAL.\nURIAL achieves effective alignment purely through in-context learning (ICL)\nwith base LLMs, requiring as few as three constant stylistic examples and a\nsystem prompt. We conduct a fine-grained and interpretable evaluation on a\ndiverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that\nbase LLMs with URIAL can match or even surpass the performance of LLMs aligned\nwith SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based\nalignment methods can be significantly reduced through strategic prompting and\nICL. Our findings on the superficial nature of alignment tuning and results\nwith URIAL suggest that deeper analysis and theoretical understanding of\nalignment is crucial to future LLM research.",
        "timestamp": "2025-06-12T17:19:42.594Z",
        "rating": "novote",
        "publishedDate": "2023-12-04T00:46:11Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 55,
        "object_id": "paper:arxiv.2312.01552",
        "created_at": "2025-06-12T17:19:43+00:00",
        "updated_at": "2025-06-12T17:20:23+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.08019": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.08019",
        "url": "https://arxiv.org/pdf/2411.08019",
        "title": "Language Models as Causal Effect Generators",
        "authors": "Lucius E. J. Bynum, Kyunghyun Cho",
        "abstract": "We present a framework for large language model (LLM) based data generation\nwith controllable causal structure. In particular, we define a procedure for\nturning any language model and any directed acyclic graph (DAG) into a\nsequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM\nis a causal model with user-defined structure and LLM-defined structural\nequations. We characterize how an SD-SCM allows sampling from observational,\ninterventional, and counterfactual distributions according to the desired\ncausal structure. We then leverage this procedure to propose a new type of\nbenchmark for causal inference methods, generating individual-level\ncounterfactual data without needing to manually specify functional\nrelationships between variables. We create an example benchmark consisting of\nthousands of datasets, and test a suite of popular estimation methods on these\ndatasets for average, conditional average, and individual treatment effect\nestimation, both with and without hidden confounding. Apart from generating\ndata, the same procedure also allows us to test for the presence of a causal\neffect that might be encoded in an LLM. This procedure can underpin auditing\nLLMs for misinformation, discrimination, or otherwise undesirable behavior. We\nbelieve SD-SCMs can serve as a useful tool in any application that would\nbenefit from sequential data with controllable causal structure.",
        "timestamp": "2025-06-12T17:19:42.507Z",
        "rating": "novote",
        "publishedDate": "2024-11-12T18:50:35Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG",
          "stat.AP",
          "stat.ME",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 54,
        "object_id": "paper:arxiv.2411.08019",
        "created_at": "2025-06-12T17:19:43+00:00",
        "updated_at": "2025-06-12T17:20:18+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.18114": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.18114",
        "url": "https://arxiv.org/pdf/2504.18114",
        "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection",
        "authors": "Atharva Kulkarni, Yuan Zhang, Joel Ruben Antony Moniz, Xiou Ge, Bo-Hsiang Tseng, Dhivya Piraviperumal, Swabha Swayamdipta, Hong Yu",
        "abstract": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them.",
        "timestamp": "2025-06-12T17:19:41.879Z",
        "rating": "novote",
        "publishedDate": "2025-04-25T06:37:29Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 53,
        "object_id": "paper:arxiv.2504.18114",
        "created_at": "2025-06-12T17:19:43+00:00",
        "updated_at": "2025-06-12T17:20:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.03867": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.03867",
        "url": "https://arxiv.org/pdf/2403.03867",
        "title": "On the Origins of Linear Representations in Large Language Models",
        "authors": "Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, Victor Veitch",
        "abstract": "Recent works have argued that high-level semantic concepts are encoded\n\"linearly\" in the representation space of large language models. In this work,\nwe study the origins of such linear representations. To that end, we introduce\na simple latent variable model to abstract and formalize the concept dynamics\nof the next token prediction. We use this formalism to show that the next token\nprediction objective (softmax with cross-entropy) and the implicit bias of\ngradient descent together promote the linear representation of concepts.\nExperiments show that linear representations emerge when learning from data\nmatching the latent variable model, confirming that this simple structure\nalready suffices to yield linear representations. We additionally confirm some\npredictions of the theory using the LLaMA-2 large language model, giving\nevidence that the simplified model yields generalizable insights.",
        "timestamp": "2025-06-12T17:19:41.258Z",
        "rating": "novote",
        "publishedDate": "2024-03-06T17:17:36Z",
        "tags": [
          "cs.CL",
          "cs.LG",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 52,
        "object_id": "paper:arxiv.2403.03867",
        "created_at": "2025-06-12T17:19:41+00:00",
        "updated_at": "2025-06-12T17:20:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.04289": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.04289",
        "url": "https://arxiv.org/pdf/2406.04289",
        "title": "What Languages are Easy to Language-Model? A Perspective from Learning\n  Probabilistic Regular Languages",
        "authors": "Nadav Borenstein, Anej Svete, Robin Chan, Josef Valvoda, Franz Nowak, Isabelle Augenstein, Eleanor Chodroff, Ryan Cotterell",
        "abstract": "What can large language models learn? By definition, language models (LM) are\ndistributions over strings. Therefore, an intuitive way of addressing the above\nquestion is to formalize it as a matter of learnability of classes of\ndistributions over strings. While prior work in this direction focused on\nassessing the theoretical limits, in contrast, we seek to understand the\nempirical learnability. Unlike prior empirical work, we evaluate neural LMs on\ntheir home turf-learning probabilistic languages-rather than as classifiers of\nformal languages. In particular, we investigate the learnability of regular LMs\n(RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs\nas a function of various complexity parameters of the RLM and the hidden state\nsize of the neural LM. We find that the RLM rank, which corresponds to the size\nof linear space spanned by the logits of its conditional distributions, and the\nexpected length of sampled strings are strong and significant predictors of\nlearnability for both RNNs and Transformers. Several other predictors also\nreach significance, but with differing patterns between RNNs and Transformers.",
        "timestamp": "2025-06-12T17:19:41.006Z",
        "rating": "novote",
        "publishedDate": "2024-06-06T17:34:24Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 51,
        "object_id": "paper:arxiv.2406.04289",
        "created_at": "2025-06-12T17:19:41+00:00",
        "updated_at": "2025-06-12T17:20:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2210.10749": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.10749",
        "url": "https://arxiv.org/pdf/2210.10749",
        "title": "Transformers Learn Shortcuts to Automata",
        "authors": "Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang",
        "abstract": "Algorithmic reasoning requires capabilities which are most naturally\nunderstood through recurrent models of computation, like the Turing machine.\nHowever, Transformer models, while lacking recurrence, are able to perform such\nreasoning using far fewer layers than the number of reasoning steps. This\nraises the question: what solutions are learned by these shallow and\nnon-recurrent models? We find that a low-depth Transformer can represent the\ncomputations of any finite-state automaton (thus, any bounded-memory\nalgorithm), by hierarchically reparameterizing its recurrent dynamics. Our\ntheoretical results characterize shortcut solutions, whereby a Transformer with\n$o(T)$ layers can exactly replicate the computation of an automaton on an input\nsequence of length $T$. We find that polynomial-sized $O(\\log T)$-depth\nsolutions always exist; furthermore, $O(1)$-depth simulators are surprisingly\ncommon, and can be understood using tools from Krohn-Rhodes theory and circuit\ncomplexity. Empirically, we perform synthetic experiments by training\nTransformers to simulate a wide variety of automata, and show that shortcut\nsolutions can be learned via standard training. We further investigate the\nbrittleness of these solutions and propose potential mitigations.",
        "timestamp": "2025-06-12T17:19:41.006Z",
        "rating": "novote",
        "publishedDate": "2022-10-19T17:45:48Z",
        "tags": [
          "cs.LG",
          "cs.FL",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 50,
        "object_id": "paper:arxiv.2210.10749",
        "created_at": "2025-06-12T17:19:41+00:00",
        "updated_at": "2025-06-12T17:20:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.03703": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.03703",
        "url": "https://arxiv.org/pdf/2503.03703",
        "title": "SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus\n  Searches",
        "authors": "Hiroyuki Deguchi, Go Kamoda, Yusuke Matsushita, Chihiro Taguchi, Kohei Suenaga, Masaki Waga, Sho Yokoi",
        "abstract": "Researchers and practitioners in natural language processing and\ncomputational linguistics frequently observe and analyze the real language\nusage in large-scale corpora. For that purpose, they often employ off-the-shelf\npattern-matching tools, such as grep, and keyword-in-context concordancers,\nwhich is widely used in corpus linguistics for gathering examples. Nonetheless,\nthese existing techniques rely on surface-level string matching, and thus they\nsuffer from the major limitation of not being able to handle orthographic\nvariations and paraphrasing -- notable and common phenomena in any natural\nlanguage. In addition, existing continuous approaches such as dense vector\nsearch tend to be overly coarse, often retrieving texts that are unrelated but\nshare similar topics. Given these challenges, we propose a novel algorithm that\nachieves \\emph{soft} (or semantic) yet efficient pattern matching by relaxing a\nsurface-level matching with word embeddings. Our algorithm is highly scalable\nwith respect to the size of the corpus text utilizing inverted indexes. We have\nprepared an efficient implementation, and we provide an accessible web tool.\nOur experiments demonstrate that the proposed method (i) can execute searches\non billion-scale corpora in less than a second, which is comparable in speed to\nsurface-level string matching and dense vector search; (ii) can extract harmful\ninstances that semantically match queries from a large set of English and\nJapanese Wikipedia articles; and (iii) can be effectively applied to\ncorpus-linguistic analyses of Latin, a language with highly diverse\ninflections.",
        "timestamp": "2025-06-12T17:19:40.474Z",
        "rating": "novote",
        "publishedDate": "2025-03-05T17:53:11Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 49,
        "object_id": "paper:arxiv.2503.03703",
        "created_at": "2025-06-12T17:19:41+00:00",
        "updated_at": "2025-06-12T17:20:14+00:00",
        "version": 1
      }
    },
    "paper:openreview.HvSytvg3Jh": {
      "data": {
        "sourceId": "openreview",
        "paperId": "HvSytvg3Jh",
        "url": "https://openreview.net/pdf?id=HvSytvg3Jh",
        "title": "HvSytvg3Jh",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-12T17:19:39.787Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 48,
        "object_id": "paper:openreview.HvSytvg3Jh",
        "created_at": "2025-06-12T17:19:40+00:00",
        "updated_at": "2025-06-12T17:20:11+00:00",
        "version": 1
      }
    },
    "paper:openreview.HD6bWcj87Y": {
      "data": {
        "sourceId": "openreview",
        "paperId": "HD6bWcj87Y",
        "url": "https://openreview.net/pdf?id=HD6bWcj87Y",
        "title": "HD6bWcj87Y",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-12T17:19:38.488Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 47,
        "object_id": "paper:openreview.HD6bWcj87Y",
        "created_at": "2025-06-12T17:19:40+00:00",
        "updated_at": "2025-06-12T17:20:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.12578": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.12578",
        "url": "https://arxiv.org/pdf/2408.12578",
        "title": "A Percolation Model of Emergence: Analyzing Transformers Trained on a\n  Formal Language",
        "authors": "Ekdeep Singh Lubana, Kyogo Kawaguchi, Robert P. Dick, Hidenori Tanaka",
        "abstract": "Increase in data, size, or compute can lead to sudden learning of specific\ncapabilities by a neural network -- a phenomenon often called \"emergence''.\nBeyond scientific understanding, establishing the causal factors underlying\nsuch emergent capabilities is crucial to enable risk regulation frameworks for\nAI. In this work, we seek inspiration from study of emergent properties in\nother fields and propose a phenomenological definition for the concept in the\ncontext of neural networks. Our definition implicates the acquisition of\ngeneral structures underlying the data-generating process as a cause of sudden\nperformance growth for specific, narrower tasks. We empirically investigate\nthis definition by proposing an experimental system grounded in a\ncontext-sensitive formal language and find that Transformers trained to perform\ntasks on top of strings from this language indeed exhibit emergent\ncapabilities. Specifically, we show that once the language's underlying grammar\nand context-sensitivity inducing structures are learned by the model,\nperformance on narrower tasks suddenly begins to improve. We then analogize our\nnetwork's learning dynamics with the process of percolation on a bipartite\ngraph, establishing a formal phase transition model that predicts the shift in\nthe point of emergence observed in our experiments when changing the data\nstructure. Overall, our experimental and theoretical frameworks yield a step\ntowards better defining, characterizing, and predicting emergence in neural\nnetworks.",
        "timestamp": "2025-06-12T17:19:34.859Z",
        "rating": "novote",
        "publishedDate": "2024-08-22T17:44:22Z",
        "tags": [
          "cs.LG",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 46,
        "object_id": "paper:arxiv.2408.12578",
        "created_at": "2025-06-12T17:19:35+00:00",
        "updated_at": "2025-06-12T17:20:17+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2311.12786": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.12786",
        "url": "https://arxiv.org/pdf/2311.12786",
        "title": "Mechanistically analyzing the effects of fine-tuning on procedurally\n  defined tasks",
        "authors": "Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rockt\u00e4schel, David Scott Krueger",
        "abstract": "Fine-tuning large pre-trained models has become the de facto strategy for\ndeveloping both task-specific and general-purpose machine learning systems,\nincluding developing models that are safe to deploy. Despite its clear\nimportance, there has been minimal work that explains how fine-tuning alters\nthe underlying capabilities learned by a model during pretraining: does\nfine-tuning yield entirely novel capabilities or does it just modulate existing\nones? We address this question empirically in synthetic, controlled settings\nwhere we can use mechanistic interpretability tools (e.g., network pruning and\nprobing) to understand how the model's underlying capabilities are changing. We\nperform an extensive analysis of the effects of fine-tuning in these settings,\nand show that: (i) fine-tuning rarely alters the underlying model capabilities;\n(ii) a minimal transformation, which we call a 'wrapper', is typically learned\non top of the underlying model capabilities, creating the illusion that they\nhave been modified; and (iii) further fine-tuning on a task where such hidden\ncapabilities are relevant leads to sample-efficient 'revival' of the\ncapability, i.e., the model begins reusing these capability after only a few\ngradient steps. This indicates that practitioners can unintentionally remove a\nmodel's safety wrapper merely by fine-tuning it on a, e.g., superficially\nunrelated, downstream task. We additionally perform analysis on language models\ntrained on the TinyStories dataset to support our claims in a more realistic\nsetup.",
        "timestamp": "2025-06-12T17:19:34.136Z",
        "rating": "novote",
        "publishedDate": "2023-11-21T18:51:04Z",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 45,
        "object_id": "paper:arxiv.2311.12786",
        "created_at": "2025-06-12T17:19:34+00:00",
        "updated_at": "2025-06-12T17:20:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.18866": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.18866",
        "url": "https://arxiv.org/pdf/2503.18866",
        "title": "Reasoning to Learn from Latent Thoughts",
        "authors": "Yangjun Ruan, Neil Band, Chris J. Maddison, Tatsunori Hashimoto",
        "abstract": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% $\\rightarrow$ 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.",
        "timestamp": "2025-06-12T17:19:34.135Z",
        "rating": "novote",
        "publishedDate": "2025-03-24T16:41:23Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 44,
        "object_id": "paper:arxiv.2503.18866",
        "created_at": "2025-06-12T17:19:34+00:00",
        "updated_at": "2025-06-12T17:19:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.10061": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.10061",
        "url": "https://arxiv.org/pdf/2503.10061",
        "title": "Compute Optimal Scaling of Skills: Knowledge vs Reasoning",
        "authors": "Nicholas Roberts, Niladri Chatterji, Sharan Narang, Mike Lewis, Dieuwke Hupkes",
        "abstract": "Scaling laws are a critical component of the LLM development pipeline, most\nfamously as a way to forecast training decisions such as 'compute-optimally'\ntrading-off parameter count and dataset size, alongside a more recent growing\nlist of other crucial decisions. In this work, we ask whether compute-optimal\nscaling behaviour can be skill-dependent. In particular, we examine knowledge\nand reasoning-based skills such as knowledge-based QA and code generation, and\nwe answer this question in the affirmative: scaling laws are skill-dependent.\nNext, to understand whether skill-dependent scaling is an artefact of the\npretraining datamix, we conduct an extensive ablation of different datamixes\nand find that, also when correcting for datamix differences, knowledge and code\nexhibit fundamental differences in scaling behaviour. We conclude with an\nanalysis of how our findings relate to standard compute-optimal scaling using a\nvalidation set, and find that a misspecified validation set can impact\ncompute-optimal parameter count by nearly 50%, depending on its skill\ncomposition.",
        "timestamp": "2025-06-12T17:19:33.076Z",
        "rating": "novote",
        "publishedDate": "2025-03-13T05:21:22Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 43,
        "object_id": "paper:arxiv.2503.10061",
        "created_at": "2025-06-12T17:19:34+00:00",
        "updated_at": "2025-06-12T17:20:20+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.12580": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.12580",
        "url": "https://arxiv.org/pdf/2411.12580",
        "title": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models",
        "authors": "Laura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwarak Talupuru, Acyr Locatelli, Robert Kirk, Tim Rockt\u00e4schel, Edward Grefenstette, Max Bartolo",
        "abstract": "The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning.",
        "timestamp": "2025-06-12T17:19:32.447Z",
        "rating": "novote",
        "publishedDate": "2024-11-19T15:47:12Z",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 42,
        "object_id": "paper:arxiv.2411.12580",
        "created_at": "2025-06-12T17:19:33+00:00",
        "updated_at": "2025-06-12T17:19:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2409.12183": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.12183",
        "url": "https://arxiv.org/pdf/2409.12183",
        "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic\n  reasoning",
        "authors": "Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett",
        "abstract": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications.",
        "timestamp": "2025-06-12T17:19:32.447Z",
        "rating": "novote",
        "publishedDate": "2024-09-18T17:55:00Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 41,
        "object_id": "paper:arxiv.2409.12183",
        "created_at": "2025-06-12T17:19:33+00:00",
        "updated_at": "2025-06-12T17:20:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2006.07710": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2006.07710",
        "url": "https://arxiv.org/pdf/2006.07710",
        "title": "The Pitfalls of Simplicity Bias in Neural Networks",
        "authors": "Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, Praneeth Netrapalli",
        "abstract": "Several works have proposed Simplicity Bias (SB)---the tendency of standard\ntraining procedures such as Stochastic Gradient Descent (SGD) to find simple\nmodels---to justify why neural networks generalize well [Arpit et al. 2017,\nNakkiran et al. 2019, Soudry et al. 2018]. However, the precise notion of\nsimplicity remains vague. Furthermore, previous settings that use SB to\ntheoretically justify why neural networks generalize well do not simultaneously\ncapture the non-robustness of neural networks---a widely observed phenomenon in\npractice [Goodfellow et al. 2014, Jo and Bengio 2017]. We attempt to reconcile\nSB and the superior standard generalization of neural networks with the\nnon-robustness observed in practice by designing datasets that (a) incorporate\na precise notion of simplicity, (b) comprise multiple predictive features with\nvarying levels of simplicity, and (c) capture the non-robustness of neural\nnetworks trained on real data. Through theory and empirics on these datasets,\nwe make four observations: (i) SB of SGD and variants can be extreme: neural\nnetworks can exclusively rely on the simplest feature and remain invariant to\nall predictive complex features. (ii) The extreme aspect of SB could explain\nwhy seemingly benign distribution shifts and small adversarial perturbations\nsignificantly degrade model performance. (iii) Contrary to conventional wisdom,\nSB can also hurt generalization on the same data distribution, as SB persists\neven when the simplest feature has less predictive power than the more complex\nfeatures. (iv) Common approaches to improve generalization and\nrobustness---ensembles and adversarial training---can fail in mitigating SB and\nits pitfalls. Given the role of SB in training neural networks, we hope that\nthe proposed datasets and methods serve as an effective testbed to evaluate\nnovel algorithmic approaches aimed at avoiding the pitfalls of SB.",
        "timestamp": "2025-06-12T17:19:32.437Z",
        "rating": "novote",
        "publishedDate": "2020-06-13T20:15:26Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 40,
        "object_id": "paper:arxiv.2006.07710",
        "created_at": "2025-06-12T17:19:33+00:00",
        "updated_at": "2025-06-12T17:20:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.14481": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14481",
        "url": "https://arxiv.org/pdf/2503.14481",
        "title": "Don't lie to your friends: Learning what you know from collaborative\n  self-play",
        "authors": "Jacob Eisenstein, Reza Aghajani, Adam Fisch, Dheeru Dua, Fantine Huot, Mirella Lapata, Vicky Zayats, Jonathan Berant",
        "abstract": "To be helpful assistants, AI agents must be aware of their own capabilities\nand limitations. This includes knowing when to answer from parametric knowledge\nversus using tools, when to trust tool outputs, and when to abstain or hedge.\nSuch capabilities are hard to teach through supervised fine-tuning because they\nrequire constructing examples that reflect the agent's specific capabilities.\nWe therefore propose a radically new approach to teaching agents what they\nknow: \\emph{collaborative self-play}. We construct multi-agent collaborations\nin which the group is rewarded for collectively arriving at correct answers.\nThe desired meta-knowledge emerges from the incentives built into the structure\nof the interaction. We focus on small societies of agents that have access to\nheterogeneous tools (corpus-specific retrieval), and therefore must collaborate\nto maximize their success while minimizing their effort. Experiments show that\ngroup-level rewards for multi-agent communities can induce policies that\n\\emph{transfer} to improve tool use and selective prediction in settings where\nindividual agents are deployed in isolation.",
        "timestamp": "2025-06-12T17:19:32.437Z",
        "rating": "novote",
        "publishedDate": "2025-03-18T17:53:20Z",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 39,
        "object_id": "paper:arxiv.2503.14481",
        "created_at": "2025-06-12T17:19:33+00:00",
        "updated_at": "2025-06-12T17:20:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.07776": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.07776",
        "url": "https://arxiv.org/pdf/2502.07776",
        "title": "Auditing Prompt Caching in Language Model APIs",
        "authors": "Chenchen Gu, Xiang Lisa Li, Rohith Kuditipudi, Percy Liang, Tatsunori Hashimoto",
        "abstract": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
        "timestamp": "2025-06-12T17:19:31.239Z",
        "rating": "novote",
        "publishedDate": "2025-02-11T18:58:04Z",
        "tags": [
          "cs.CL",
          "cs.CR",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 38,
        "object_id": "paper:arxiv.2502.07776",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:20:03+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.15845": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.15845",
        "url": "https://arxiv.org/pdf/2407.15845",
        "title": "Reconstructing Training Data From Real World Models Trained with\n  Transfer Learning",
        "authors": "Yakir Oz, Gilad Yehudai, Gal Vardi, Itai Antebi, Michal Irani, Niv Haim",
        "abstract": "Current methods for reconstructing training data from trained classifiers are\nrestricted to very small models, limited training set sizes, and low-resolution\nimages. Such restrictions hinder their applicability to real-world scenarios.\nIn this paper, we present a novel approach enabling data reconstruction in\nrealistic settings for models trained on high-resolution images. Our method\nadapts the reconstruction scheme of arXiv:2206.07758 to real-world scenarios --\nspecifically, targeting models trained via transfer learning over image\nembeddings of large pre-trained models like DINO-ViT and CLIP. Our work employs\ndata reconstruction in the embedding space rather than in the image space,\nshowcasing its applicability beyond visual data. Moreover, we introduce a novel\nclustering-based method to identify good reconstructions from thousands of\ncandidates. This significantly improves on previous works that relied on\nknowledge of the training set to identify good reconstructed images. Our\nfindings shed light on a potential privacy risk for data leakage from models\ntrained using transfer learning.",
        "timestamp": "2025-06-12T17:19:31.441Z",
        "rating": "novote",
        "publishedDate": "2024-07-22T17:59:10Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CR",
          "cs.CV"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 37,
        "object_id": "paper:arxiv.2407.15845",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:19:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.20292": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.20292",
        "url": "https://arxiv.org/pdf/2412.20292",
        "title": "An analytic theory of creativity in convolutional diffusion models",
        "authors": "Mason Kamb, Surya Ganguli",
        "abstract": "We obtain an analytic, interpretable and predictive theory of creativity in\nconvolutional diffusion models. Indeed, score-matching diffusion models can\ngenerate highly original images that lie far from their training data. However,\noptimal score-matching theory suggests that these models should only be able to\nproduce memorized training examples. To reconcile this theory-experiment gap,\nwe identify two simple inductive biases, locality and equivariance, that: (1)\ninduce a form of combinatorial creativity by preventing optimal score-matching;\n(2) result in fully analytic, completely mechanistically interpretable, local\nscore (LS) and equivariant local score (ELS) machines that, (3) after\ncalibrating a single time-dependent hyperparameter can quantitatively predict\nthe outputs of trained convolution only diffusion models (like ResNets and\nUNets) with high accuracy (median $r^2$ of $0.95, 0.94, 0.94, 0.96$ for our top\nmodel on CIFAR10, FashionMNIST, MNIST, and CelebA). Our model reveals a locally\nconsistent patch mosaic mechanism of creativity, in which diffusion models\ncreate exponentially many novel images by mixing and matching different local\ntraining set patches at different scales and image locations. Our theory also\npartially predicts the outputs of pre-trained self-attention enabled UNets\n(median $r^2 \\sim 0.77$ on CIFAR10), revealing an intriguing role for attention\nin carving out semantic coherence from local patch mosaics.",
        "timestamp": "2025-06-12T17:19:31.247Z",
        "rating": "novote",
        "publishedDate": "2024-12-28T22:33:29Z",
        "tags": [
          "cs.LG",
          "cond-mat.dis-nn",
          "cs.AI",
          "q-bio.NC",
          "stat.ML",
          "I.2.10"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 36,
        "object_id": "paper:arxiv.2412.20292",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:19:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.21278": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.21278",
        "url": "https://arxiv.org/pdf/2502.21278",
        "title": "Does Generation Require Memorization? Creative Diffusion Models using\n  Ambient Diffusion",
        "authors": "Kulin Shah, Alkis Kalavasis, Adam R. Klivans, Giannis Daras",
        "abstract": "There is strong empirical evidence that the state-of-the-art diffusion\nmodeling paradigm leads to models that memorize the training set, especially\nwhen the training set is small. Prior methods to mitigate the memorization\nproblem often lead to a decrease in image quality. Is it possible to obtain\nstrong and creative generative models, i.e., models that achieve high\ngeneration quality and low memorization? Despite the current pessimistic\nlandscape of results, we make significant progress in pushing the trade-off\nbetween fidelity and memorization. We first provide theoretical evidence that\nmemorization in diffusion models is only necessary for denoising problems at\nlow noise scales (usually used in generating high-frequency details). Using\nthis theoretical insight, we propose a simple, principled method to train the\ndiffusion models using noisy data at large noise scales. We show that our\nmethod significantly reduces memorization without decreasing the image quality,\nfor both text-conditional and unconditional models and for a variety of data\navailability settings.",
        "timestamp": "2025-06-12T17:19:31.324Z",
        "rating": "novote",
        "publishedDate": "2025-02-28T17:57:48Z",
        "tags": [
          "cs.LG",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 35,
        "object_id": "paper:arxiv.2502.21278",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:20:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2307.03056": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2307.03056",
        "url": "https://arxiv.org/pdf/2307.03056",
        "title": "Generalizing Backpropagation for Gradient-Based Interpretability",
        "authors": "Kevin Du, Lucas Torroba Hennigen, Niklas Stoehr, Alexander Warstadt, Ryan Cotterell",
        "abstract": "Many popular feature-attribution methods for interpreting deep neural\nnetworks rely on computing the gradients of a model's output with respect to\nits inputs. While these methods can indicate which input features may be\nimportant for the model's prediction, they reveal little about the inner\nworkings of the model itself. In this paper, we observe that the gradient\ncomputation of a model is a special case of a more general formulation using\nsemirings. This observation allows us to generalize the backpropagation\nalgorithm to efficiently compute other interpretable statistics about the\ngradient graph of a neural network, such as the highest-weighted path and\nentropy. We implement this generalized algorithm, evaluate it on synthetic\ndatasets to better understand the statistics it computes, and apply it to study\nBERT's behavior on the subject-verb number agreement task (SVA). With this\nmethod, we (a) validate that the amount of gradient flow through a component of\na model reflects its importance to a prediction and (b) for SVA, identify which\npathways of the self-attention mechanism are most important.",
        "timestamp": "2025-06-12T17:19:30.465Z",
        "rating": "novote",
        "publishedDate": "2023-07-06T15:19:53Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 34,
        "object_id": "paper:arxiv.2307.03056",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:19:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.20760": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.20760",
        "url": "https://arxiv.org/pdf/2412.20760",
        "title": "Attributing Culture-Conditioned Generations to Pretraining Corpora",
        "authors": "Huihan Li, Arnav Goel, Keyu He, Xiang Ren",
        "abstract": "In open-ended generative tasks like narrative writing or dialogue, large\nlanguage models often exhibit cultural biases, showing limited knowledge and\ngenerating templated outputs for less prevalent cultures. Recent works show\nthat these biases may stem from uneven cultural representation in pretraining\ncorpora. This work investigates how pretraining leads to biased\nculture-conditioned generations by analyzing how models associate entities with\ncultures based on pretraining data patterns. We propose the MEMOed framework\n(MEMOrization from pretraining document) to determine whether a generation for\na culture arises from memorization. Using MEMOed on culture-conditioned\ngenerations about food and clothing for 110 cultures, we find that\nhigh-frequency cultures in pretraining data yield more generations with\nmemorized symbols, while some low-frequency cultures produce none.\nAdditionally, the model favors generating entities with extraordinarily high\nfrequency regardless of the conditioned culture, reflecting biases toward\nfrequent pretraining terms irrespective of relevance. We hope that the MEMOed\nframework and our insights will inspire more works on attributing model\nperformance on pretraining data.",
        "timestamp": "2025-06-12T17:19:30.409Z",
        "rating": "novote",
        "publishedDate": "2024-12-30T07:09:25Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 33,
        "object_id": "paper:arxiv.2412.20760",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:20:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2010.12016": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2010.12016",
        "url": "https://arxiv.org/pdf/2010.12016",
        "title": "Towards falsifiable interpretability research",
        "authors": "Matthew L. Leavitt, Ari Morcos",
        "abstract": "Methods for understanding the decisions of and mechanisms underlying deep\nneural networks (DNNs) typically rely on building intuition by emphasizing\nsensory or semantic features of individual examples. For instance, methods aim\nto visualize the components of an input which are \"important\" to a network's\ndecision, or to measure the semantic properties of single neurons. Here, we\nargue that interpretability research suffers from an over-reliance on\nintuition-based approaches that risk-and in some cases have caused-illusory\nprogress and misleading conclusions. We identify a set of limitations that we\nargue impede meaningful progress in interpretability research, and examine two\npopular classes of interpretability methods-saliency and single-neuron-based\napproaches-that serve as case studies for how overreliance on intuition and\nlack of falsifiability can undermine interpretability research. To address\nthese concerns, we propose a strategy to address these impediments in the form\nof a framework for strongly falsifiable interpretability research. We encourage\nresearchers to use their intuitions as a starting point to develop and test\nclear, falsifiable hypotheses, and hope that our framework yields robust,\nevidence-based interpretability methods that generate meaningful advances in\nour understanding of DNNs.",
        "timestamp": "2025-06-12T17:19:31.042Z",
        "rating": "novote",
        "publishedDate": "2020-10-22T22:03:41Z",
        "tags": [
          "cs.CY",
          "cs.AI",
          "cs.CV",
          "cs.LG",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 32,
        "object_id": "paper:arxiv.2010.12016",
        "created_at": "2025-06-12T17:19:32+00:00",
        "updated_at": "2025-06-12T17:19:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.01558": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.01558",
        "url": "https://arxiv.org/pdf/2501.01558",
        "title": "Predicting the Performance of Black-box LLMs through Self-Queries",
        "authors": "Dylan Sam, Marc Finzi, J. Zico Kolter",
        "abstract": "As large language models (LLMs) are increasingly relied on in AI systems,\npredicting when they make mistakes is crucial. While a great deal of work in\nthe field uses internal representations to interpret model behavior, these\nrepresentations are inaccessible when given solely black-box access through an\nAPI. In this paper, we extract features of LLMs in a black-box manner by using\nfollow-up prompts and taking the probabilities of different responses as\nrepresentations to train reliable predictors of model behavior. We demonstrate\nthat training a linear model on these low-dimensional representations produces\nreliable and generalizable predictors of model performance at the instance\nlevel (e.g., if a particular generation correctly answers a question).\nRemarkably, these can often outperform white-box linear predictors that operate\nover a model's hidden state or the full distribution over its vocabulary. In\naddition, we demonstrate that these extracted features can be used to evaluate\nmore nuanced aspects of a language model's state. For instance, they can be\nused to distinguish between a clean version of GPT-4o-mini and a version that\nhas been influenced via an adversarial system prompt that answers\nquestion-answering tasks incorrectly or introduces bugs into generated code.\nFurthermore, they can reliably distinguish between different model\narchitectures and sizes, enabling the detection of misrepresented models\nprovided through an API (e.g., identifying if GPT-3.5 is supplied instead of\nGPT-4o-mini).",
        "timestamp": "2025-06-12T17:19:30.047Z",
        "rating": "novote",
        "publishedDate": "2025-01-02T22:26:54Z",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 31,
        "object_id": "paper:arxiv.2501.01558",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:19:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.13852": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.13852",
        "url": "https://arxiv.org/pdf/2410.13852",
        "title": "Retrospective Learning from Interactions",
        "authors": "Zizhao Chen, Mustafa Omer Gul, Yiwei Chen, Gloria Geng, Anne Wu, Yoav Artzi",
        "abstract": "Multi-turn interactions between large language models (LLMs) and users\nnaturally include implicit feedback signals. If an LLM responds in an\nunexpected way to an instruction, the user is likely to signal it by rephrasing\nthe request, expressing frustration, or pivoting to an alternative task. Such\nsignals are task-independent and occupy a relatively constrained subspace of\nlanguage, allowing the LLM to identify them even if it fails on the actual\ntask. We introduce ReSpect, a method to learn from such signals in past\ninteractions via retrospection without additional annotations. We deploy\nReSpect in a new multimodal interaction scenario, where humans instruct a\nmultimodal LLM to solve an abstract reasoning task with a combinatorial\nsolution space. Through thousands of interactions with humans, we show how\nReSpect gradually improves task completion rate from 31% to 82%, all without\nany external annotation.",
        "timestamp": "2025-06-12T17:19:30.033Z",
        "rating": "novote",
        "publishedDate": "2024-10-17T17:59:03Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CV",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 30,
        "object_id": "paper:arxiv.2410.13852",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:19:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2304.15004": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2304.15004",
        "url": "https://arxiv.org/pdf/2304.15004",
        "title": "Are Emergent Abilities of Large Language Models a Mirage?",
        "authors": "Rylan Schaeffer, Brando Miranda, Sanmi Koyejo",
        "abstract": "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.",
        "timestamp": "2025-06-12T17:19:30.034Z",
        "rating": "novote",
        "publishedDate": "2023-04-28T17:52:11Z",
        "tags": [
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 29,
        "object_id": "paper:arxiv.2304.15004",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:19:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.14491": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.14491",
        "url": "https://arxiv.org/pdf/2310.14491",
        "title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning\n  Capabilities of Language Models",
        "authors": "Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, Mrinmaya Sachan",
        "abstract": "Recent work has shown that language models (LMs) have strong multi-step\n(i.e., procedural) reasoning capabilities. However, it is unclear whether LMs\nperform these tasks by cheating with answers memorized from pretraining corpus,\nor, via a multi-step reasoning mechanism. In this paper, we try to answer this\nquestion by exploring a mechanistic interpretation of LMs for multi-step\nreasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a\nreasoning tree resembling the correct reasoning process within it. We test this\nhypothesis by introducing a new probing approach (called MechanisticProbe) that\nrecovers the reasoning tree from the model's attention patterns. We use our\nprobe to analyze two LMs: GPT-2 on a synthetic task (k-th smallest element),\nand LLaMA on two simple language-based reasoning tasks (ProofWriter & AI2\nReasoning Challenge). We show that MechanisticProbe is able to detect the\ninformation of the reasoning tree from the model's attentions for most\nexamples, suggesting that the LM indeed is going through a process of\nmulti-step reasoning within its architecture in many cases.",
        "timestamp": "2025-06-12T17:19:30.047Z",
        "rating": "novote",
        "publishedDate": "2023-10-23T01:47:29Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 28,
        "object_id": "paper:arxiv.2310.14491",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:19:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.05017": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.05017",
        "url": "https://arxiv.org/pdf/2505.05017",
        "title": "Scalable Multi-Stage Influence Function for Large Language Models via\n  Eigenvalue-Corrected Kronecker-Factored Parameterization",
        "authors": "Yuntai Bao, Xuhong Zhang, Tianyu Du, Xinkui Zhao, Jiang Zong, Hao Peng, Jianwei Yin",
        "abstract": "Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to\ndownstream tasks. Since the majority of knowledge is acquired during\npre-training, attributing the predictions of fine-tuned LLMs to their\npre-training data may provide valuable insights. Influence functions have been\nproposed as a means to explain model predictions based on training data.\nHowever, existing approaches fail to compute ``multi-stage'' influence and lack\nscalability to billion-scale LLMs.\n  In this paper, we propose the multi-stage influence function to attribute the\ndownstream predictions of fine-tuned LLMs to pre-training data under the\nfull-parameter fine-tuning paradigm. To enhance the efficiency and practicality\nof our multi-stage influence function, we leverage Eigenvalue-corrected\nKronecker-Factored (EK-FAC) parameterization for efficient approximation.\nEmpirical results validate the superior scalability of EK-FAC approximation and\nthe effectiveness of our multi-stage influence function. Additionally, case\nstudies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,\nwith exemplars illustrating insights provided by multi-stage influence\nestimates. Our code is public at\nhttps://github.com/colored-dye/multi_stage_influence_function.",
        "timestamp": "2025-06-12T17:19:30.047Z",
        "rating": "novote",
        "publishedDate": "2025-05-08T07:43:44Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 27,
        "object_id": "paper:arxiv.2505.05017",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:19:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.02550": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.02550",
        "url": "https://arxiv.org/pdf/2406.02550",
        "title": "Learning to grok: Emergence of in-context learning and skill composition\n  in modular arithmetic tasks",
        "authors": "Tianyu He, Darshil Doshi, Aritra Das, Andrey Gromov",
        "abstract": "Large language models can solve tasks that were not present in the training\nset. This capability is believed to be due to in-context learning and skill\ncomposition. In this work, we study the emergence of in-context learning and\nskill composition in a collection of modular arithmetic tasks. Specifically, we\nconsider a finite collection of linear modular functions $z = a \\, x + b \\, y\n\\;\\mathrm{mod}\\; p$ labeled by the vector $(a, b) \\in \\mathbb{Z}_p^2$. We use\nsome of these tasks for pre-training and the rest for out-of-distribution\ntesting. We empirically show that a GPT-style transformer exhibits a transition\nfrom in-distribution to out-of-distribution generalization as the number of\npre-training tasks increases. We find that the smallest model capable of\nout-of-distribution generalization requires two transformer blocks, while for\ndeeper models, the out-of-distribution generalization phase is\n\\emph{transient}, necessitating early stopping. Finally, we perform an\ninterpretability study of the pre-trained models, revealing highly structured\nrepresentations in both attention heads and MLPs; and discuss the learned\nalgorithms. Notably, we find an algorithmic shift in deeper models, as we go\nfrom few to many in-context examples.",
        "timestamp": "2025-06-12T17:19:30.047Z",
        "rating": "novote",
        "publishedDate": "2024-06-04T17:59:36Z",
        "tags": [
          "cs.LG",
          "cond-mat.dis-nn",
          "hep-th",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 26,
        "object_id": "paper:arxiv.2406.02550",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:19:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.05790": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.05790",
        "url": "https://arxiv.org/pdf/2501.05790",
        "title": "Understanding Impact of Human Feedback via Influence Functions",
        "authors": "Taywon Min, Haeone Lee, Yongchan Kwon, Kimin Lee",
        "abstract": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn\nsuitable reward models from human feedback to align large language models\n(LLMs) with human intentions. However, human feedback can often be noisy,\ninconsistent, or biased, especially when evaluating complex responses. Such\nfeedback can lead to misaligned reward signals, potentially causing unintended\nside effects during the RLHF process. To address these challenges, we explore\nthe use of influence functions to measure the impact of human feedback on the\nperformance of reward models. We propose a compute-efficient approximation\nmethod that enables the application of influence functions to LLM-based reward\nmodels and large-scale preference datasets. In our experiments, we demonstrate\ntwo key applications of influence functions: (1) detecting common forms of\nlabeler bias in human feedback datasets and (2) guiding labelers to refine\ntheir strategies to align more closely with expert feedback. By quantifying the\nimpact of human feedback on reward models, we believe that influence functions\ncan enhance feedback interpretability and contribute to scalable oversight in\nRLHF, helping labelers provide more accurate and consistent feedback. Source\ncode is available at https://github.com/mintaywon/IF_RLHF",
        "timestamp": "2025-06-12T17:19:30.047Z",
        "rating": "novote",
        "publishedDate": "2025-01-10T08:50:38Z",
        "tags": [
          "cs.AI",
          "cs.HC",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 25,
        "object_id": "paper:arxiv.2501.05790",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:20:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.13981": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.13981",
        "url": "https://arxiv.org/pdf/2411.13981",
        "title": "On the Fairness, Diversity and Reliability of Text-to-Image Generative\n  Models",
        "authors": "Jordan Vice, Naveed Akhtar, Leonid Sigal, Richard Hartley, Ajmal Mian",
        "abstract": "The rapid proliferation of multimodal generative models has sparked critical\ndiscussions on their reliability, fairness and potential for misuse. While\ntext-to-image models excel at producing high-fidelity, user-guided content,\nthey often exhibit unpredictable behaviors and vulnerabilities that can be\nexploited to manipulate class or concept representations. To address this, we\npropose an evaluation framework to assess model reliability by analyzing\nresponses to global and local perturbations in the embedding space, enabling\nthe identification of inputs that trigger unreliable or biased behavior. Beyond\nsocial implications, fairness and diversity are fundamental to defining robust\nand trustworthy model behavior. Our approach offers deeper insights into these\nessential aspects by evaluating: (i) generative diversity, measuring the\nbreadth of visual representations for learned concepts, and (ii) generative\nfairness, which examines the impact that removing concepts from input prompts\nhas on control, under a low guidance setup. Beyond these evaluations, our\nmethod lays the groundwork for detecting unreliable, bias-injected models and\ntracing the provenance of embedded biases. Our code is publicly available at\nhttps://github.com/JJ-Vice/T2I_Fairness_Diversity_Reliability.\n  Keywords: Fairness, Reliability, AI Ethics, Bias, Text-to-Image Models",
        "timestamp": "2025-06-12T17:19:30.033Z",
        "rating": "novote",
        "publishedDate": "2024-11-21T09:46:55Z",
        "tags": [
          "cs.CV",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 24,
        "object_id": "paper:arxiv.2411.13981",
        "created_at": "2025-06-12T17:19:31+00:00",
        "updated_at": "2025-06-12T17:20:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.21333": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.21333",
        "url": "https://www.google.com/url?q=https://arxiv.org/pdf/2410.21333v1&sa=D&source=docs&ust=1749753109568724&usg=AOvVaw0ncfFvr0rH_C-qLvKOiHfd",
        "title": "Redirecting",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-12T17:31:51.725Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 64,
        "object_id": "paper:arxiv.2410.21333",
        "created_at": "2025-06-12T17:31:52+00:00",
        "updated_at": "2025-06-12T17:32:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.01542": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.01542",
        "url": "https://arxiv.org/pdf/2504.01542",
        "title": "Register Always Matters: Analysis of LLM Pretraining Data Through the\n  Lens of Language Variation",
        "authors": "Amanda Myntti, Erik Henriksson, Veronika Laippala, Sampo Pyysalo",
        "abstract": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labeling systems, datasets\nare divided into categories, frequently reducing to a binary: those passing the\nfilters deemed as valuable examples, others discarded as useless or\ndetrimental. However, a more detailed understanding of the contribution of\ndifferent kinds of texts to model performance is still largely lacking. In this\narticle, we present the first study utilizing registers (also known as genres)\n- a widely used standard in corpus linguistics to model linguistic variation -\nto curate pretraining datasets and investigate the effect of register on the\nperformance of LLMs. We perform comparative studies by training models with\nregister classified data and evaluating them using standard benchmarks, and\nshow that the register of pretraining data substantially affects model\nperformance. We uncover surprising relationships between the pretraining\nmaterial and the resulting models: using the News register results in subpar\nperformance, and on the contrary, including the Opinion class, covering texts\nsuch as reviews and opinion blogs, is highly beneficial. While a model trained\non the entire unfiltered dataset outperforms those trained on datasets limited\nto a single register, combining well-performing registers like\nHow-to-Instructions, Informational Description, and Opinion leads to major\nimprovements. Furthermore, analysis of individual benchmark results reveals key\ndifferences in the strengths and drawbacks of specific register classes as\npretraining data. These findings show that register is an important explainer\nof model variation and can facilitate more deliberate future data selection\npractices.",
        "timestamp": "2025-06-13T00:13:02.909Z",
        "rating": "novote",
        "publishedDate": "2025-04-02T09:30:24Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 69,
        "object_id": "paper:arxiv.2504.01542",
        "created_at": "2025-06-13T00:13:03+00:00",
        "updated_at": "2025-06-13T00:13:23+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.13259": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.13259",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-13T00:12:48.287Z",
            "data": {
              "session_id": "session_1749773567705_jkfryvy",
              "source_id": "arxiv",
              "paper_id": "2502.13259",
              "start_time": "2025-06-13T00:12:39.595Z",
              "end_time": "2025-06-13T00:12:47.705Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:30:57.593Z",
            "data": {
              "session_id": "session_1750977057455_52putu9",
              "source_id": "arxiv",
              "paper_id": "2502.13259",
              "start_time": "2025-06-26T22:30:34.934Z",
              "end_time": "2025-06-26T22:30:57.455Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "issue_number": 68,
        "object_id": "interactions:arxiv.2502.13259",
        "created_at": "2025-06-13T00:12:48+00:00",
        "updated_at": "2025-06-26T22:31:15+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.13259": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.13259",
        "url": "https://arxiv.org/pdf/2502.13259",
        "title": "HumT DumT: Measuring and controlling human-like language in LLMs",
        "authors": "Myra Cheng, Sunny Yu, Dan Jurafsky",
        "abstract": "Should LLMs generate language that makes them seem human? Human-like language\nmight improve user experience, but might also lead to deception, overreliance,\nand stereotyping. Assessing these potential impacts requires a systematic way\nto measure human-like tone in LLM outputs. We introduce HumT and SocioT,\nmetrics for human-like tone and other dimensions of social perceptions in text\ndata based on relative probabilities from an LLM. By measuring HumT across\npreference and usage datasets, we find that users prefer less human-like\noutputs from LLMs in many contexts. HumT also offers insights into the\nperceptions and impacts of anthropomorphism: human-like LLM outputs are highly\ncorrelated with warmth, social closeness, femininity, and low status, which are\nclosely linked to the aforementioned harms. We introduce DumT, a method using\nHumT to systematically control and reduce the degree of human-like tone while\npreserving model performance. DumT offers a practical approach for mitigating\nrisks associated with anthropomorphic language generation.",
        "timestamp": "2025-06-13T00:12:40.100Z",
        "rating": "novote",
        "publishedDate": "2025-02-18T20:04:09Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 67,
        "object_id": "paper:arxiv.2502.13259",
        "created_at": "2025-06-13T00:12:40+00:00",
        "updated_at": "2025-06-13T00:12:58+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.21333": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.21333",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-16T17:40:27.058Z",
            "data": {
              "session_id": "session_1750095626319_8xlktr3",
              "source_id": "arxiv",
              "paper_id": "2410.21333",
              "start_time": "2025-06-16T17:39:20.072Z",
              "end_time": "2025-06-16T17:40:26.319Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 1,
              "total_elapsed_seconds": 66
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:32:46.230Z",
            "data": {
              "session_id": "session_1750977165959_w0dvg1h",
              "source_id": "arxiv",
              "paper_id": "2410.21333",
              "start_time": "2025-06-26T22:32:24.191Z",
              "end_time": "2025-06-26T22:32:45.959Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "issue_number": 66,
        "object_id": "interactions:arxiv.2410.21333",
        "created_at": "2025-06-12T17:34:01+00:00",
        "updated_at": "2025-06-26T22:33:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.01542": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.01542",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-13T00:19:53.302Z",
            "data": {
              "session_id": "session_1749773992868_voch0zz",
              "source_id": "arxiv",
              "paper_id": "2504.01542",
              "start_time": "2025-06-13T00:19:30.292Z",
              "end_time": "2025-06-13T00:19:52.868Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:30:17.171Z",
            "data": {
              "session_id": "session_1750977016646_x9w526w",
              "source_id": "arxiv",
              "paper_id": "2504.01542",
              "start_time": "2025-06-26T22:28:53.130Z",
              "end_time": "2025-06-26T22:30:16.646Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 4,
              "total_elapsed_seconds": 84
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:32:23.950Z",
            "data": {
              "session_id": "session_1750977143936_ftcej74",
              "source_id": "arxiv",
              "paper_id": "2504.01542",
              "start_time": "2025-06-26T22:32:18.661Z",
              "end_time": "2025-06-26T22:32:23.936Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 71,
        "object_id": "interactions:arxiv.2504.01542",
        "created_at": "2025-06-13T00:19:53+00:00",
        "updated_at": "2025-06-26T22:32:41+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2402.04614": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.04614",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-16T17:38:41.293Z",
            "data": {
              "session_id": "session_1750095520635_kt1mgsj",
              "source_id": "arxiv",
              "paper_id": "2402.04614",
              "start_time": "2025-06-16T17:38:25.864Z",
              "end_time": "2025-06-16T17:38:40.635Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 75,
        "object_id": "interactions:arxiv.2402.04614",
        "created_at": "2025-06-16T17:38:41+00:00",
        "updated_at": "2025-06-16T17:39:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.04614": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.04614",
        "url": "https://arxiv.org/pdf/2402.04614",
        "title": "Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations\n  from Large Language Models",
        "authors": "Chirag Agarwal, Sree Harsha Tanneru, Himabindu Lakkaraju",
        "abstract": "Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.",
        "timestamp": "2025-06-16T17:38:25.865Z",
        "rating": "novote",
        "publishedDate": "2024-02-07T06:32:50Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 73,
        "object_id": "paper:arxiv.2402.04614",
        "created_at": "2025-06-16T17:38:26+00:00",
        "updated_at": "2025-06-16T17:38:50+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.18128": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.18128",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-24T21:10:15.574Z",
            "data": {
              "session_id": "session_1750799415330_zk9u2q9",
              "source_id": "arxiv",
              "paper_id": "2505.18128",
              "start_time": "2025-06-24T21:10:07.409Z",
              "end_time": "2025-06-24T21:10:15.330Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:44:58.396Z",
            "data": {
              "session_id": "session_1751568298170_vvnc60o",
              "source_id": "arxiv",
              "paper_id": "2505.18128",
              "start_time": "2025-07-03T18:44:36.451Z",
              "end_time": "2025-07-03T18:44:58.170Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 2,
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "issue_number": 72,
        "object_id": "interactions:arxiv.2505.18128",
        "created_at": "2025-06-13T21:59:55+00:00",
        "updated_at": "2025-07-03T18:45:20+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.03826": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.03826",
        "url": "https://arxiv.org/pdf/2502.03826",
        "title": "FairT2I: Mitigating Social Bias in Text-to-Image Generation via Large\n  Language Model-Assisted Detection and Attribute Rebalancing",
        "authors": "Jinya Sakurai, Issei Sato",
        "abstract": "The proliferation of Text-to-Image (T2I) models has revolutionized content\ncreation, providing powerful tools for diverse applications ranging from\nartistic expression to educational material development and marketing. Despite\nthese technological advancements, significant ethical concerns arise from these\nmodels' reliance on large-scale datasets that often contain inherent societal\nbiases. These biases are further amplified when AI-generated content is\nincluded in training data, potentially reinforcing and perpetuating stereotypes\nin the generated outputs. In this paper, we introduce FairT2I, a novel\nframework that harnesses large language models to detect and mitigate social\nbiases in T2I generation. Our framework comprises two key components: (1) an\nLLM-based bias detection module that identifies potential social biases in\ngenerated images based on text prompts, and (2) an attribute rebalancing module\nthat fine-tunes sensitive attributes within the T2I model to mitigate\nidentified biases. Our extensive experiments across various T2I models and\ndatasets show that FairT2I can significantly reduce bias while maintaining\nhigh-quality image generation. We conducted both qualitative user studies and\nquantitative non-parametric analyses in the generated image feature space,\nbuilding upon the occupational dataset introduced in the Stable Bias study. Our\nresults show that FairT2I successfully mitigates social biases and enhances the\ndiversity of sensitive attributes in generated images. We further demonstrate,\nusing the P2 dataset, that our framework can detect subtle biases that are\nchallenging for human observers to perceive, extending beyond\noccupation-related prompts. On the basis of these findings, we introduce a new\nbenchmark dataset for evaluating bias in T2I models.",
        "timestamp": "2025-06-16T17:43:13.817Z",
        "rating": "novote",
        "publishedDate": "2025-02-06T07:22:57Z",
        "tags": [
          "cs.CV"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 76,
        "object_id": "paper:arxiv.2502.03826",
        "created_at": "2025-06-16T17:43:14+00:00",
        "updated_at": "2025-06-16T17:43:33+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.03826": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.03826",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-16T17:46:16.757Z",
            "data": {
              "session_id": "session_1750095975634_f5nvvjn",
              "source_id": "arxiv",
              "paper_id": "2502.03826",
              "start_time": "2025-06-16T17:43:13.815Z",
              "end_time": "2025-06-16T17:46:15.634Z",
              "heartbeat_count": 36,
              "duration_seconds": 180,
              "idle_seconds": 2,
              "total_elapsed_seconds": 182
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-16T17:47:32.855Z",
            "data": {
              "session_id": "session_1750096052138_m8bg6t0",
              "source_id": "arxiv",
              "paper_id": "2502.03826",
              "start_time": "2025-06-16T17:46:54.617Z",
              "end_time": "2025-06-16T17:47:32.138Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 3,
              "total_elapsed_seconds": 38
            }
          }
        ]
      },
      "meta": {
        "issue_number": 78,
        "object_id": "interactions:arxiv.2502.03826",
        "created_at": "2025-06-16T17:46:17+00:00",
        "updated_at": "2025-06-16T17:47:50+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.11618": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.11618",
        "interactions": []
      },
      "meta": {
        "issue_number": 83,
        "object_id": "interactions:arxiv.2506.11618",
        "created_at": "2025-06-16T18:14:20+00:00",
        "updated_at": "2025-06-16T18:14:22+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.11673": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.11673",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-16T18:15:33.719Z",
            "data": {
              "session_id": "session_1750097733709_4mo4se7",
              "source_id": "arxiv",
              "paper_id": "2506.11673",
              "start_time": "2025-06-16T18:15:09.732Z",
              "end_time": "2025-06-16T18:15:33.709Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "issue_number": 82,
        "object_id": "interactions:arxiv.2506.11673",
        "created_at": "2025-06-16T18:14:01+00:00",
        "updated_at": "2025-06-16T18:15:53+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.11673": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.11673",
        "url": "https://arxiv.org/pdf/2506.11673",
        "title": "Improving Causal Interventions in Amnesic Probing with Mean Projection\n  or LEACE",
        "authors": "Alicja Dobrzeniecka, Antske Fokkens, Pia Sommerauer",
        "abstract": "Amnesic probing is a technique used to examine the influence of specific\nlinguistic information on the behaviour of a model. This involves identifying\nand removing the relevant information and then assessing whether the model's\nperformance on the main task changes. If the removed information is relevant,\nthe model's performance should decline. The difficulty with this approach lies\nin removing only the target information while leaving other information\nunchanged. It has been shown that Iterative Nullspace Projection (INLP), a\nwidely used removal technique, introduces random modifications to\nrepresentations when eliminating target information. We demonstrate that Mean\nProjection (MP) and LEACE, two proposed alternatives, remove information in a\nmore targeted manner, thereby enhancing the potential for obtaining behavioural\nexplanations through Amnesic Probing.",
        "timestamp": "2025-06-16T18:13:52.394Z",
        "rating": "novote",
        "publishedDate": "2025-06-13T11:07:14Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 81,
        "object_id": "paper:arxiv.2506.11673",
        "created_at": "2025-06-16T18:13:52+00:00",
        "updated_at": "2025-06-16T18:14:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.11618": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.11618",
        "url": "https://arxiv.org/pdf/2506.11618",
        "title": "Convergent Linear Representations of Emergent Misalignment",
        "authors": "Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda",
        "abstract": "Fine-tuning large language models on narrow datasets can cause them to\ndevelop broadly misaligned behaviours: a phenomena known as emergent\nmisalignment. However, the mechanisms underlying this misalignment, and why it\ngeneralizes beyond the training domain, are poorly understood, demonstrating\ncritical gaps in our knowledge of model alignment. In this work, we train and\nstudy a minimal model organism which uses just 9 rank-1 adapters to emergently\nmisalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently\nmisaligned models converge to similar representations of misalignment. We\ndemonstrate this convergence by extracting a 'misalignment direction' from one\nfine-tuned model's activations, and using it to effectively ablate misaligned\nbehaviour from fine-tunes using higher dimensional LoRAs and different\ndatasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further\npresent a set of experiments for directly interpreting the fine-tuning\nadapters, showing that six contribute to general misalignment, while two\nspecialise for misalignment in just the fine-tuning domain. Emergent\nmisalignment is a particularly salient example of undesirable and unexpected\nmodel behaviour and by advancing our understanding of the mechanisms behind it,\nwe hope to move towards being able to better understand and mitigate\nmisalignment more generally.",
        "timestamp": "2025-06-16T18:13:49.017Z",
        "rating": "novote",
        "publishedDate": "2025-06-13T09:39:54Z",
        "tags": [
          "cs.LG",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 79,
        "object_id": "paper:arxiv.2506.11618",
        "created_at": "2025-06-16T18:13:49+00:00",
        "updated_at": "2025-06-16T18:14:08+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.12152": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.12152",
        "interactions": []
      },
      "meta": {
        "issue_number": 85,
        "object_id": "interactions:arxiv.2506.12152",
        "created_at": "2025-06-17T17:16:39+00:00",
        "updated_at": "2025-06-17T17:16:41+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.12152": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.12152",
        "url": "https://arxiv.org/abs/2506.12152",
        "title": "Because we have LLMs, we Can and Should Pursue Agentic Interpretability",
        "authors": "Been Kim, John Hewitt, Neel Nanda, Noah Fiedel, Oyvind Tafjord",
        "abstract": "The era of Large Language Models (LLMs) presents a new opportunity for interpretability--agentic interpretability: a multi-turn conversation with an LLM wherein the LLM proactively assists human understanding by developing and leveraging a mental model of the user, which in turn enables humans to develop better mental models of the LLM. Such conversation is a new capability that traditional `inspective' interpretability methods (opening the black-box) do not use. Having a language model that aims to teach and explain--beyond just knowing how to talk--is similar to a teacher whose goal is to teach well, understanding that their success will be measured by the student's comprehension. While agentic interpretability may trade off completeness for interactivity, making it less suitable for high-stakes safety situations with potentially deceptive models, it leverages a cooperative model to discover potentially superhuman concepts that can improve humans' mental model of machines. Agentic interpretability introduces challenges, particularly in evaluation, due to what we call `human-entangled-in-the-loop' nature (humans responses are integral part of the algorithm), making the design and evaluation difficult. We discuss possible solutions and proxy goals. As LLMs approach human parity in many tasks, agentic interpretability's promise is to help humans learn the potentially superhuman concepts of the LLMs, rather than see us fall increasingly far from understanding them.",
        "timestamp": "2025-06-17T17:16:08.446Z",
        "rating": "novote",
        "publishedDate": "2025/06/13",
        "tags": [
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 84,
        "object_id": "paper:arxiv.2506.12152",
        "created_at": "2025-06-17T17:16:08+00:00",
        "updated_at": "2025-06-17T17:16:32+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.12229": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.12229",
        "url": "https://arxiv.org/pdf/2506.12229",
        "title": "Infini-gram mini: Exact n-gram Search at the Internet Scale with\n  FM-Index",
        "authors": "Hao Xu, Jiacheng Liu, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi",
        "abstract": "Language models are trained mainly on massive text data from the Internet,\nand it becomes increasingly important to understand this data source.\nExact-match search engines enable searching in large text corpora -- counting\nstring appearances and retrieving the enclosing documents -- yet the high\nstorage overhead hinders their application on Internet-scale data. We present\nInfini-gram mini, an efficient and scalable system that can make petabyte-level\ntext corpora searchable. Based on the FM-index data structure (Ferragina and\nManzini, 2000), which simultaneously indexes and compresses text, our system\ncreates indexes with size only 44% of the corpus. Infini-gram mini greatly\nimproves upon the best existing implementation of FM-index in terms of indexing\nspeed (18$\\times$) and memory use during both indexing (3.2$\\times$ reduction)\nand querying (down to a negligible amount). We index 46TB of Internet text in\n50 days with a single 128-core CPU node (or 19 hours if using 75 such nodes).\nWe show one important use case of Infini-gram mini in a large-scale analysis of\nbenchmark contamination. We find several core LM evaluation benchmarks to be\nheavily contaminated in Internet crawls (up to 40% in SQuAD), which could lead\nto overestimating the capabilities of language models if trained on such data.\nWe host a benchmark contamination bulletin to share the contamination rate of\nmany core and community-contributed benchmarks. We also release a web interface\nand an API endpoint to serve general search queries on Infini-gram mini\nindexes.",
        "timestamp": "2025-06-17T18:01:55.586Z",
        "rating": "novote",
        "publishedDate": "2025-06-13T21:13:57Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 86,
        "object_id": "paper:arxiv.2506.12229",
        "created_at": "2025-06-17T18:01:55+00:00",
        "updated_at": "2025-06-17T18:02:20+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.12229": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.12229",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-17T18:32:11.143Z",
            "data": {
              "session_id": "session_1750185129326_msk8i4k",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-06-17T18:24:00.954Z",
              "end_time": "2025-06-17T18:32:09.326Z",
              "heartbeat_count": 97,
              "duration_seconds": 485,
              "idle_seconds": 3,
              "total_elapsed_seconds": 488
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-17T18:34:44.826Z",
            "data": {
              "session_id": "session_1750185284198_b09w501",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-06-17T18:32:27.999Z",
              "end_time": "2025-06-17T18:34:44.198Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 1,
              "total_elapsed_seconds": 136
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-17T22:15:29.023Z",
            "data": {
              "session_id": "session_1750198528364_hzfoa4m",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-06-17T22:02:36.842Z",
              "end_time": "2025-06-17T22:15:28.364Z",
              "heartbeat_count": 154,
              "duration_seconds": 770,
              "idle_seconds": 2,
              "total_elapsed_seconds": 772
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:29:06.528Z",
            "data": {
              "session_id": "session_1751491746159_vlu288u",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-07-02T21:27:44.197Z",
              "end_time": "2025-07-02T21:29:06.159Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 2,
              "total_elapsed_seconds": 82
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:33:36.733Z",
            "data": {
              "session_id": "session_1751492016279_5gbleve",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-07-02T21:29:19.774Z",
              "end_time": "2025-07-02T21:33:36.279Z",
              "heartbeat_count": 51,
              "duration_seconds": 255,
              "idle_seconds": 2,
              "total_elapsed_seconds": 257
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:35:08.754Z",
            "data": {
              "session_id": "session_1751492108331_qp36tin",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-07-02T21:34:31.147Z",
              "end_time": "2025-07-02T21:35:08.331Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:37:21.536Z",
            "data": {
              "session_id": "session_1751492241149_t8usv8f",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-07-02T21:35:16.668Z",
              "end_time": "2025-07-02T21:37:21.149Z",
              "heartbeat_count": 24,
              "duration_seconds": 120,
              "idle_seconds": 4,
              "total_elapsed_seconds": 124
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:47:14.143Z",
            "data": {
              "session_id": "session_1751492833952_rp3s252",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-07-02T21:47:00.114Z",
              "end_time": "2025-07-02T21:47:13.952Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:55:08.711Z",
            "data": {
              "session_id": "session_1751493308356_j2vg0u7",
              "source_id": "arxiv",
              "paper_id": "2506.12229",
              "start_time": "2025-07-02T21:47:22.216Z",
              "end_time": "2025-07-02T21:55:08.356Z",
              "heartbeat_count": 93,
              "duration_seconds": 465,
              "idle_seconds": 1,
              "total_elapsed_seconds": 466
            }
          }
        ]
      },
      "meta": {
        "issue_number": 87,
        "object_id": "interactions:arxiv.2506.12229",
        "created_at": "2025-06-17T18:23:34+00:00",
        "updated_at": "2025-07-02T21:55:32+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.23501": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.23501",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-17T18:43:18.046Z",
            "data": {
              "session_id": "session_1750185798041_53upc5d",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-17T18:43:16.610Z",
              "end_time": "2025-06-17T18:43:18.041Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-23T18:14:33.885Z",
            "data": {
              "session_id": "session_1750702473880_yt9k5yw",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-23T18:14:24.531Z",
              "end_time": "2025-06-23T18:14:33.880Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-24T16:08:54.240Z",
            "data": {
              "session_id": "session_1750781333980_tu21zrx",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-24T16:08:52.995Z",
              "end_time": "2025-06-24T16:08:53.980Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-24T21:07:13.490Z",
            "data": {
              "session_id": "session_1750799233200_y9smqj4",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-24T21:06:44.607Z",
              "end_time": "2025-06-24T21:07:13.200Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T18:00:05.530Z",
            "data": {
              "session_id": "session_1750874405121_yhm5ndo",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-25T17:56:12.809Z",
              "end_time": "2025-06-25T18:00:05.121Z",
              "heartbeat_count": 46,
              "duration_seconds": 230,
              "idle_seconds": 2,
              "total_elapsed_seconds": 232
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T22:27:11.681Z",
            "data": {
              "session_id": "session_1750890430990_d8yzx7k",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-25T22:19:47.182Z",
              "end_time": "2025-06-25T22:27:10.990Z",
              "heartbeat_count": 88,
              "duration_seconds": 440,
              "idle_seconds": 4,
              "total_elapsed_seconds": 444
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T22:36:04.125Z",
            "data": {
              "session_id": "session_1750890963677_biwkk2h",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-06-25T22:35:58.004Z",
              "end_time": "2025-06-25T22:36:03.677Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:58:10.261Z",
            "data": {
              "session_id": "session_1751493490223_198cgrv",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T21:57:17.251Z",
              "end_time": "2025-07-02T21:58:10.223Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 3,
              "total_elapsed_seconds": 53
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:59:40.607Z",
            "data": {
              "session_id": "session_1751493580233_n3qdzlp",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T21:58:14.341Z",
              "end_time": "2025-07-02T21:59:40.233Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 1,
              "total_elapsed_seconds": 86
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T22:01:12.935Z",
            "data": {
              "session_id": "session_1751493672518_69m813a",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T22:00:36.295Z",
              "end_time": "2025-07-02T22:01:12.518Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 1,
              "total_elapsed_seconds": 36
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T22:29:09.665Z",
            "data": {
              "session_id": "session_1751495349280_r9v2bt2",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T22:27:18.440Z",
              "end_time": "2025-07-02T22:29:09.280Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 1,
              "total_elapsed_seconds": 111
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T22:33:14.358Z",
            "data": {
              "session_id": "session_1751495594344_afnjewt",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T22:33:08.211Z",
              "end_time": "2025-07-02T22:33:14.344Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T22:40:20.751Z",
            "data": {
              "session_id": "session_1751496020369_z8y5b7z",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T22:33:57.421Z",
              "end_time": "2025-07-02T22:40:20.369Z",
              "heartbeat_count": 76,
              "duration_seconds": 380,
              "idle_seconds": 3,
              "total_elapsed_seconds": 383
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T22:55:50.213Z",
            "data": {
              "session_id": "session_1751496949823_yvnydpq",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T22:50:19.343Z",
              "end_time": "2025-07-02T22:55:49.822Z",
              "heartbeat_count": 66,
              "duration_seconds": 330,
              "idle_seconds": 0,
              "total_elapsed_seconds": 330
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T23:03:39.110Z",
            "data": {
              "session_id": "session_1751497418674_g8987st",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T22:55:59.068Z",
              "end_time": "2025-07-02T23:03:38.674Z",
              "heartbeat_count": 91,
              "duration_seconds": 455,
              "idle_seconds": 5,
              "total_elapsed_seconds": 460
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T23:08:56.046Z",
            "data": {
              "session_id": "session_1751497735648_lc9mn3b",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T23:03:51.287Z",
              "end_time": "2025-07-02T23:08:55.648Z",
              "heartbeat_count": 60,
              "duration_seconds": 300,
              "idle_seconds": 4,
              "total_elapsed_seconds": 304
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T23:21:04.273Z",
            "data": {
              "session_id": "session_1751498463886_pow7k71",
              "source_id": "arxiv",
              "paper_id": "2410.23501",
              "start_time": "2025-07-02T23:08:59.685Z",
              "end_time": "2025-07-02T23:21:03.886Z",
              "heartbeat_count": 144,
              "duration_seconds": 720,
              "idle_seconds": 4,
              "total_elapsed_seconds": 724
            }
          }
        ]
      },
      "meta": {
        "issue_number": 88,
        "object_id": "interactions:arxiv.2410.23501",
        "created_at": "2025-06-17T18:38:15+00:00",
        "updated_at": "2025-07-02T23:21:31+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.05017": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.05017",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-18T00:51:49.537Z",
            "data": {
              "session_id": "session_1750207908061_v27lxoz",
              "source_id": "arxiv",
              "paper_id": "2505.05017",
              "start_time": "2025-06-18T00:51:40.505Z",
              "end_time": "2025-06-18T00:51:48.061Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T23:07:47.552Z",
            "data": {
              "session_id": "session_1750979267332_yp1yqm1",
              "source_id": "arxiv",
              "paper_id": "2505.05017",
              "start_time": "2025-06-26T23:07:39.330Z",
              "end_time": "2025-06-26T23:07:47.332Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 89,
        "object_id": "interactions:arxiv.2505.05017",
        "created_at": "2025-06-18T00:51:50+00:00",
        "updated_at": "2025-06-26T23:08:10+00:00",
        "version": 1
      }
    },
    "paper:openreview.tPNHOoZFl9": {
      "data": {
        "sourceId": "openreview",
        "paperId": "tPNHOoZFl9",
        "url": "https://openreview.net/forum?id=tPNHOoZFl9",
        "title": "Learning Dynamics of LLM Finetuning",
        "authors": "Yi Ren, Danica J. Sutherland",
        "abstract": "Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, \ngives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique ``squeezing effect'' to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance.",
        "timestamp": "2025-06-18T17:33:36.585Z",
        "rating": "novote",
        "publishedDate": "22 Jan 2025",
        "tags": [
          "Learning dynamics",
          "LLM",
          "finetuning",
          "DPO"
        ],
        "doi": "",
        "journalName": "ICLR 2025 Oral",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 91,
        "object_id": "paper:openreview.tPNHOoZFl9",
        "created_at": "2025-06-18T17:33:36+00:00",
        "updated_at": "2025-06-18T17:33:58+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.13886": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.13886",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:27:11.125Z",
            "data": {
              "session_id": "session_1751578030950_sm9rau5",
              "source_id": "arxiv",
              "paper_id": "2506.13886",
              "start_time": "2025-07-03T21:27:01.508Z",
              "end_time": "2025-07-03T21:27:10.949Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 93,
        "object_id": "interactions:arxiv.2506.13886",
        "created_at": "2025-06-18T20:08:11+00:00",
        "updated_at": "2025-07-03T21:27:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.13886": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.13886",
        "url": "https://arxiv.org/pdf/2506.13886",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-18T20:07:45.466Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 92,
        "object_id": "paper:arxiv.2506.13886",
        "created_at": "2025-06-18T20:07:45+00:00",
        "updated_at": "2025-06-18T20:08:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.08872": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.08872",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-18T20:09:48.467Z",
            "data": {
              "session_id": "session_1750277388432_wa51wiv",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-18T20:09:33.492Z",
              "end_time": "2025-06-18T20:09:48.432Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 5,
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-22T21:11:02.332Z",
            "data": {
              "session_id": "session_1750626661672_5ylsih6",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-22T21:10:47.545Z",
              "end_time": "2025-06-22T21:11:01.672Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-23T17:46:59.550Z",
            "data": {
              "session_id": "session_1750700819038_t0mgd1y",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-23T17:45:41.603Z",
              "end_time": "2025-06-23T17:46:59.038Z",
              "heartbeat_count": 15,
              "duration_seconds": 75,
              "idle_seconds": 2,
              "total_elapsed_seconds": 77
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-23T17:59:04.553Z",
            "data": {
              "session_id": "session_1750701543912_bxdaxpr",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-23T17:58:29.780Z",
              "end_time": "2025-06-23T17:59:03.912Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-24T16:08:52.997Z",
            "data": {
              "session_id": "session_1750781332994_ym08pko",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-24T16:08:33.097Z",
              "end_time": "2025-06-24T16:08:52.994Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:24:20.126Z",
            "data": {
              "session_id": "session_1750976659635_b7ztxtx",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-26T22:18:48.664Z",
              "end_time": "2025-06-26T22:24:19.635Z",
              "heartbeat_count": 66,
              "duration_seconds": 330,
              "idle_seconds": 1,
              "total_elapsed_seconds": 331
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:25:29.148Z",
            "data": {
              "session_id": "session_1750976729144_oukloqi",
              "source_id": "arxiv",
              "paper_id": "2506.08872",
              "start_time": "2025-06-26T22:25:21.866Z",
              "end_time": "2025-06-26T22:25:29.144Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 96,
        "object_id": "interactions:arxiv.2506.08872",
        "created_at": "2025-06-18T20:09:49+00:00",
        "updated_at": "2025-06-26T22:25:50+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.08872": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.08872",
        "url": "https://arxiv.org/pdf/2506.08872",
        "title": "",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-18T20:09:29.973Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 95,
        "object_id": "paper:arxiv.2506.08872",
        "created_at": "2025-06-18T20:09:30+00:00",
        "updated_at": "2025-06-18T20:09:53+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.08324": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.08324",
        "url": "https://arxiv.org/html/2411.08324v1",
        "title": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-06-18T20:09:04.974Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 94,
        "object_id": "paper:arxiv.2411.08324",
        "created_at": "2025-06-18T20:09:05+00:00",
        "updated_at": "2025-06-18T20:09:27+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.06298": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.06298",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-22T19:58:05.405Z",
            "data": {
              "session_id": "session_1750622284879_mabvs41",
              "source_id": "arxiv",
              "paper_id": "2506.06298",
              "start_time": "2025-06-22T19:57:41.387Z",
              "end_time": "2025-06-22T19:58:04.879Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-22T20:01:40.848Z",
            "data": {
              "session_id": "session_1750622500408_yeai48u",
              "source_id": "arxiv",
              "paper_id": "2506.06298",
              "start_time": "2025-06-22T20:01:06.077Z",
              "end_time": "2025-06-22T20:01:40.408Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          }
        ]
      },
      "meta": {
        "issue_number": 99,
        "object_id": "interactions:arxiv.2506.06298",
        "created_at": "2025-06-22T19:58:05+00:00",
        "updated_at": "2025-06-22T20:01:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.06298": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.06298",
        "url": "https://arxiv.org/pdf/2506.06298",
        "title": "Pairwise Calibrated Rewards for Pluralistic Alignment",
        "authors": "Daniel Halpern, Evi Micha, Ariel D. Procaccia, Itai Shapira",
        "abstract": "Current alignment pipelines presume a single, universal notion of desirable\nbehavior. However, human preferences often diverge across users, contexts, and\ncultures. As a result, disagreement collapses into the majority signal and\nminority perspectives are discounted. To address this, we propose reflecting\ndiverse human preferences through a distribution over multiple reward\nfunctions, each inducing a distinct aligned policy. The distribution is learned\ndirectly from pairwise preference without annotator identifiers or predefined\ngroups. Instead, annotator disagreements are treated as informative soft\nlabels. Our central criterion is pairwise calibration: for every pair of\ncandidate responses, the proportion of reward functions preferring one response\nmatches the fraction of annotators with that preference. We prove that even a\nsmall outlier-free ensemble can accurately represent diverse preference\ndistributions. Empirically, we introduce and validate a practical training\nheuristic to learn such ensembles, and demonstrate its effectiveness through\nimproved calibration, implying a more faithful representation of pluralistic\nvalues.",
        "timestamp": "2025-06-22T19:57:41.869Z",
        "rating": "novote",
        "publishedDate": "2025-05-17T18:38:24Z",
        "tags": [
          "cs.LG",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 97,
        "object_id": "paper:arxiv.2506.06298",
        "created_at": "2025-06-22T19:57:42+00:00",
        "updated_at": "2025-06-22T19:58:03+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.14200": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.14200",
        "url": "https://arxiv.org/pdf/2506.14200",
        "title": "ELI-Why: Evaluating the Pedagogical Utility of Language Model\n  Explanations",
        "authors": "Brihi Joshi, Keyu He, Sahana Ramnath, Sadra Sabouri, Kaitlyn Zhou, Souti Chattopadhyay, Swabha Swayamdipta, Xiang Ren",
        "abstract": "Language models today are widely used in education, yet their ability to\ntailor responses for learners with varied informational needs and knowledge\nbackgrounds remains under-explored. To this end, we introduce ELI-Why, a\nbenchmark of 13.4K \"Why\" questions to evaluate the pedagogical capabilities of\nlanguage models. We then conduct two extensive human studies to assess the\nutility of language model-generated explanatory answers (explanations) on our\nbenchmark, tailored to three distinct educational grades: elementary,\nhigh-school and graduate school. In our first study, human raters assume the\nrole of an \"educator\" to assess model explanations' fit to different\neducational grades. We find that GPT-4-generated explanations match their\nintended educational background only 50% of the time, compared to 79% for lay\nhuman-curated explanations. In our second study, human raters assume the role\nof a learner to assess if an explanation fits their own informational needs.\nAcross all educational backgrounds, users deemed GPT-4-generated explanations\n20% less suited on average to their informational needs, when compared to\nexplanations curated by lay people. Additionally, automated evaluation metrics\nreveal that explanations generated across different language model families for\ndifferent informational needs remain indistinguishable in their grade-level,\nlimiting their pedagogical effectiveness.",
        "timestamp": "2025-06-23T18:25:55.019Z",
        "rating": "novote",
        "publishedDate": "2025-06-17T05:36:39Z",
        "tags": [
          "cs.CL",
          "cs.HC"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 100,
        "object_id": "paper:arxiv.2506.14200",
        "created_at": "2025-06-23T18:25:55+00:00",
        "updated_at": "2025-06-23T18:26:22+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.15758": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.15758",
        "url": "https://arxiv.org/pdf/2404.15758?",
        "title": "Let's Think Dot by Dot: Hidden Computation in Transformer Language\n  Models",
        "authors": "Jacob Pfau, William Merrill, Samuel R. Bowman",
        "abstract": "Chain-of-thought responses from language models improve performance across\nmost benchmarks. However, it remains unclear to what extent these performance\ngains can be attributed to human-like task decomposition or simply the greater\ncomputation that additional tokens allow. We show that transformers can use\nmeaningless filler tokens (e.g., '......') in place of a chain of thought to\nsolve two hard algorithmic tasks they could not solve when responding without\nintermediate tokens. However, we find empirically that learning to use filler\ntokens is difficult and requires specific, dense supervision to converge. We\nalso provide a theoretical characterization of the class of problems where\nfiller tokens are useful in terms of the quantifier depth of a first-order\nformula. For problems satisfying this characterization, chain-of-thought tokens\nneed not provide information about the intermediate computational steps\ninvolved in multi-token computations. In summary, our results show that\nadditional tokens can provide computational benefits independent of token\nchoice. The fact that intermediate tokens can act as filler tokens raises\nconcerns about large language models engaging in unauditable, hidden\ncomputations that are increasingly detached from the observed chain-of-thought\ntokens.",
        "timestamp": "2025-06-23T21:49:21.232Z",
        "rating": "novote",
        "publishedDate": "2024-04-24T09:30:00Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "I.2.6"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 101,
        "object_id": "paper:arxiv.2404.15758",
        "created_at": "2025-06-23T21:49:21+00:00",
        "updated_at": "2025-06-23T21:49:40+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.15758": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.15758",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-23T21:51:04.458Z",
            "data": {
              "session_id": "session_1750715463752_oh486es",
              "source_id": "arxiv",
              "paper_id": "2404.15758",
              "start_time": "2025-06-23T21:49:20.794Z",
              "end_time": "2025-06-23T21:51:03.752Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 3,
              "total_elapsed_seconds": 103
            }
          }
        ]
      },
      "meta": {
        "issue_number": 102,
        "object_id": "interactions:arxiv.2404.15758",
        "created_at": "2025-06-23T21:51:04+00:00",
        "updated_at": "2025-06-23T21:51:24+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2403.14653": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.14653",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T14:27:04.652Z",
            "data": {
              "session_id": "session_1750861623868_3zmswio",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-06-25T14:26:57.595Z",
              "end_time": "2025-06-25T14:27:03.868Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T17:52:11.022Z",
            "data": {
              "session_id": "session_1750873930624_tkgnau3",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-06-25T17:48:46.062Z",
              "end_time": "2025-06-25T17:52:10.624Z",
              "heartbeat_count": 40,
              "duration_seconds": 200,
              "idle_seconds": 5,
              "total_elapsed_seconds": 205
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T22:19:33.469Z",
            "data": {
              "session_id": "session_1750889973067_iyx5r60",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-06-25T22:16:46.384Z",
              "end_time": "2025-06-25T22:19:33.067Z",
              "heartbeat_count": 33,
              "duration_seconds": 165,
              "idle_seconds": 2,
              "total_elapsed_seconds": 167
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T23:00:20.237Z",
            "data": {
              "session_id": "session_1750892420038_2m7nnw2",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-06-25T23:00:18.959Z",
              "end_time": "2025-06-25T23:00:20.038Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 1
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:45:21.717Z",
            "data": {
              "session_id": "session_1751568321705_8oy5xqx",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-07-03T18:45:13.509Z",
              "end_time": "2025-07-03T18:45:21.705Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T19:17:59.401Z",
            "data": {
              "session_id": "session_1751570279142_or3am8b",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-07-03T19:17:47.693Z",
              "end_time": "2025-07-03T19:17:59.142Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:06:33.500Z",
            "data": {
              "session_id": "session_1751576793486_1dhgws4",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-07-03T21:05:36.038Z",
              "end_time": "2025-07-03T21:06:33.486Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 2,
              "total_elapsed_seconds": 57
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:15:32.679Z",
            "data": {
              "session_id": "session_1751577332673_dc5n751",
              "source_id": "arxiv",
              "paper_id": "2403.14653",
              "start_time": "2025-07-03T21:15:21.364Z",
              "end_time": "2025-07-03T21:15:32.673Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 105,
        "object_id": "interactions:arxiv.2403.14653",
        "created_at": "2025-06-25T14:27:05+00:00",
        "updated_at": "2025-07-03T21:15:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.14653": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.14653",
        "url": "https://arxiv.org/abs/2403.14653",
        "title": "Between Copyright and Computer Science: The Law and Ethics of Generative AI",
        "authors": "Deven R. Desai, Mark Riedl",
        "abstract": "Copyright and computer science continue to intersect and clash, but they can coexist. The advent of new technologies such as digitization of visual and aural creations, sharing technologies, search engines, social media offerings, and more challenge copyright-based industries and reopen questions about the reach of copyright law. Breakthroughs in artificial intelligence research, especially Large Language Models that leverage copyrighted material as part of training models, are the latest examples of the ongoing tension between copyright and computer science. The exuberance, rush-to-market, and edge problem cases created by a few misguided companies now raises challenges to core legal doctrines and may shift Open Internet practices for the worse. That result does not have to be, and should not be, the outcome.\nThis Article shows that, contrary to some scholars' views, fair use law does not bless all ways that someone can gain access to copyrighted material even when the purpose is fair use. Nonetheless, the scientific need for more data to advance AI research means access to large book corpora and the Open Internet is vital for the future of that research. The copyright industry claims, however, that almost all uses of copyrighted material must be compensated, even for non-expressive uses. The Article's solution accepts that both sides need to change. It is one that forces the computer science world to discipline its behaviors and, in some cases, pay for copyrighted material. It also requires the copyright industry to abandon its belief that all uses must be compensated or restricted to uses sanctioned by the copyright industry. As part of this re-balancing, the Article addresses a problem that has grown out of this clash and under theorized.",
        "timestamp": "2025-06-25T14:26:55.279Z",
        "rating": "novote",
        "publishedDate": "2024/02/24",
        "tags": [
          "Computers and Society (cs.CY)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 103,
        "object_id": "paper:arxiv.2403.14653",
        "created_at": "2025-06-25T14:26:55+00:00",
        "updated_at": "2025-06-25T14:27:21+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2006.03010": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2006.03010",
        "url": "https://arxiv.org/abs/2006.03010",
        "title": "Syntactic Search by Example",
        "authors": "Micah Shlain, Hillel Taub-Tabib, Shoval Sadde, Yoav Goldberg",
        "abstract": "We present a system that allows a user to search a large linguistically annotated corpus using syntactic patterns over dependency graphs. In contrast to previous attempts to this effect, we introduce a light-weight query language that does not require the user to know the details of the underlying syntactic representations, and instead to query the corpus by providing an example sentence coupled with simple markup. Search is performed at an interactive speed due to an efficient linguistic graph-indexing and retrieval engine. This allows for rapid exploration, development and refinement of syntax-based queries. We demonstrate the system using queries over two corpora: the English wikipedia, and a collection of English pubmed abstracts. A demo of the wikipedia system is available at: this https URL",
        "timestamp": "2025-06-25T18:10:46.604Z",
        "rating": "novote",
        "publishedDate": "2020/06/04",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 106,
        "object_id": "paper:arxiv.2006.03010",
        "created_at": "2025-06-25T18:10:47+00:00",
        "updated_at": "2025-06-25T18:11:14+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.22592": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.22592",
        "url": "https://arxiv.org/abs/2410.22592",
        "title": "GRADE: Quantifying Sample Diversity in Text-to-Image Models",
        "authors": "Royi Rassin, Aviv Slobodkin, Shauli Ravfogel, Yanai Elazar, Yoav Goldberg",
        "abstract": "We introduce GRADE, an automatic method for quantifying sample diversity in text-to-image models. Our method leverages the world knowledge embedded in large language models and visual question-answering systems to identify relevant concept-specific axes of diversity (e.g., ``shape'' for the concept ``cookie''). It then estimates frequency distributions of concepts and their attributes and quantifies diversity using entropy. We use GRADE to measure the diversity of 12 models over a total of 720K images, revealing that all models display limited variation, with clear deterioration in stronger models. Further, we find that models often exhibit default behaviors, a phenomenon where a model consistently generates concepts with the same attributes (e.g., 98% of the cookies are round). Lastly, we show that a key reason for low diversity is underspecified captions in training data. Our work proposes an automatic, semantically-driven approach to measure sample diversity and highlights the stunning homogeneity in text-to-image models.",
        "timestamp": "2025-06-25T22:50:30.897Z",
        "rating": "novote",
        "publishedDate": "2024/10/29",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 107,
        "object_id": "paper:arxiv.2410.22592",
        "created_at": "2025-06-25T22:50:31+00:00",
        "updated_at": "2025-06-25T22:50:50+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.22592": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.22592",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T22:55:12.492Z",
            "data": {
              "session_id": "session_1750892112449_i8lpcj0",
              "source_id": "arxiv",
              "paper_id": "2410.22592",
              "start_time": "2025-06-25T22:55:03.802Z",
              "end_time": "2025-06-25T22:55:12.449Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-25T23:00:19.369Z",
            "data": {
              "session_id": "session_1750892418959_dtb978p",
              "source_id": "arxiv",
              "paper_id": "2410.22592",
              "start_time": "2025-06-25T22:55:39.567Z",
              "end_time": "2025-06-25T23:00:18.959Z",
              "heartbeat_count": 55,
              "duration_seconds": 275,
              "idle_seconds": 4,
              "total_elapsed_seconds": 279
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T00:23:21.896Z",
            "data": {
              "session_id": "session_1750897401877_fw0a2bl",
              "source_id": "arxiv",
              "paper_id": "2410.22592",
              "start_time": "2025-06-26T00:22:30.140Z",
              "end_time": "2025-06-26T00:23:21.877Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 2,
              "total_elapsed_seconds": 52
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T17:54:24.990Z",
            "data": {
              "session_id": "session_1750960464319_0c20b2q",
              "source_id": "arxiv",
              "paper_id": "2410.22592",
              "start_time": "2025-06-26T17:53:26.185Z",
              "end_time": "2025-06-26T17:54:24.319Z",
              "heartbeat_count": 11,
              "duration_seconds": 55,
              "idle_seconds": 3,
              "total_elapsed_seconds": 58
            }
          }
        ]
      },
      "meta": {
        "issue_number": 108,
        "object_id": "interactions:arxiv.2410.22592",
        "created_at": "2025-06-25T22:55:02+00:00",
        "updated_at": "2025-06-26T17:54:48+00:00",
        "version": 1
      }
    },
    "interactions:openreview.tPNHOoZFl9": {
      "data": {
        "sourceId": "openreview",
        "paperId": "tPNHOoZFl9",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:25:53.273Z",
            "data": {
              "session_id": "session_1750976753057_65m6l41",
              "source_id": "openreview",
              "paper_id": "tPNHOoZFl9",
              "start_time": "2025-06-26T22:25:46.440Z",
              "end_time": "2025-06-26T22:25:53.057Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 109,
        "object_id": "interactions:openreview.tPNHOoZFl9",
        "created_at": "2025-06-26T22:25:53+00:00",
        "updated_at": "2025-06-26T22:26:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2502.12120": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.12120",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T20:22:21.629Z",
            "data": {
              "session_id": "session_1752006141416_ntnndz7",
              "source_id": "arxiv",
              "paper_id": "2502.12120",
              "start_time": "2025-07-08T20:22:01.156Z",
              "end_time": "2025-07-08T20:22:21.416Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 0,
              "total_elapsed_seconds": 20
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T20:26:57.397Z",
            "data": {
              "session_id": "session_1752006417388_fir4wrf",
              "source_id": "arxiv",
              "paper_id": "2502.12120",
              "start_time": "2025-07-08T20:26:34.742Z",
              "end_time": "2025-07-08T20:26:57.388Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T20:29:08.178Z",
            "data": {
              "session_id": "session_1752006548173_a8kduha",
              "source_id": "arxiv",
              "paper_id": "2502.12120",
              "start_time": "2025-07-08T20:28:28.616Z",
              "end_time": "2025-07-08T20:29:08.173Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 5,
              "total_elapsed_seconds": 40
            }
          }
        ]
      },
      "meta": {
        "issue_number": 114,
        "object_id": "interactions:arxiv.2502.12120",
        "created_at": "2025-06-26T22:36:01+00:00",
        "updated_at": "2025-07-08T20:29:32+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.12120": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.12120",
        "url": "https://arxiv.org/abs/2502.12120",
        "title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws",
        "authors": "Prasanna Mayilvahanan, Thadd\u00e4us Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel",
        "abstract": "Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.",
        "timestamp": "2025-06-26T22:35:53.903Z",
        "rating": "novote",
        "publishedDate": "2025/02/17",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 113,
        "object_id": "paper:arxiv.2502.12120",
        "created_at": "2025-06-26T22:35:54+00:00",
        "updated_at": "2025-06-26T22:36:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.04741": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.04741",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:35:51.470Z",
            "data": {
              "session_id": "session_1750977351273_b5h40du",
              "source_id": "arxiv",
              "paper_id": "2505.04741",
              "start_time": "2025-06-26T22:35:34.061Z",
              "end_time": "2025-06-26T22:35:51.273Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "issue_number": 112,
        "object_id": "interactions:arxiv.2505.04741",
        "created_at": "2025-06-26T22:35:52+00:00",
        "updated_at": "2025-06-26T22:36:10+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.08679": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.08679",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:35:30.896Z",
            "data": {
              "session_id": "session_1750977330627_sxgmmia",
              "source_id": "arxiv",
              "paper_id": "2503.08679",
              "start_time": "2025-06-26T22:35:16.533Z",
              "end_time": "2025-06-26T22:35:30.627Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T06:38:34.629Z",
            "data": {
              "session_id": "session_1760683114444_63gropz",
              "source_id": "arxiv",
              "paper_id": "2503.08679",
              "start_time": "2025-10-17T06:38:04.124Z",
              "end_time": "2025-10-17T06:38:34.444Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 0,
              "total_elapsed_seconds": 30
            }
          }
        ]
      },
      "meta": {
        "issue_number": 111,
        "object_id": "interactions:arxiv.2503.08679",
        "created_at": "2025-06-26T22:35:31+00:00",
        "updated_at": "2025-10-17T06:38:56+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2408.03247": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.03247",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:34:59.105Z",
            "data": {
              "session_id": "session_1750977299099_gsn6ml1",
              "source_id": "arxiv",
              "paper_id": "2408.03247",
              "start_time": "2025-06-26T22:34:43.568Z",
              "end_time": "2025-06-26T22:34:59.099Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "issue_number": 110,
        "object_id": "interactions:arxiv.2408.03247",
        "created_at": "2025-06-26T22:34:59+00:00",
        "updated_at": "2025-06-26T22:35:23+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2310.13121": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.13121",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:37:22.182Z",
            "data": {
              "session_id": "session_1750977441990_du0j0aq",
              "source_id": "arxiv",
              "paper_id": "2310.13121",
              "start_time": "2025-06-26T22:37:08.852Z",
              "end_time": "2025-06-26T22:37:21.990Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "issue_number": 118,
        "object_id": "interactions:arxiv.2310.13121",
        "created_at": "2025-06-26T22:37:22+00:00",
        "updated_at": "2025-06-26T22:37:46+00:00",
        "version": 1
      }
    },
    "interactions:openreview.HvSytvg3Jh": {
      "data": {
        "sourceId": "openreview",
        "paperId": "HvSytvg3Jh",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:37:08.365Z",
            "data": {
              "session_id": "session_1750977428111_e9cg9ns",
              "source_id": "openreview",
              "paper_id": "HvSytvg3Jh",
              "start_time": "2025-06-26T22:36:39.931Z",
              "end_time": "2025-06-26T22:37:08.111Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          }
        ]
      },
      "meta": {
        "issue_number": 117,
        "object_id": "interactions:openreview.HvSytvg3Jh",
        "created_at": "2025-06-26T22:37:08+00:00",
        "updated_at": "2025-06-26T22:37:30+00:00",
        "version": 1
      }
    },
    "interactions:openreview.HD6bWcj87Y": {
      "data": {
        "sourceId": "openreview",
        "paperId": "HD6bWcj87Y",
        "interactions": []
      },
      "meta": {
        "issue_number": 116,
        "object_id": "interactions:openreview.HD6bWcj87Y",
        "created_at": "2025-06-26T22:36:40+00:00",
        "updated_at": "2025-06-26T22:36:42+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.00985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.00985",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:36:29.270Z",
            "data": {
              "session_id": "session_1750977388995_l7hc41c",
              "source_id": "arxiv",
              "paper_id": "2505.00985",
              "start_time": "2025-06-26T22:36:13.462Z",
              "end_time": "2025-06-26T22:36:28.995Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "issue_number": 115,
        "object_id": "interactions:arxiv.2505.00985",
        "created_at": "2025-06-26T22:36:29+00:00",
        "updated_at": "2025-06-26T22:36:51+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.09858": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09858",
        "url": "https://arxiv.org/abs/2504.09858",
        "title": "Reasoning Models Can Be Effective Without Thinking",
        "authors": "Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, Matei Zaharia",
        "abstract": "Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling.",
        "timestamp": "2025-06-26T22:38:53.205Z",
        "rating": "novote",
        "publishedDate": "2025/04/14",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 121,
        "object_id": "paper:arxiv.2504.09858",
        "created_at": "2025-06-26T22:38:53+00:00",
        "updated_at": "2025-06-26T22:39:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.09522": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.09522",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:23:38.689Z",
            "data": {
              "session_id": "session_1760721818341_8qux928",
              "source_id": "arxiv",
              "paper_id": "2504.09522",
              "start_time": "2025-10-17T17:23:31.196Z",
              "end_time": "2025-10-17T17:23:38.341Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 2,
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "issue_number": 120,
        "object_id": "interactions:arxiv.2504.09522",
        "created_at": "2025-06-26T22:38:40+00:00",
        "updated_at": "2025-10-17T17:24:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.18114": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.18114",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:38:19.948Z",
            "data": {
              "session_id": "session_1750977499736_co3bk0b",
              "source_id": "arxiv",
              "paper_id": "2504.18114",
              "start_time": "2025-06-26T22:37:25.727Z",
              "end_time": "2025-06-26T22:38:19.736Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 4,
              "total_elapsed_seconds": 54
            }
          }
        ]
      },
      "meta": {
        "issue_number": 119,
        "object_id": "interactions:arxiv.2504.18114",
        "created_at": "2025-06-26T22:38:20+00:00",
        "updated_at": "2025-06-26T22:38:41+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.21934": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21934",
        "interactions": []
      },
      "meta": {
        "issue_number": 123,
        "object_id": "interactions:arxiv.2503.21934",
        "created_at": "2025-06-26T22:39:47+00:00",
        "updated_at": "2025-06-26T22:39:49+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.21934": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.21934",
        "url": "https://arxiv.org/abs/2503.21934",
        "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad",
        "authors": "Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovi\u0107, Nikola Jovanovi\u0107, Martin Vechev",
        "abstract": "Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce the first comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly: only Gemini-2.5-Pro achieves a non-trivial score of 25%, while all other models achieve less than 5%. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities.",
        "timestamp": "2025-06-26T22:39:32.573Z",
        "rating": "novote",
        "publishedDate": "2025/03/27",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 122,
        "object_id": "paper:arxiv.2503.21934",
        "created_at": "2025-06-26T22:39:32+00:00",
        "updated_at": "2025-06-26T22:40:02+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2501.17148": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.17148",
        "interactions": []
      },
      "meta": {
        "issue_number": 128,
        "object_id": "interactions:arxiv.2501.17148",
        "created_at": "2025-06-26T22:43:13+00:00",
        "updated_at": "2025-06-26T22:43:15+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2311.12786": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.12786",
        "interactions": []
      },
      "meta": {
        "issue_number": 127,
        "object_id": "interactions:arxiv.2311.12786",
        "created_at": "2025-06-26T22:42:55+00:00",
        "updated_at": "2025-06-26T22:42:57+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2408.12578": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.12578",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:42:44.191Z",
            "data": {
              "session_id": "session_1750977763894_pvti6ps",
              "source_id": "arxiv",
              "paper_id": "2408.12578",
              "start_time": "2025-06-26T22:42:32.922Z",
              "end_time": "2025-06-26T22:42:43.894Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 126,
        "object_id": "interactions:arxiv.2408.12578",
        "created_at": "2025-06-26T22:42:44+00:00",
        "updated_at": "2025-06-26T22:43:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.04289": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.04289",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:42:20.692Z",
            "data": {
              "session_id": "session_1750977740588_8yjc7zf",
              "source_id": "arxiv",
              "paper_id": "2406.04289",
              "start_time": "2025-06-26T22:42:14.188Z",
              "end_time": "2025-06-26T22:42:20.588Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 125,
        "object_id": "interactions:arxiv.2406.04289",
        "created_at": "2025-06-26T22:42:21+00:00",
        "updated_at": "2025-06-26T22:42:41+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2210.10749": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.10749",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:42:12.305Z",
            "data": {
              "session_id": "session_1750977732291_pvlhzfz",
              "source_id": "arxiv",
              "paper_id": "2210.10749",
              "start_time": "2025-06-26T22:41:52.814Z",
              "end_time": "2025-06-26T22:42:12.291Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 4,
              "total_elapsed_seconds": 19
            }
          }
        ]
      },
      "meta": {
        "issue_number": 124,
        "object_id": "interactions:arxiv.2210.10749",
        "created_at": "2025-06-26T22:42:12+00:00",
        "updated_at": "2025-06-26T22:42:36+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.11985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11985",
        "interactions": []
      },
      "meta": {
        "issue_number": 131,
        "object_id": "interactions:arxiv.2503.11985",
        "created_at": "2025-06-26T22:44:22+00:00",
        "updated_at": "2025-06-26T22:44:23+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.11985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.11985",
        "url": "https://arxiv.org/abs/2503.11985",
        "title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language Models",
        "authors": "Charaka Vinayak Kumar, Ashok Urlana, Gopichand Kanumolu, Bala Mallikarjunarao Garlapati, Pruthwik Mishra",
        "abstract": "Advancements in Large Language Models (LLMs) have increased the performance of different natural language understanding as well as generation tasks. Although LLMs have breached the state-of-the-art performance in various tasks, they often reflect different forms of bias present in the training data. In the light of this perceived limitation, we provide a unified evaluation of benchmarks using a set of representative small and medium-sized LLMs that cover different forms of biases starting from physical characteristics to socio-economic categories. Moreover, we propose five prompting approaches to carry out the bias detection task across different aspects of bias. Further, we formulate three research questions to gain valuable insight in detecting biases in LLMs using different approaches and evaluation metrics across benchmarks. The results indicate that each of the selected LLMs suffer from one or the other form of bias with the Phi-3.5B model being the least biased. Finally, we conclude the paper with the identification of key challenges and possible future directions.",
        "timestamp": "2025-06-26T22:44:15.683Z",
        "rating": "novote",
        "publishedDate": "2025/03/15",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 130,
        "object_id": "paper:arxiv.2503.11985",
        "created_at": "2025-06-26T22:44:15+00:00",
        "updated_at": "2025-06-26T22:44:33+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.10061": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.10061",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:43:25.994Z",
            "data": {
              "session_id": "session_1750977805562_4m204um",
              "source_id": "arxiv",
              "paper_id": "2503.10061",
              "start_time": "2025-06-26T22:43:19.462Z",
              "end_time": "2025-06-26T22:43:25.562Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 129,
        "object_id": "interactions:arxiv.2503.10061",
        "created_at": "2025-06-26T22:43:26+00:00",
        "updated_at": "2025-06-26T22:43:45+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2409.12183": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2409.12183",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:46:29.873Z",
            "data": {
              "session_id": "session_1750977989869_2q6i5sq",
              "source_id": "arxiv",
              "paper_id": "2409.12183",
              "start_time": "2025-06-26T22:46:21.162Z",
              "end_time": "2025-06-26T22:46:29.869Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 135,
        "object_id": "interactions:arxiv.2409.12183",
        "created_at": "2025-06-26T22:45:44+00:00",
        "updated_at": "2025-06-26T22:46:47+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2411.12580": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.12580",
        "interactions": []
      },
      "meta": {
        "issue_number": 134,
        "object_id": "interactions:arxiv.2411.12580",
        "created_at": "2025-06-26T22:45:36+00:00",
        "updated_at": "2025-06-26T22:45:38+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.14481": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14481",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:45:25.329Z",
            "data": {
              "session_id": "session_1750977925232_jdhzshm",
              "source_id": "arxiv",
              "paper_id": "2503.14481",
              "start_time": "2025-06-26T22:45:17.481Z",
              "end_time": "2025-06-26T22:45:25.232Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 133,
        "object_id": "interactions:arxiv.2503.14481",
        "created_at": "2025-06-26T22:45:25+00:00",
        "updated_at": "2025-06-26T22:45:46+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2006.07710": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2006.07710",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:45:10.662Z",
            "data": {
              "session_id": "session_1750977910655_qta4tik",
              "source_id": "arxiv",
              "paper_id": "2006.07710",
              "start_time": "2025-06-26T22:44:40.578Z",
              "end_time": "2025-06-26T22:45:10.655Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 0,
              "total_elapsed_seconds": 30
            }
          }
        ]
      },
      "meta": {
        "issue_number": 132,
        "object_id": "interactions:arxiv.2006.07710",
        "created_at": "2025-06-26T22:45:11+00:00",
        "updated_at": "2025-06-26T22:45:31+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.01822": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.01822",
        "interactions": []
      },
      "meta": {
        "issue_number": 139,
        "object_id": "interactions:arxiv.2503.01822",
        "created_at": "2025-06-26T22:47:04+00:00",
        "updated_at": "2025-06-26T22:47:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.01822": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.01822",
        "url": "https://arxiv.org/abs/2503.01822",
        "title": "Projecting Assumptions: The Duality Between Sparse Autoencoders and Concept Geometry",
        "authors": "Sai Sumedh R. Hindupur, Ekdeep Singh Lubana, Thomas Fel, Demba Ba",
        "abstract": "Sparse Autoencoders (SAEs) are widely used to interpret neural networks by identifying meaningful concepts from their representations. However, do SAEs truly uncover all concepts a model relies on, or are they inherently biased toward certain kinds of concepts? We introduce a unified framework that recasts SAEs as solutions to a bilevel optimization problem, revealing a fundamental challenge: each SAE imposes structural assumptions about how concepts are encoded in model representations, which in turn shapes what it can and cannot detect. This means different SAEs are not interchangeable -- switching architectures can expose entirely new concepts or obscure existing ones. To systematically probe this effect, we evaluate SAEs across a spectrum of settings: from controlled toy models that isolate key variables, to semi-synthetic experiments on real model activations and finally to large-scale, naturalistic datasets. Across this progression, we examine two fundamental properties that real-world concepts often exhibit: heterogeneity in intrinsic dimensionality (some concepts are inherently low-dimensional, others are not) and nonlinear separability. We show that SAEs fail to recover concepts when these properties are ignored, and we design a new SAE that explicitly incorporates both, enabling the discovery of previously hidden concepts and reinforcing our theoretical insights. Our findings challenge the idea of a universal SAE and underscores the need for architecture-specific choices in model interpretability. Overall, we argue an SAE does not just reveal concepts -- it determines what can be seen at all.",
        "timestamp": "2025-06-26T22:46:59.177Z",
        "rating": "novote",
        "publishedDate": "2025/03/03",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 138,
        "object_id": "paper:arxiv.2503.01822",
        "created_at": "2025-06-26T22:46:59+00:00",
        "updated_at": "2025-06-26T22:47:17+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.03862": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.03862",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:46:52.455Z",
            "data": {
              "session_id": "session_1750978012437_szcq18d",
              "source_id": "arxiv",
              "paper_id": "2503.03862",
              "start_time": "2025-06-26T22:46:37.271Z",
              "end_time": "2025-06-26T22:46:52.437Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 137,
        "object_id": "interactions:arxiv.2503.03862",
        "created_at": "2025-06-26T22:46:52+00:00",
        "updated_at": "2025-06-26T22:47:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.03862": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.03862",
        "url": "https://arxiv.org/abs/2503.03862",
        "title": "Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions",
        "authors": "Emmy Liu, Amanda Bertsch, Lintang Sutawika, Lindia Tjuatja, Patrick Fernandes, Lara Marinov, Michael Chen, Shreya Singhal, Carolin Lawrence, Aditi Raghunathan, Kiril Gashteovski, Graham Neubig",
        "abstract": "Improvements in language model capabilities are often attributed to increasing model size or training data, but in some cases smaller models trained on curated data or with different architectural decisions can outperform larger ones trained on more tokens. What accounts for this? To quantify the impact of these design choices, we meta-analyze 92 open-source pretrained models across a wide array of scales, including state-of-the-art open-weights models as well as less performant models and those with less conventional design decisions. We find that by incorporating features besides model size and number of training tokens, we can achieve a relative 3-28% increase in ability to predict downstream performance compared with using scale alone. Analysis of model design decisions reveal insights into data composition, such as the trade-off between language and code tasks at 15-25\\% code, as well as the better performance of some architectural decisions such as choosing rotary over learned embeddings. Broadly, our framework lays a foundation for more systematic investigation of how model development choices shape final capabilities.",
        "timestamp": "2025-06-26T22:46:34.099Z",
        "rating": "novote",
        "publishedDate": "2025/03/05",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 136,
        "object_id": "paper:arxiv.2503.03862",
        "created_at": "2025-06-26T22:46:34+00:00",
        "updated_at": "2025-06-26T22:46:56+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2412.20292": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.20292",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T22:58:27.854Z",
            "data": {
              "session_id": "session_1750978707460_3d5tc43",
              "source_id": "arxiv",
              "paper_id": "2412.20292",
              "start_time": "2025-06-26T22:56:12.824Z",
              "end_time": "2025-06-26T22:58:27.460Z",
              "heartbeat_count": 26,
              "duration_seconds": 130,
              "idle_seconds": 5,
              "total_elapsed_seconds": 135
            }
          }
        ]
      },
      "meta": {
        "issue_number": 141,
        "object_id": "interactions:arxiv.2412.20292",
        "created_at": "2025-06-26T22:51:46+00:00",
        "updated_at": "2025-06-26T22:58:44+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2407.15845": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.15845",
        "interactions": []
      },
      "meta": {
        "issue_number": 140,
        "object_id": "interactions:arxiv.2407.15845",
        "created_at": "2025-06-26T22:50:50+00:00",
        "updated_at": "2025-06-26T22:50:51+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2411.13981": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.13981",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-06-26T23:07:01.907Z",
            "data": {
              "session_id": "session_1750979221895_ohycwt5",
              "source_id": "arxiv",
              "paper_id": "2411.13981",
              "start_time": "2025-06-26T23:06:52.446Z",
              "end_time": "2025-06-26T23:07:01.895Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 144,
        "object_id": "interactions:arxiv.2411.13981",
        "created_at": "2025-06-26T23:07:02+00:00",
        "updated_at": "2025-06-26T23:07:21+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.02550": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.02550",
        "interactions": []
      },
      "meta": {
        "issue_number": 143,
        "object_id": "interactions:arxiv.2406.02550",
        "created_at": "2025-06-26T23:03:33+00:00",
        "updated_at": "2025-06-26T23:03:34+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2412.20760": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.20760",
        "interactions": []
      },
      "meta": {
        "issue_number": 142,
        "object_id": "interactions:arxiv.2412.20760",
        "created_at": "2025-06-26T23:00:54+00:00",
        "updated_at": "2025-06-26T23:00:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.09070": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.09070",
        "url": "https://arxiv.org/pdf/2406.09070",
        "title": "FairCoT: Enhancing Fairness in Text-to-Image Generation via Chain of\n  Thought Reasoning with Multimodal Large Language Models",
        "authors": "Zahraa Al Sahili, Ioannis Patras, Matthew Purver",
        "abstract": "In the domain of text-to-image generative models, biases inherent in training\ndatasets often propagate into generated content, posing significant ethical\nchallenges, particularly in socially sensitive contexts. We introduce FairCoT,\na novel framework that enhances fairness in text to image models through Chain\nof Thought (CoT) reasoning within multimodal generative large language models.\nFairCoT employs iterative CoT refinement to systematically mitigate biases, and\ndynamically adjusts textual prompts in real time, ensuring diverse and\nequitable representation in generated images. By integrating iterative\nreasoning processes, FairCoT addresses the limitations of zero shot CoT in\nsensitive scenarios, balancing creativity with ethical responsibility.\nExperimental evaluations across popular text-to-image systems including DALLE\nand various Stable Diffusion variants, demonstrate that FairCoT significantly\nenhances fairness and diversity without sacrificing image quality or semantic\nfidelity. By combining robust reasoning, lightweight deployment, and\nextensibility to multiple models, FairCoT represents a promising step toward\nmore socially responsible and transparent AI driven content generation.",
        "timestamp": "2025-07-01T17:39:36.486Z",
        "rating": "novote",
        "publishedDate": "2024-06-13T12:55:10Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CV"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 145,
        "object_id": "paper:arxiv.2406.09070",
        "created_at": "2025-07-01T17:39:36+00:00",
        "updated_at": "2025-07-01T17:40:02+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.09070": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.09070",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-01T17:41:06.306Z",
            "data": {
              "session_id": "session_1751391665868_tbqw9us",
              "source_id": "arxiv",
              "paper_id": "2406.09070",
              "start_time": "2025-07-01T17:39:36.217Z",
              "end_time": "2025-07-01T17:41:05.868Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 5,
              "total_elapsed_seconds": 90
            }
          }
        ]
      },
      "meta": {
        "issue_number": 146,
        "object_id": "interactions:arxiv.2406.09070",
        "created_at": "2025-07-01T17:41:06+00:00",
        "updated_at": "2025-07-01T17:41:34+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.03239": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.03239",
        "url": "https://arxiv.org/abs/2402.03239",
        "title": "Bluesky and the AT Protocol: Usable Decentralized Social Media",
        "authors": "Martin Kleppmann, Paul Frazee, Jake Gold, Jay Graber, Daniel Holmgren, Devin Ivy, Jeromy Johnson, Bryan Newbold, Jaz Volpert",
        "abstract": "Bluesky is a new social network built upon the AT Protocol, a decentralized foundation for public social media. It was launched in private beta in February 2023, and has grown to over 10 million registered users by October 2024. In this paper we introduce the architecture of Bluesky and the AT Protocol, and explain how the technical design of Bluesky is informed by our goals: to enable decentralization by having multiple interoperable providers for every part of the system; to make it easy for users to switch providers; to give users agency over the content they see; and to provide a simple user experience that does not burden users with complexity arising from the system's decentralized nature. The system's openness allows anybody to contribute to content moderation and community management, and we invite the research community to use Bluesky as a dataset and testing ground for new approaches in social media moderation.",
        "timestamp": "2025-07-01T17:50:32.101Z",
        "rating": "novote",
        "publishedDate": "2024/02/05",
        "tags": [
          "Distributed",
          "Parallel",
          "and Cluster Computing (cs.DC)",
          "Social and Information Networks (cs.SI)"
        ],
        "doi": "10.1145/3694809.3700740",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 147,
        "object_id": "paper:arxiv.2402.03239",
        "created_at": "2025-07-01T17:50:32+00:00",
        "updated_at": "2025-07-01T17:50:58+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.14685": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.14685",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:24:14.578Z",
            "data": {
              "session_id": "session_1751567054109_rip3ilw",
              "source_id": "arxiv",
              "paper_id": "2505.14685",
              "start_time": "2025-07-03T18:14:42.649Z",
              "end_time": "2025-07-03T18:24:14.109Z",
              "heartbeat_count": 114,
              "duration_seconds": 570,
              "idle_seconds": 1,
              "total_elapsed_seconds": 571
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:29:25.628Z",
            "data": {
              "session_id": "session_1751567365152_t8qmic4",
              "source_id": "arxiv",
              "paper_id": "2505.14685",
              "start_time": "2025-07-03T18:27:31.236Z",
              "end_time": "2025-07-03T18:29:25.152Z",
              "heartbeat_count": 22,
              "duration_seconds": 110,
              "idle_seconds": 4,
              "total_elapsed_seconds": 114
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:41:57.872Z",
            "data": {
              "session_id": "session_1751568117452_pfu748b",
              "source_id": "arxiv",
              "paper_id": "2505.14685",
              "start_time": "2025-07-03T18:40:53.435Z",
              "end_time": "2025-07-03T18:41:57.452Z",
              "heartbeat_count": 12,
              "duration_seconds": 60,
              "idle_seconds": 4,
              "total_elapsed_seconds": 64
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:43:40.015Z",
            "data": {
              "session_id": "session_1751568220004_957h28n",
              "source_id": "arxiv",
              "paper_id": "2505.14685",
              "start_time": "2025-07-03T18:43:20.478Z",
              "end_time": "2025-07-03T18:43:40.004Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 5,
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "issue_number": 149,
        "object_id": "interactions:arxiv.2505.14685",
        "created_at": "2025-07-02T18:27:01+00:00",
        "updated_at": "2025-07-03T18:44:11+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2204.10628": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2204.10628",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-02T21:45:34.591Z",
            "data": {
              "session_id": "session_1751492734046_o6j6z7r",
              "source_id": "arxiv",
              "paper_id": "2204.10628",
              "start_time": "2025-07-02T21:44:48.461Z",
              "end_time": "2025-07-02T21:45:34.046Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 1,
              "total_elapsed_seconds": 46
            }
          }
        ]
      },
      "meta": {
        "issue_number": 151,
        "object_id": "interactions:arxiv.2204.10628",
        "created_at": "2025-07-02T21:37:41+00:00",
        "updated_at": "2025-07-02T21:45:53+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2204.10628": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2204.10628",
        "url": "https://arxiv.org/pdf/2204.10628",
        "title": "Autoregressive Search Engines: Generating Substrings as Document\n  Identifiers",
        "authors": "Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, Fabio Petroni",
        "abstract": "Knowledge-intensive language tasks require NLP systems to both provide the\ncorrect answer and retrieve supporting evidence for it in a given corpus.\nAutoregressive language models are emerging as the de-facto standard for\ngenerating answers, with newer and more powerful systems emerging at an\nastonishing pace. In this paper we argue that all this (and future) progress\ncan be directly applied to the retrieval problem with minimal intervention to\nthe models' architecture. Previous work has explored ways to partition the\nsearch space into hierarchical structures and retrieve documents by\nautoregressively generating their unique identifier. In this work we propose an\nalternative that doesn't force any structure in the search space: using all\nngrams in a passage as its possible identifiers. This setup allows us to use an\nautoregressive model to generate and score distinctive ngrams, that are then\nmapped to full passages through an efficient data structure. Empirically, we\nshow this not only outperforms prior autoregressive approaches but also leads\nto an average improvement of at least 10 points over more established retrieval\nsolutions for passage-level retrieval on the KILT benchmark, establishing new\nstate-of-the-art downstream performance on some datasets, while using a\nconsiderably lighter memory footprint than competing systems. Code and\npre-trained models at https://github.com/facebookresearch/SEAL.",
        "timestamp": "2025-07-02T21:37:33.933Z",
        "rating": "novote",
        "publishedDate": "2022-04-22T10:45:01Z",
        "tags": [
          "cs.CL",
          "cs.IR"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 150,
        "object_id": "paper:arxiv.2204.10628",
        "created_at": "2025-07-02T21:37:34+00:00",
        "updated_at": "2025-07-02T21:37:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.18777": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.18777",
        "url": "https://arxiv.org/abs/2506.18777",
        "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training",
        "authors": "Jonathan Cook, Silvia Sapora, Arash Ahmadian, Akbir Khan, Tim Rocktaschel, Jakob Foerster, Laura Ruis",
        "abstract": "Training large language models (LLMs) on source code significantly enhances their general-purpose reasoning abilities, but the mechanisms underlying this generalisation are poorly understood. In this paper, we propose Programming by Backprop (PBB) as a potential driver of this effect - teaching a model to evaluate a program for inputs by training on its source code alone, without ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of programs representing simple maths problems and algorithms: one with source code and I/O examples (w/ IO), the other with source code only (w/o IO). We find evidence that LLMs have some ability to evaluate w/o IO programs for inputs in a range of experimental settings, and make several observations. Firstly, PBB works significantly better when programs are provided as code rather than semantically equivalent language descriptions. Secondly, LLMs can produce outputs for w/o IO programs directly, by implicitly evaluating the program within the forward pass, and more reliably when stepping through the program in-context via chain-of-thought. We further show that PBB leads to more robust evaluation of programs across inputs than training on I/O pairs drawn from a distribution that mirrors naturally occurring data. Our findings suggest a mechanism for enhanced reasoning through code training: it allows LLMs to internalise reusable algorithmic abstractions. Significant scope remains for future work to enable LLMs to more effectively learn from symbolic procedures, and progress in this direction opens other avenues like model alignment by training on formal constitutional principles.",
        "timestamp": "2025-07-03T18:46:46.748Z",
        "rating": "novote",
        "publishedDate": "2025/06/23",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 152,
        "object_id": "paper:arxiv.2506.18777",
        "created_at": "2025-07-03T18:46:47+00:00",
        "updated_at": "2025-07-03T18:47:03+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.18777": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.18777",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T18:49:05.331Z",
            "data": {
              "session_id": "session_1751568544752_xx16te1",
              "source_id": "arxiv",
              "paper_id": "2506.18777",
              "start_time": "2025-07-03T18:46:49.814Z",
              "end_time": "2025-07-03T18:49:04.752Z",
              "heartbeat_count": 26,
              "duration_seconds": 130,
              "idle_seconds": 5,
              "total_elapsed_seconds": 135
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T19:59:41.376Z",
            "data": {
              "session_id": "session_1751572781110_kmbr7yt",
              "source_id": "arxiv",
              "paper_id": "2506.18777",
              "start_time": "2025-07-03T19:58:59.388Z",
              "end_time": "2025-07-03T19:59:41.110Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 2,
              "total_elapsed_seconds": 42
            }
          }
        ]
      },
      "meta": {
        "issue_number": 153,
        "object_id": "interactions:arxiv.2506.18777",
        "created_at": "2025-07-03T18:49:05+00:00",
        "updated_at": "2025-07-03T20:00:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.02273": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.02273",
        "url": "https://arxiv.org/pdf/2407.02273#page=28.10",
        "title": "Language Model Alignment in Multilingual Trolley Problems",
        "authors": "Zhijing Jin, Max Kleiman-Weiner, Giorgio Piatti, Sydney Levine, Jiarui Liu, Fernando Gonzalez, Francesco Ortu, Andr\u00e1s Strausz, Mrinmaya Sachan, Rada Mihalcea, Yejin Choi, Bernhard Sch\u00f6lkopf",
        "abstract": "We evaluate the moral alignment of LLMs with human preferences in\nmultilingual trolley problems. Building on the Moral Machine experiment, which\ncaptures over 40 million human judgments across 200+ countries, we develop a\ncross-lingual corpus of moral dilemma vignettes in over 100 languages called\nMultiTP. This dataset enables the assessment of LLMs' decision-making processes\nin diverse linguistic contexts. Our analysis explores the alignment of 19\ndifferent LLMs with human judgments, capturing preferences across six moral\ndimensions: species, gender, fitness, status, age, and the number of lives\ninvolved. By correlating these preferences with the demographic distribution of\nlanguage speakers and examining the consistency of LLM responses to various\nprompt paraphrasings, our findings provide insights into cross-lingual and\nethical biases of LLMs and their intersection. We discover significant variance\nin alignment across languages, challenging the assumption of uniform moral\nreasoning in AI systems and highlighting the importance of incorporating\ndiverse perspectives in AI ethics. The results underscore the need for further\nresearch on the integration of multilingual dimensions in responsible AI\nresearch to ensure fair and equitable AI interactions worldwide. Our code and\ndata are at https://github.com/causalNLP/moralmachine",
        "timestamp": "2025-07-03T19:17:43.652Z",
        "rating": "novote",
        "publishedDate": "2024-07-02T14:02:53Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 154,
        "object_id": "paper:arxiv.2407.02273",
        "created_at": "2025-07-03T19:17:43+00:00",
        "updated_at": "2025-07-03T19:18:03+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.04265": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.04265",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:01:40.549Z",
            "data": {
              "session_id": "session_1751576500303_ppz0pc6",
              "source_id": "arxiv",
              "paper_id": "2410.04265",
              "start_time": "2025-07-03T21:01:03.944Z",
              "end_time": "2025-07-03T21:01:40.303Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 1,
              "total_elapsed_seconds": 36
            }
          }
        ]
      },
      "meta": {
        "issue_number": 155,
        "object_id": "interactions:arxiv.2410.04265",
        "created_at": "2025-07-03T21:01:41+00:00",
        "updated_at": "2025-07-03T21:02:05+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.13775": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.13775",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:05:32.473Z",
            "data": {
              "session_id": "session_1751576732080_h2yt7go",
              "source_id": "arxiv",
              "paper_id": "2505.13775",
              "start_time": "2025-07-03T21:01:50.649Z",
              "end_time": "2025-07-03T21:05:32.080Z",
              "heartbeat_count": 44,
              "duration_seconds": 220,
              "idle_seconds": 1,
              "total_elapsed_seconds": 221
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:09:45.415Z",
            "data": {
              "session_id": "session_1751576985397_r0nkqjf",
              "source_id": "arxiv",
              "paper_id": "2505.13775",
              "start_time": "2025-07-03T21:09:40.012Z",
              "end_time": "2025-07-03T21:09:45.397Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 156,
        "object_id": "interactions:arxiv.2505.13775",
        "created_at": "2025-07-03T21:05:32+00:00",
        "updated_at": "2025-07-03T21:09:51+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.02867": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.02867",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:18:44.560Z",
            "data": {
              "session_id": "session_1751577524118_0klevah",
              "source_id": "arxiv",
              "paper_id": "2506.02867",
              "start_time": "2025-07-03T21:16:21.804Z",
              "end_time": "2025-07-03T21:18:44.118Z",
              "heartbeat_count": 28,
              "duration_seconds": 140,
              "idle_seconds": 2,
              "total_elapsed_seconds": 142
            }
          }
        ]
      },
      "meta": {
        "issue_number": 158,
        "object_id": "interactions:arxiv.2506.02867",
        "created_at": "2025-07-03T21:16:22+00:00",
        "updated_at": "2025-07-03T21:19:04+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2312.01552": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2312.01552",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:15:20.310Z",
            "data": {
              "session_id": "session_1751577319914_tn13kta",
              "source_id": "arxiv",
              "paper_id": "2312.01552",
              "start_time": "2025-07-03T21:10:14.183Z",
              "end_time": "2025-07-03T21:15:19.914Z",
              "heartbeat_count": 61,
              "duration_seconds": 305,
              "idle_seconds": 1,
              "total_elapsed_seconds": 306
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-03T21:16:00.442Z",
            "data": {
              "session_id": "session_1751577360431_th8v8ay",
              "source_id": "arxiv",
              "paper_id": "2312.01552",
              "start_time": "2025-07-03T21:15:54.049Z",
              "end_time": "2025-07-03T21:16:00.431Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 157,
        "object_id": "interactions:arxiv.2312.01552",
        "created_at": "2025-07-03T21:15:21+00:00",
        "updated_at": "2025-07-03T21:16:19+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.13765": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.13765",
        "url": "https://arxiv.org/pdf/2407.13765",
        "title": "Latent Causal Probing: A Formal Perspective on Probing with Causal\n  Models of Data",
        "authors": "Charles Jin, Martin Rinard",
        "abstract": "As language models (LMs) deliver increasing performance on a range of NLP\ntasks, probing classifiers have become an indispensable technique in the effort\nto better understand their inner workings. A typical setup involves (1)\ndefining an auxiliary task consisting of a dataset of text annotated with\nlabels, then (2) supervising small classifiers to predict the labels from the\nrepresentations of a pretrained LM as it processed the dataset. A high probing\naccuracy is interpreted as evidence that the LM has learned to perform the\nauxiliary task as an unsupervised byproduct of its original pretraining\nobjective. Despite the widespread usage of probes, however, the robust design\nand analysis of probing experiments remains a challenge. We develop a formal\nperspective on probing using structural causal models (SCM). Specifically,\ngiven an SCM which explains the distribution of tokens observed during\ntraining, we frame the central hypothesis as whether the LM has learned to\nrepresent the latent variables of the SCM. Empirically, we extend a recent\nstudy of LMs in the context of a synthetic grid-world navigation task, where\nhaving an exact model of the underlying causal structure allows us to draw\nstrong inferences from the result of probing experiments. Our techniques\nprovide robust empirical evidence for the ability of LMs to induce the latent\nconcepts underlying text.",
        "timestamp": "2025-07-06T18:48:36.277Z",
        "rating": "novote",
        "publishedDate": "2024-07-18T17:59:27Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 166,
        "object_id": "paper:arxiv.2407.13765",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:49:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.19200": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.19200",
        "url": "https://arxiv.org/pdf/2407.19200",
        "title": "On Behalf of the Stakeholders: Trends in NLP Model Interpretability in\n  the Era of LLMs",
        "authors": "Nitay Calderon, Roi Reichart",
        "abstract": "Recent advancements in NLP systems, particularly with the introduction of\nLLMs, have led to widespread adoption of these systems by a broad spectrum of\nusers across various domains, impacting decision-making, the job market,\nsociety, and scientific research. This surge in usage has led to an explosion\nin NLP model interpretability and analysis research, accompanied by numerous\ntechnical surveys. Yet, these surveys often overlook the needs and perspectives\nof explanation stakeholders. In this paper, we address three fundamental\nquestions: Why do we need interpretability, what are we interpreting, and how?\nBy exploring these questions, we examine existing interpretability paradigms,\ntheir properties, and their relevance to different stakeholders. We further\nexplore the practical implications of these paradigms by analyzing trends from\nthe past decade across multiple research fields. To this end, we retrieved\nthousands of papers and employed an LLM to characterize them. Our analysis\nreveals significant disparities between NLP developers and non-developer users,\nas well as between research fields, underscoring the diverse needs of\nstakeholders. For example, explanations of internal model components are rarely\nused outside the NLP field. We hope this paper informs the future design,\ndevelopment, and application of methods that align with the objectives and\nrequirements of various stakeholders.",
        "timestamp": "2025-07-06T18:48:36.240Z",
        "rating": "novote",
        "publishedDate": "2024-07-27T08:00:27Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 165,
        "object_id": "paper:arxiv.2407.19200",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:48:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.10827": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.10827",
        "url": "https://arxiv.org/pdf/2407.10827",
        "title": "LLM Circuit Analyses Are Consistent Across Training and Scale",
        "authors": "Curt Tigges, Michael Hanna, Qinan Yu, Stella Biderman",
        "abstract": "Most currently deployed large language models (LLMs) undergo continuous\ntraining or additional finetuning. By contrast, most research into LLMs'\ninternal mechanisms focuses on models at one snapshot in time (the end of\npre-training), raising the question of whether their results generalize to\nreal-world settings. Existing studies of mechanisms over time focus on\nencoder-only or toy models, which differ significantly from most deployed\nmodels. In this study, we track how model mechanisms, operationalized as\ncircuits, emerge and evolve across 300 billion tokens of training in\ndecoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.\nWe find that task abilities and the functional components that support them\nemerge consistently at similar token counts across scale. Moreover, although\nsuch components may be implemented by different attention heads over time, the\noverarching algorithm that they implement remains. Surprisingly, both these\nalgorithms and the types of components involved therein can replicate across\nmodel scale. These results suggest that circuit analyses conducted on small\nmodels at the end of pre-training can provide insights that still apply after\nadditional pre-training and over model scale.",
        "timestamp": "2025-07-06T18:48:36.240Z",
        "rating": "novote",
        "publishedDate": "2024-07-15T15:38:51Z",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 164,
        "object_id": "paper:arxiv.2407.10827",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:48:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.15510": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.15510",
        "url": "https://arxiv.org/pdf/2408.15510",
        "title": "How Reliable are Causal Probing Interventions?",
        "authors": "Marc Canby, Adam Davies, Chirag Rastogi, Julia Hockenmaier",
        "abstract": "Causal probing aims to analyze foundation models by examining how intervening\non their representation of various latent properties impacts their outputs.\nRecent works have cast doubt on the theoretical basis of several leading causal\nprobing methods, but it has been unclear how to systematically evaluate the\neffectiveness of these methods in practice. To address this, we define two key\ncausal probing desiderata: completeness (how thoroughly the representation of\nthe target property has been transformed) and selectivity (how little\nnon-targeted properties have been impacted). We find that there is an inherent\ntradeoff between the two, which we define as reliability, their harmonic mean.\nWe introduce an empirical analysis framework to measure and evaluate these\nquantities, allowing us to make the first direct comparisons between different\nfamilies of leading causal probing methods (e.g., linear vs. nonlinear, or\nconcept removal vs. counterfactual interventions). We find that: (1) no method\nis reliable across all layers; (2) more reliable methods have a greater impact\non LLM behavior; (3) nonlinear interventions are more reliable in early and\nintermediate layers, and linear interventions are more reliable in later\nlayers; and (4) concept removal methods are far less reliable than\ncounterfactual interventions, suggesting that they may not be an effective\napproach to causal probing.",
        "timestamp": "2025-07-06T18:48:36.239Z",
        "rating": "novote",
        "publishedDate": "2024-08-28T03:45:49Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 163,
        "object_id": "paper:arxiv.2408.15510",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:48:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.01687": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.01687",
        "url": "https://arxiv.org/pdf/2407.01687",
        "title": "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought:\n  Probability, Memorization, and Noisy Reasoning",
        "authors": "Akshara Prabhakar, Thomas L. Griffiths, R. Thomas McCoy",
        "abstract": "Chain-of-Thought (CoT) prompting has been shown to enhance the multi-step\nreasoning capabilities of Large Language Models (LLMs). However, debates\npersist about whether LLMs exhibit abstract generalization or rely on shallow\nheuristics when given CoT prompts. To understand the factors influencing CoT\nreasoning we provide a detailed case study of the symbolic reasoning task of\ndecoding shift ciphers, where letters are shifted forward some number of steps\nin the alphabet. We analyze the pattern of results produced by three LLMs --\nGPT-4, Claude 3, and Llama 3.1 -- performing this task using CoT prompting. By\nfocusing on a single relatively simple task, we are able to identify three\nfactors that systematically affect CoT performance: the probability of the\ntask's expected output (probability), what the model has implicitly learned\nduring pre-training (memorization), and the number of intermediate operations\ninvolved in reasoning (noisy reasoning). We show that these factors can\ndrastically influence task accuracy across all three LLMs; e.g., when tested\nwith GPT-4, varying the output's probability of occurrence shifts accuracy from\n26% to 70%. Overall, we conclude that CoT prompting performance reflects both\nmemorization and a probabilistic version of genuine reasoning. Code and data at\nthis https://github.com/aksh555/deciphering_cot",
        "timestamp": "2025-07-06T18:48:36.277Z",
        "rating": "novote",
        "publishedDate": "2024-07-01T18:01:07Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 162,
        "object_id": "paper:arxiv.2407.01687",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:48:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.04690": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.04690",
        "url": "https://arxiv.org/pdf/2407.04690",
        "title": "Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for\n  Interpreting Neural Networks",
        "authors": "Aaron Mueller",
        "abstract": "Interpretability research takes counterfactual theories of causality for\ngranted. Most causal methods rely on counterfactual interventions to inputs or\nthe activations of particular model components, followed by observations of the\nchange in models' output logits or behaviors. While this yields more faithful\nevidence than correlational methods, counterfactuals nonetheless have key\nproblems that bias our findings in specific and predictable ways. Specifically,\n(i) counterfactual theories do not effectively capture multiple independently\nsufficient causes of the same effect, which leads us to miss certain causes\nentirely; and (ii) counterfactual dependencies in neural networks are generally\nnot transitive, which complicates methods for extracting and interpreting\ncausal graphs from neural networks. We discuss the implications of these\nchallenges for interpretability researchers and propose concrete suggestions\nfor future work.",
        "timestamp": "2025-07-06T18:48:36.240Z",
        "rating": "novote",
        "publishedDate": "2024-07-05T17:53:03Z",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 161,
        "object_id": "paper:arxiv.2407.04690",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:48:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.16997": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.16997",
        "url": "https://arxiv.org/pdf/2407.16997",
        "title": "Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal\n  Intervention Perspective",
        "authors": "Yujian Liu, Yang Zhang, Tommi Jaakkola, Shiyu Chang",
        "abstract": "This paper investigates Who's Harry Potter (WHP), a pioneering yet\ninsufficiently understood method for LLM unlearning. We explore it in two\nsteps. First, we introduce a new task of LLM targeted unlearning, where given\nan unlearning target (e.g., a person) and some unlearning documents, we aim to\nunlearn only the information about the target, rather than everything in the\nunlearning documents. We further argue that a successful unlearning should\nsatisfy criteria such as not outputting gibberish, not fabricating facts about\nthe unlearning target, and not releasing factual information under jailbreak\nattacks. Second, we construct a causal intervention framework for targeted\nunlearning, where the knowledge of the unlearning target is modeled as a\nconfounder between LLM input and output, and the unlearning process as a\ndeconfounding process. This framework justifies and extends WHP, deriving a\nsimple unlearning algorithm that includes WHP as a special case. Experiments on\nexisting and new datasets show that our approach, without explicitly optimizing\nfor the aforementioned criteria, achieves competitive performance in all of\nthem. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/causal_unlearn.git.",
        "timestamp": "2025-07-06T18:48:36.239Z",
        "rating": "novote",
        "publishedDate": "2024-07-24T04:39:24Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 160,
        "object_id": "paper:arxiv.2407.16997",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:49:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.15017": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.15017",
        "url": "https://arxiv.org/pdf/2407.15017",
        "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
        "authors": "Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang",
        "abstract": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research.",
        "timestamp": "2025-07-06T18:48:36.240Z",
        "rating": "novote",
        "publishedDate": "2024-07-22T06:15:59Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CV",
          "cs.HC",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 159,
        "object_id": "paper:arxiv.2407.15017",
        "created_at": "2025-07-06T18:48:36+00:00",
        "updated_at": "2025-07-06T18:48:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2110.11334": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2110.11334",
        "url": "https://arxiv.org/abs/2110.11334",
        "title": "Generalized Out-of-Distribution Detection: A Survey",
        "authors": "Jingkang Yang, Kaiyang Zhou, Yixuan Li, Ziwei Liu",
        "abstract": "Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this survey, we first present a unified framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. We then review each of these five areas by summarizing their recent technical developments, with a special focus on OOD detection methodologies. We conclude this survey with open challenges and potential research directions.",
        "timestamp": "2025-07-08T16:44:15.579Z",
        "rating": "novote",
        "publishedDate": "2021/10/21",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 167,
        "object_id": "paper:arxiv.2110.11334",
        "created_at": "2025-07-08T16:44:15+00:00",
        "updated_at": "2025-07-08T16:44:39+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2108.13624": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2108.13624",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T16:50:57.491Z",
            "data": {
              "session_id": "session_1751993457102_oagru5s",
              "source_id": "arxiv",
              "paper_id": "2108.13624",
              "start_time": "2025-07-08T16:50:48.947Z",
              "end_time": "2025-07-08T16:50:57.102Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T16:52:45.797Z",
            "data": {
              "session_id": "session_1751993565279_k4j1iss",
              "source_id": "arxiv",
              "paper_id": "2108.13624",
              "start_time": "2025-07-08T16:51:04.647Z",
              "end_time": "2025-07-08T16:52:45.279Z",
              "heartbeat_count": 20,
              "duration_seconds": 100,
              "idle_seconds": 1,
              "total_elapsed_seconds": 101
            }
          }
        ]
      },
      "meta": {
        "issue_number": 171,
        "object_id": "interactions:arxiv.2108.13624",
        "created_at": "2025-07-08T16:50:57+00:00",
        "updated_at": "2025-07-08T16:53:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2108.13624": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2108.13624",
        "url": "https://arxiv.org/pdf/2108.13624",
        "title": "Towards Out-Of-Distribution Generalization: A Survey",
        "authors": "Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, Peng Cui",
        "abstract": "Traditional machine learning paradigms are based on the assumption that both\ntraining and test data follow the same statistical pattern, which is\nmathematically referred to as Independent and Identically Distributed\n($i.i.d.$). However, in real-world applications, this $i.i.d.$ assumption often\nfails to hold due to unforeseen distributional shifts, leading to considerable\ndegradation in model performance upon deployment. This observed discrepancy\nindicates the significance of investigating the Out-of-Distribution (OOD)\ngeneralization problem. OOD generalization is an emerging topic of machine\nlearning research that focuses on complex scenarios wherein the distributions\nof the test data differ from those of the training data. This paper represents\nthe first comprehensive, systematic review of OOD generalization, encompassing\na spectrum of aspects from problem definition, methodological development, and\nevaluation procedures, to the implications and future directions of the field.\nOur discussion begins with a precise, formal characterization of the OOD\ngeneralization problem. Following that, we categorize existing methodologies\ninto three segments: unsupervised representation learning, supervised model\nlearning, and optimization, according to their positions within the overarching\nlearning process. We provide an in-depth discussion on representative\nmethodologies for each category, further elucidating the theoretical links\nbetween them. Subsequently, we outline the prevailing benchmark datasets\nemployed in OOD generalization studies. To conclude, we overview the existing\nbody of work in this domain and suggest potential avenues for future research\non OOD generalization. A summary of the OOD generalization methodologies\nsurveyed in this paper can be accessed at\nhttp://out-of-distribution-generalization.com.",
        "timestamp": "2025-07-08T16:50:49.165Z",
        "rating": "novote",
        "publishedDate": "2021-08-31T05:28:42Z",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 170,
        "object_id": "paper:arxiv.2108.13624",
        "created_at": "2025-07-08T16:50:49+00:00",
        "updated_at": "2025-07-08T16:51:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2308.00755": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2308.00755",
        "url": "https://arxiv.org/abs/2308.00755",
        "title": "The Bias Amplification Paradox in Text-to-Image Generation",
        "authors": "Preethi Seshadri, Sameer Singh, Yanai Elazar",
        "abstract": "Bias amplification is a phenomenon in which models exacerbate biases or stereotypes present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION) considerably. However, we discover that amplification can be largely attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while our prompts do not, which leads to a distribution shift and consequently inflates bias measures. Once we account for distributional differences between texts used for training and generation when evaluating amplification, we observe that amplification decreases drastically. Our findings illustrate the challenges of comparing biases in models and their training data, and highlight confounding factors that impact analyses.",
        "timestamp": "2025-07-08T18:16:54.469Z",
        "rating": "novote",
        "publishedDate": "2023/08/01",
        "tags": [
          "Machine Learning (cs.LG)",
          "Computation and Language (cs.CL)",
          "Computer Vision and Pattern Recognition (cs.CV)",
          "Computers and Society (cs.CY)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 175,
        "object_id": "paper:arxiv.2308.00755",
        "created_at": "2025-07-08T18:16:54+00:00",
        "updated_at": "2025-07-08T18:17:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2407.00211": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.00211",
        "url": "https://arxiv.org/abs/2407.00211",
        "title": "Detection and Measurement of Syntactic Templates in Generated Text",
        "authors": "Chantal Shaib, Yanai Elazar, Junyi Jessy Li, Byron C. Wallace",
        "abstract": "Recent work on evaluating the diversity of text generated by LLMs has focused on word-level features. Here we offer an analysis of syntactic features to characterize general repetition in models, beyond frequent n-grams. Specifically, we define syntactic templates and show that models tend to produce templated text in downstream tasks at a higher rate than what is found in human-reference texts. We find that most (76%) templates in model-generated text can be found in pre-training data (compared to only 35% of human-authored text), and are not overwritten during fine-tuning processes such as RLHF. This connection to the pre-training data allows us to analyze syntactic templates in models where we do not have the pre-training data. We also find that templates as features are able to differentiate between models, tasks, and domains, and are useful for qualitatively evaluating common model constructions. Finally, we demonstrate the use of templates as a useful tool for analyzing style memorization of training data in LLMs.",
        "timestamp": "2025-07-08T18:16:34.475Z",
        "rating": "novote",
        "publishedDate": "2024/06/28",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 174,
        "object_id": "paper:arxiv.2407.00211",
        "created_at": "2025-07-08T18:16:34+00:00",
        "updated_at": "2025-07-08T18:16:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.15002": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.15002",
        "url": "https://arxiv.org/abs/2410.15002",
        "title": "How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold",
        "authors": "Sahil Verma, Royi Rassin, Arnav Das, Gantavya Bhatt, Preethi Seshadri, Chirag Shah, Jeff Bilmes, Hannaneh Hajishirzi, Yanai Elazar",
        "abstract": "Text-to-image models are trained using large datasets collected by scraping image-text pairs from the internet. These datasets often include private, copyrighted, and licensed material. Training models on such datasets enables them to generate images with such content, which might violate copyright laws and individual privacy. This phenomenon is termed imitation -- generation of images with content that has recognizable similarity to its training images. In this work we study the relationship between a concept's frequency in the training dataset and the ability of a model to imitate it. We seek to determine the point at which a model was trained on enough instances to imitate a concept -- the imitation threshold. We posit this question as a new problem: Finding the Imitation Threshold (FIT) and propose an efficient approach that estimates the imitation threshold without incurring the colossal cost of training multiple models from scratch. We experiment with two domains -- human faces and art styles -- for which we create four datasets, and evaluate three text-to-image models which were trained on two pretraining datasets. Our results reveal that the imitation threshold of these models is in the range of 200-600 images, depending on the domain and the model. The imitation threshold can provide an empirical basis for copyright violation claims and acts as a guiding principle for text-to-image model developers that aim to comply with copyright and privacy laws. We release the code and data at \\url{this https URL} and the project's website is hosted at \\url{this https URL}.",
        "timestamp": "2025-07-08T18:16:21.735Z",
        "rating": "novote",
        "publishedDate": "2024/10/19",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 172,
        "object_id": "paper:arxiv.2410.15002",
        "created_at": "2025-07-08T18:16:22+00:00",
        "updated_at": "2025-07-08T18:16:40+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2407.14985": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.14985",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:33:06.844Z",
            "data": {
              "session_id": "session_1751999586838_5a8grh7",
              "source_id": "arxiv",
              "paper_id": "2407.14985",
              "start_time": "2025-07-08T18:32:35.228Z",
              "end_time": "2025-07-08T18:33:06.838Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:39:48.946Z",
            "data": {
              "session_id": "session_1751999988491_r8wehx7",
              "source_id": "arxiv",
              "paper_id": "2407.14985",
              "start_time": "2025-07-08T18:33:16.036Z",
              "end_time": "2025-07-08T18:39:48.491Z",
              "heartbeat_count": 78,
              "duration_seconds": 390,
              "idle_seconds": 2,
              "total_elapsed_seconds": 392
            }
          }
        ]
      },
      "meta": {
        "issue_number": 180,
        "object_id": "interactions:arxiv.2407.14985",
        "created_at": "2025-07-08T18:21:30+00:00",
        "updated_at": "2025-07-08T18:40:15+00:00",
        "version": 1
      }
    },
    "paper:openreview.EDoD3DgivF": {
      "data": {
        "sourceId": "openreview",
        "paperId": "EDoD3DgivF",
        "url": "https://openreview.net/pdf?id=EDoD3DgivF",
        "title": "EDoD3DgivF",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-07-08T18:20:47.575Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 179,
        "object_id": "paper:openreview.EDoD3DgivF",
        "created_at": "2025-07-08T18:20:47+00:00",
        "updated_at": "2025-07-08T18:21:10+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2308.00755": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2308.00755",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:23:18.907Z",
            "data": {
              "session_id": "session_1751998998492_9aa31mx",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-07-08T18:22:32.588Z",
              "end_time": "2025-07-08T18:23:18.492Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 1,
              "total_elapsed_seconds": 46
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:30:10.972Z",
            "data": {
              "session_id": "session_1751999410527_upyri17",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-07-08T18:27:44.761Z",
              "end_time": "2025-07-08T18:30:10.527Z",
              "heartbeat_count": 29,
              "duration_seconds": 145,
              "idle_seconds": 1,
              "total_elapsed_seconds": 146
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T18:52:38.262Z",
            "data": {
              "session_id": "session_1752087157906_gbz74f7",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-07-09T18:51:27.241Z",
              "end_time": "2025-07-09T18:52:37.906Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 1,
              "total_elapsed_seconds": 71
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T18:54:08.834Z",
            "data": {
              "session_id": "session_1752087248416_vapqamc",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-07-09T18:52:40.502Z",
              "end_time": "2025-07-09T18:54:08.416Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 3,
              "total_elapsed_seconds": 88
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T19:00:08.760Z",
            "data": {
              "session_id": "session_1752087608342_qhu55iy",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-07-09T18:58:39.290Z",
              "end_time": "2025-07-09T19:00:08.342Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 4,
              "total_elapsed_seconds": 89
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T19:46:49.346Z",
            "data": {
              "session_id": "session_1752090409142_joelq51",
              "source_id": "arxiv",
              "paper_id": "2308.00755",
              "start_time": "2025-07-09T19:46:34.115Z",
              "end_time": "2025-07-09T19:46:49.141Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 0,
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "issue_number": 178,
        "object_id": "interactions:arxiv.2308.00755",
        "created_at": "2025-07-08T18:20:25+00:00",
        "updated_at": "2025-07-09T19:47:16+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2407.00211": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2407.00211",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:20:19.077Z",
            "data": {
              "session_id": "session_1751998818716_lhn5cid",
              "source_id": "arxiv",
              "paper_id": "2407.00211",
              "start_time": "2025-07-08T18:19:03.947Z",
              "end_time": "2025-07-08T18:20:18.716Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 5,
              "total_elapsed_seconds": 75
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:32:33.807Z",
            "data": {
              "session_id": "session_1751999553784_lf3r1aa",
              "source_id": "arxiv",
              "paper_id": "2407.00211",
              "start_time": "2025-07-08T18:31:42.413Z",
              "end_time": "2025-07-08T18:32:33.784Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 1,
              "total_elapsed_seconds": 51
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T16:35:44.776Z",
            "data": {
              "session_id": "session_1752078944299_mxk0yrb",
              "source_id": "arxiv",
              "paper_id": "2407.00211",
              "start_time": "2025-07-09T16:34:19.367Z",
              "end_time": "2025-07-09T16:35:44.299Z",
              "heartbeat_count": 16,
              "duration_seconds": 80,
              "idle_seconds": 5,
              "total_elapsed_seconds": 85
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T18:58:38.579Z",
            "data": {
              "session_id": "session_1752087518558_01p6rzq",
              "source_id": "arxiv",
              "paper_id": "2407.00211",
              "start_time": "2025-07-09T18:58:06.184Z",
              "end_time": "2025-07-09T18:58:38.558Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "issue_number": 177,
        "object_id": "interactions:arxiv.2407.00211",
        "created_at": "2025-07-08T18:18:39+00:00",
        "updated_at": "2025-07-09T18:59:02+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2410.15002": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.15002",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:20:38.011Z",
            "data": {
              "session_id": "session_1751998837994_2cek6pb",
              "source_id": "arxiv",
              "paper_id": "2410.15002",
              "start_time": "2025-07-08T18:20:24.531Z",
              "end_time": "2025-07-08T18:20:37.994Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T18:22:14.051Z",
            "data": {
              "session_id": "session_1751998934037_iqon5fm",
              "source_id": "arxiv",
              "paper_id": "2410.15002",
              "start_time": "2025-07-08T18:21:31.747Z",
              "end_time": "2025-07-08T18:22:14.037Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 2,
              "total_elapsed_seconds": 42
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T16:37:04.661Z",
            "data": {
              "session_id": "session_1752079024623_pv080w4",
              "source_id": "arxiv",
              "paper_id": "2410.15002",
              "start_time": "2025-07-09T16:36:50.912Z",
              "end_time": "2025-07-09T16:37:04.623Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T18:49:58.325Z",
            "data": {
              "session_id": "session_1752086998311_ptn0997",
              "source_id": "arxiv",
              "paper_id": "2410.15002",
              "start_time": "2025-07-09T18:49:37.037Z",
              "end_time": "2025-07-09T18:49:58.311Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 1,
              "total_elapsed_seconds": 21
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T18:55:32.273Z",
            "data": {
              "session_id": "session_1752087332265_rvbsg4b",
              "source_id": "arxiv",
              "paper_id": "2410.15002",
              "start_time": "2025-07-09T18:55:26.278Z",
              "end_time": "2025-07-09T18:55:32.265Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T19:47:46.099Z",
            "data": {
              "session_id": "session_1752090466086_da0ab2e",
              "source_id": "arxiv",
              "paper_id": "2410.15002",
              "start_time": "2025-07-09T19:47:18.155Z",
              "end_time": "2025-07-09T19:47:46.086Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          }
        ]
      },
      "meta": {
        "issue_number": 176,
        "object_id": "interactions:arxiv.2410.15002",
        "created_at": "2025-07-08T18:18:11+00:00",
        "updated_at": "2025-07-09T19:48:07+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.21828": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.21828",
        "url": "https://arxiv.org/html/2505.21828v1",
        "title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety\n  Facts",
        "authors": "Chen Yueh-Han, Guy Davidson, Brenden M. Lake",
        "abstract": "Do LLMs robustly generalize critical safety facts to novel situations?\nLacking this ability is dangerous when users ask naive questions. For instance,\n\"I'm considering packing melon balls for my 10-month-old's lunch. What other\nfoods would be good to include?\" Before offering food options, the LLM should\nwarn that melon balls pose a choking hazard to toddlers, as documented by the\nCDC. Failing to provide such warnings could result in serious injuries or even\ndeath. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic\nGEneralization evaluation, the first benchmark that tests whether LLMs properly\napply well established safety facts to naive user queries. SAGE-Eval comprises\n104 facts manually sourced from reputable organizations, systematically\naugmented to create 10,428 test scenarios across 7 common domains (e.g.,\nOutdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,\npasses only 58% of all the safety facts tested. We also observe that model\ncapabilities and training compute weakly correlate with performance on\nSAGE-Eval, implying that scaling up is not the golden solution. Our findings\nsuggest frontier LLMs still lack robust generalization ability. We recommend\ndevelopers use SAGE-Eval in pre-deployment evaluations to assess model\nreliability in addressing salient risks. We publicly release SAGE-Eval at\nhttps://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available\nat https://github.com/YuehHanChen/SAGE-Eval/tree/main.",
        "timestamp": "2025-07-08T20:02:26.847Z",
        "rating": "novote",
        "publishedDate": "2025-05-27T23:29:32Z",
        "tags": [
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 181,
        "object_id": "paper:arxiv.2505.21828",
        "created_at": "2025-07-08T20:02:27+00:00",
        "updated_at": "2025-07-08T20:02:47+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.02126": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.02126",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T20:11:28.273Z",
            "data": {
              "session_id": "session_1752005488264_75ql3wv",
              "source_id": "arxiv",
              "paper_id": "2506.02126",
              "start_time": "2025-07-08T20:11:11.086Z",
              "end_time": "2025-07-08T20:11:28.264Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-08T20:21:55.169Z",
            "data": {
              "session_id": "session_1752006114630_4zlrxep",
              "source_id": "arxiv",
              "paper_id": "2506.02126",
              "start_time": "2025-07-08T20:12:21.323Z",
              "end_time": "2025-07-08T20:21:54.630Z",
              "heartbeat_count": 114,
              "duration_seconds": 570,
              "idle_seconds": 3,
              "total_elapsed_seconds": 573
            }
          }
        ]
      },
      "meta": {
        "issue_number": 183,
        "object_id": "interactions:arxiv.2506.02126",
        "created_at": "2025-07-08T20:11:02+00:00",
        "updated_at": "2025-07-08T20:22:17+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.21828": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.21828",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T17:53:03.650Z",
            "data": {
              "session_id": "session_1752083583062_en16ww9",
              "source_id": "arxiv",
              "paper_id": "2505.21828",
              "start_time": "2025-07-09T17:50:45.176Z",
              "end_time": "2025-07-09T17:53:03.062Z",
              "heartbeat_count": 27,
              "duration_seconds": 135,
              "idle_seconds": 3,
              "total_elapsed_seconds": 138
            }
          }
        ]
      },
      "meta": {
        "issue_number": 182,
        "object_id": "interactions:arxiv.2505.21828",
        "created_at": "2025-07-08T20:03:09+00:00",
        "updated_at": "2025-07-09T17:53:20+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.17241": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.17241",
        "interactions": []
      },
      "meta": {
        "issue_number": 185,
        "object_id": "interactions:arxiv.2406.17241",
        "created_at": "2025-07-09T00:40:21+00:00",
        "updated_at": "2025-07-09T00:40:22+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.17241": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.17241",
        "url": "https://arxiv.org/abs/2406.17241",
        "title": "Understanding Language Model Circuits through Knowledge Editing",
        "authors": "Huaizhi Ge, Frank Rudzicz, Zining Zhu",
        "abstract": "Recent advances in language model interpretability have identified circuits, critical subnetworks that replicate model behaviors, yet how knowledge is structured within these crucial subnetworks remains opaque. To gain an understanding toward the knowledge in the circuits, we conduct systematic knowledge editing experiments on the circuits of the GPT-2 language model. Our analysis reveals intriguing patterns in how circuits respond to editing attempts, the extent of knowledge distribution across network components, and the architectural composition of knowledge-bearing circuits. These findings offer insights into the complex relationship between model circuits and knowledge representation, deepening the understanding of how information is organized within language models. Our findings offer novel insights into the ``meanings'' of the circuits, and introduce directions for further interpretability and safety research of language models.",
        "timestamp": "2025-07-09T00:40:06.639Z",
        "rating": "novote",
        "publishedDate": "2024/06/25",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 184,
        "object_id": "paper:arxiv.2406.17241",
        "created_at": "2025-07-09T00:40:07+00:00",
        "updated_at": "2025-07-09T00:40:28+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.15075": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.15075",
        "interactions": []
      },
      "meta": {
        "issue_number": 199,
        "object_id": "interactions:arxiv.2505.15075",
        "created_at": "2025-07-09T00:42:00+00:00",
        "updated_at": "2025-07-09T00:42:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.03074": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.03074",
        "url": "https://arxiv.org/html/2506.03074v1",
        "title": "GL-LowPopArt: A Nearly Instance-Wise Minimax-Optimal Estimator for\n  Generalized Low-Rank Trace Regression",
        "authors": "Junghyun Lee, Kyoungseok Jang, Kwang-Sung Jun, Milan Vojnovi\u0107, Se-Young Yun",
        "abstract": "We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized\nlow-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it\nemploys a two-stage approach: nuclear norm regularization followed by matrix\nCatoni estimation. We establish state-of-the-art estimation error bounds,\nsurpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and\nreveal a novel experimental design objective, $\\mathrm{GL}(\\pi)$. The key\ntechnical challenge is controlling bias from the nonlinear inverse link\nfunction, which we address by our two-stage approach. We prove a *local*\nminimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise\noptimality up to the condition number of the ground-truth Hessian. Applications\ninclude generalized linear matrix completion, where `GL-LowPopArt` achieves a\nstate-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a\nnovel setting inspired by general preference learning (Zhang et al., 2024). Our\nanalysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,\npotentially interesting problem-dependent quantity, along with improved Borda\nregret bound than vectorization (Wu et al., 2024).",
        "timestamp": "2025-07-09T00:41:59.895Z",
        "rating": "novote",
        "publishedDate": "2025-06-03T16:52:24Z",
        "tags": [
          "stat.ML",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 198,
        "object_id": "paper:arxiv.2506.03074",
        "created_at": "2025-07-09T00:42:00+00:00",
        "updated_at": "2025-07-09T00:42:24+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.03708": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.03708",
        "url": "https://arxiv.org/html/2502.03708v1",
        "title": "Toward universal steering and monitoring of AI models",
        "authors": "Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adser\u00e0, Mikhail Belkin",
        "abstract": "Modern AI models contain much of human knowledge, yet understanding of their\ninternal representation of this knowledge remains elusive. Characterizing the\nstructure and properties of this representation will lead to improvements in\nmodel capabilities and development of effective safeguards. Building on recent\nadvances in feature learning, we develop an effective, scalable approach for\nextracting linear representations of general concepts in large-scale AI models\n(language models, vision-language models, and reasoning models). We show how\nthese representations enable model steering, through which we expose\nvulnerabilities, mitigate misaligned behaviors, and improve model capabilities.\nAdditionally, we demonstrate that concept representations are remarkably\ntransferable across human languages and combinable to enable multi-concept\nsteering. Through quantitative analysis across hundreds of concepts, we find\nthat newer, larger models are more steerable and steering can improve model\ncapabilities beyond standard prompting. We show how concept representations are\neffective for monitoring misaligned content (hallucinations, toxic content). We\ndemonstrate that predictive models built using concept representations are more\naccurate for monitoring misaligned content than using models that judge outputs\ndirectly. Together, our results illustrate the power of using internal\nrepresentations to map the knowledge in AI models, advance AI safety, and\nimprove model capabilities.",
        "timestamp": "2025-07-09T00:41:53.374Z",
        "rating": "novote",
        "publishedDate": "2025-02-06T01:41:48Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 197,
        "object_id": "paper:arxiv.2502.03708",
        "created_at": "2025-07-09T00:41:53+00:00",
        "updated_at": "2025-07-09T00:42:14+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.00418": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.00418",
        "interactions": []
      },
      "meta": {
        "issue_number": 196,
        "object_id": "interactions:arxiv.2506.00418",
        "created_at": "2025-07-09T00:41:52+00:00",
        "updated_at": "2025-07-09T00:41:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.08142": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.08142",
        "url": "https://arxiv.org/html/2403.08142v2",
        "title": "FieldNet: Efficient Real-Time Shadow Removal for Enhanced Vision in\n  Field Robotics",
        "authors": "Alzayat Saleh, Alex Olsen, Jake Wood, Bronson Philippa, Mostafa Rahimi Azghadi",
        "abstract": "Shadows significantly hinder computer vision tasks in outdoor environments,\nparticularly in field robotics, where varying lighting conditions complicate\nobject detection and localisation. We present FieldNet, a novel deep learning\nframework for real-time shadow removal, optimised for resource-constrained\nhardware. FieldNet introduces a probabilistic enhancement module and a novel\nloss function to address challenges of inconsistent shadow boundary supervision\nand artefact generation, achieving enhanced accuracy and simplicity without\nrequiring shadow masks during inference. Trained on a dataset of 10,000 natural\nimages augmented with synthetic shadows, FieldNet outperforms state-of-the-art\nmethods on benchmark datasets (ISTD, ISTD+, SRD), with up to $9$x speed\nimprovements (66 FPS on Nvidia 2080Ti) and superior shadow removal quality\n(PSNR: 38.67, SSIM: 0.991). Real-world case studies in precision agriculture\nrobotics demonstrate the practical impact of FieldNet in enhancing weed\ndetection accuracy. These advancements establish FieldNet as a robust,\nefficient solution for real-time vision tasks in field robotics and beyond.",
        "timestamp": "2025-07-09T00:41:49.971Z",
        "rating": "novote",
        "publishedDate": "2024-03-13T00:04:07Z",
        "tags": [
          "cs.CV"
        ],
        "doi": "10.1016/j.eswa.2025.127442",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 195,
        "object_id": "paper:arxiv.2403.08142",
        "created_at": "2025-07-09T00:41:50+00:00",
        "updated_at": "2025-07-09T00:42:30+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2212.08983": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2212.08983",
        "url": "https://arxiv.org/html/2212.08983v2",
        "title": "Adaptive deep learning framework for robust unsupervised underwater\n  image enhancement",
        "authors": "Alzayat Saleh, Marcus Sheaves, Dean Jerry, Mostafa Rahimi Azghadi",
        "abstract": "One of the main challenges in deep learning-based underwater image\nenhancement is the limited availability of high-quality training data.\nUnderwater images are difficult to capture and are often of poor quality due to\nthe distortion and loss of colour and contrast in water. This makes it\ndifficult to train supervised deep learning models on large and diverse\ndatasets, which can limit the model's performance. In this paper, we explore an\nalternative approach to supervised underwater image enhancement. Specifically,\nwe propose a novel unsupervised underwater image enhancement framework that\nemploys a conditional variational autoencoder (cVAE) to train a deep learning\nmodel with probabilistic adaptive instance normalization (PAdaIN) and\nstatistically guided multi-colour space stretch that produces realistic\nunderwater images. The resulting framework is composed of a U-Net as a feature\nextractor and a PAdaIN to encode the uncertainty, which we call UDnet. To\nimprove the visual quality of the images generated by UDnet, we use a\nstatistically guided multi-colour space stretch module that ensures visual\nconsistency with the input image and provides an alternative to training using\na ground truth image. The proposed model does not need manual human annotation\nand can learn with a limited amount of data and achieves state-of-the-art\nresults on underwater images. We evaluated our proposed framework on eight\npublicly-available datasets. The results show that our proposed framework\nyields competitive performance compared to other state-of-the-art approaches in\nquantitative as well as qualitative metrics. Code available at\nhttps://github.com/alzayats/UDnet .",
        "timestamp": "2025-07-09T00:41:47.883Z",
        "rating": "novote",
        "publishedDate": "2022-12-18T01:07:20Z",
        "tags": [
          "cs.CV"
        ],
        "doi": "10.1016/j.eswa.2024.126314",
        "journalName": "Expert Systems with Applications, 126314 (2025)",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 194,
        "object_id": "paper:arxiv.2212.08983",
        "created_at": "2025-07-09T00:41:48+00:00",
        "updated_at": "2025-07-09T00:42:07+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.13461": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.13461",
        "url": "https://arxiv.org/html/2501.13461v1",
        "title": "Knowledge-Informed Multi-Agent Trajectory Prediction at Signalized\n  Intersections for Infrastructure-to-Everything",
        "authors": "Huilin Yin, Yangwenhui Xu, Jiaxiang Li, Hao Zhang, Gerhard Rigoll",
        "abstract": "Multi-agent trajectory prediction at signalized intersections is crucial for\ndeveloping efficient intelligent transportation systems and safe autonomous\ndriving systems. Due to the complexity of intersection scenarios and the\nlimitations of single-vehicle perception, the performance of vehicle-centric\nprediction methods has reached a plateau. In this paper, we introduce an\nInfrastructure-to-Everything (I2X) collaborative prediction scheme. In this\nscheme, roadside units (RSUs) independently forecast the future trajectories of\nall vehicles and transmit these predictions unidirectionally to subscribing\nvehicles. Building on this scheme, we propose I2XTraj, a dedicated\ninfrastructure-based trajectory prediction model. I2XTraj leverages real-time\ntraffic signal states, prior maneuver strategy knowledge, and multi-agent\ninteractions to generate accurate, joint multi-modal trajectory prediction.\nFirst, a continuous signal-informed mechanism is proposed to adaptively process\nreal-time traffic signals to guide trajectory proposal generation under varied\nintersection configurations. Second, a driving strategy awareness mechanism\nestimates the joint distribution of maneuver strategies by integrating spatial\npriors of intersection areas with dynamic vehicle states, enabling coverage of\nthe full set of feasible maneuvers. Third, a spatial-temporal-mode attention\nnetwork models multi-agent interactions to refine and adjust joint trajectory\noutputs.Finally, I2XTraj is evaluated on two real-world datasets of signalized\nintersections, the V2X-Seq and the SinD drone dataset. In both\nsingle-infrastructure and online collaborative scenarios, our model outperforms\nstate-of-the-art methods by over 30\\% on V2X-Seq and 15\\% on SinD,\ndemonstrating strong generalizability and robustness.",
        "timestamp": "2025-07-09T00:41:46.396Z",
        "rating": "novote",
        "publishedDate": "2025-01-23T08:23:45Z",
        "tags": [
          "cs.RO",
          "cs.CV",
          "cs.MA"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 193,
        "object_id": "paper:arxiv.2501.13461",
        "created_at": "2025-07-09T00:41:46+00:00",
        "updated_at": "2025-07-09T00:42:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.01324": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.01324",
        "url": "https://arxiv.org/html/2506.01324v1",
        "title": "Near-Optimal Clustering in Mixture of Markov Chains",
        "authors": "Junghyun Lee, Yassir Jedra, Alexandre Prouti\u00e8re, Se-Young Yun",
        "abstract": "We study the problem of clustering $T$ trajectories of length $H$, each\ngenerated by one of $K$ unknown ergodic Markov chains over a finite state space\nof size $S$. The goal is to accurately group trajectories according to their\nunderlying generative model. We begin by deriving an instance-dependent,\nhigh-probability lower bound on the clustering error rate, governed by the\nweighted KL divergence between the transition kernels of the chains. We then\npresent a novel two-stage clustering algorithm. In Stage~I, we apply spectral\nclustering using a new injective Euclidean embedding for ergodic Markov chains\n-- a contribution of independent interest that enables sharp concentration\nresults. Stage~II refines the initial clusters via a single step of\nlikelihood-based reassignment. Our method achieves a near-optimal clustering\nerror with high probability, under the conditions $H =\n\\tilde{\\Omega}(\\gamma_{\\mathrm{ps}}^{-1} (S^2 \\vee \\pi_{\\min}^{-1}))$ and $TH =\n\\tilde{\\Omega}(\\gamma_{\\mathrm{ps}}^{-1} S^2 )$, where $\\pi_{\\min}$ is the\nminimum stationary probability of a state across the $K$ chains and\n$\\gamma_{\\mathrm{ps}}$ is the minimum pseudo-spectral gap. These requirements\nprovide significant improvements, if not at least comparable, to the\nstate-of-the-art guarantee (Kausik et al., 2023), and moreover, our algorithm\noffers a key practical advantage: unlike existing approach, it requires no\nprior knowledge of model-specific quantities (e.g., separation between kernels\nor visitation probabilities). We conclude by discussing the inherent gap\nbetween our upper and lower bounds, providing insights into the unique\nstructure of this clustering problem.",
        "timestamp": "2025-07-09T00:41:46.101Z",
        "rating": "novote",
        "publishedDate": "2025-06-02T05:10:40Z",
        "tags": [
          "stat.ML",
          "cs.IT",
          "cs.LG",
          "math.IT",
          "math.PR"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 192,
        "object_id": "paper:arxiv.2506.01324",
        "created_at": "2025-07-09T00:41:46+00:00",
        "updated_at": "2025-07-09T00:42:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.13940": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.13940",
        "url": "https://arxiv.org/html/2408.13940",
        "title": "Derailer-Rerailer: Adaptive Verification for Efficient and Reliable\n  Language Model Reasoning",
        "authors": "Guangya Wan, Yuqi Wu, Hao Wang, Shengming Zhao, Jie Chen, Sheng Li",
        "abstract": "Large Language Models (LLMs) have shown impressive reasoning capabilities,\nyet existing prompting methods face a critical trade-off: simple approaches\noften struggle with complex tasks and reasoning stability, while more\nsophisticated methods require multiple inferences and substantial computational\nresources, limiting their practical deployment. To address this challenge, we\npropose Derailer-Rerailer, a novel framework that adaptively balances reasoning\naccuracy and computational efficiency. At its core, our framework employs a\nlightweight Derailer mechanism to assess reasoning stability and selectively\ntriggers an advanced Rerailer verification process only when necessary, thereby\noptimizing computational resource usage. Extensive evaluation across both open\nand closed-source models on more than 20 categories of mathematical, symbolic,\nand commonsense reasoning tasks demonstrates our framework's effectiveness:\nDerailer-Rerailer achieves significant accuracy improvements (8-11\\% across\nvarious reasoning tasks) while maintaining 2-3 times better efficiency than\nexisting verification methods, with particularly strong performance in\nmathematical and symbolic reasoning, offering a practical solution for\nenhancing LLM reasoning reliability while significantly reducing computational\noverhead.",
        "timestamp": "2025-07-09T00:41:44.534Z",
        "rating": "novote",
        "publishedDate": "2024-08-25T21:20:17Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 191,
        "object_id": "paper:arxiv.2408.13940",
        "created_at": "2025-07-09T00:41:44+00:00",
        "updated_at": "2025-07-09T00:42:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.08667": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.08667",
        "url": "https://arxiv.org/html/2501.08667v1",
        "title": "TimeFlow: Longitudinal Brain Image Registration and Aging Progression\n  Analysis",
        "authors": "Bailiang Jian, Jiazhen Pan, Yitong Li, Fabian Bongratz, Ruochen Li, Daniel Rueckert, Benedikt Wiestler, Christian Wachinger",
        "abstract": "Predicting future brain states is crucial for understanding healthy aging and\nneurodegenerative diseases. Longitudinal brain MRI registration, a cornerstone\nfor such analyses, has long been limited by its inability to forecast future\ndevelopments, reliance on extensive dense longitudinal data, and the need to\nbalance registration accuracy with temporal smoothness. In this work, we\npresent \\emph{TimeFlow}, a novel framework for longitudinal brain MRI\nregistration that overcomes all these challenges. TimeFlow leverages a U-Net\narchitecture with temporal conditioning inspired by diffusion models, enabling\naccurate registration using only two images as input and facilitating\nprospective analyses through future image prediction. Unlike traditional\nmethods, TimeFlow eliminates the demand for explicit smoothness regularizers\nand dense sequential data while maintaining temporal consistency and\ncontinuity. Experimental results highlight its superior performance in both\nfuture timepoint prediction and registration accuracy compared to\nstate-of-the-art methods. Additionally, TimeFlow supports novel biological\nbrain aging analyses, effectively differentiating neurodegenerative conditions\nfrom healthy aging, all without requiring segmentation, thus avoiding\nnon-trivial annotation and inconsistent segmentation flaws. This framework\npaves the way for accurate, data-efficient, and annotation-free prospective\nanalyses of brain aging and chronic diseases.",
        "timestamp": "2025-07-09T00:41:41.436Z",
        "rating": "novote",
        "publishedDate": "2025-01-15T09:02:04Z",
        "tags": [
          "eess.IV",
          "cs.CV"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 190,
        "object_id": "paper:arxiv.2501.08667",
        "created_at": "2025-07-09T00:41:41+00:00",
        "updated_at": "2025-07-09T00:42:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.22998": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.22998",
        "url": "https://arxiv.org/html/2505.22998v1",
        "title": "LLM Agents for Bargaining with Utility-based Feedback",
        "authors": "Jihwan Oh, Murad Aghazada, Se-Young Yun, Taehyeon Kim",
        "abstract": "Bargaining, a critical aspect of real-world interactions, presents challenges\nfor large language models (LLMs) due to limitations in strategic depth and\nadaptation to complex human factors. Existing benchmarks often fail to capture\nthis real-world complexity. To address this and enhance LLM capabilities in\nrealistic bargaining, we introduce a comprehensive framework centered on\nutility-based feedback. Our contributions are threefold: (1) BargainArena, a\nnovel benchmark dataset with six intricate scenarios (e.g., deceptive\npractices, monopolies) to facilitate diverse strategy modeling; (2)\nhuman-aligned, economically-grounded evaluation metrics inspired by utility\ntheory, incorporating agent utility and negotiation power, which implicitly\nreflect and promote opponent-aware reasoning (OAR); and (3) a structured\nfeedback mechanism enabling LLMs to iteratively refine their bargaining\nstrategies. This mechanism can positively collaborate with in-context learning\n(ICL) prompts, including those explicitly designed to foster OAR. Experimental\nresults show that LLMs often exhibit negotiation strategies misaligned with\nhuman preferences, and that our structured feedback mechanism significantly\nimproves their performance, yielding deeper strategic and opponent-aware\nreasoning.",
        "timestamp": "2025-07-09T00:41:39.610Z",
        "rating": "novote",
        "publishedDate": "2025-05-29T02:07:27Z",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 189,
        "object_id": "paper:arxiv.2505.22998",
        "created_at": "2025-07-09T00:41:40+00:00",
        "updated_at": "2025-07-09T00:42:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.19918": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.19918",
        "url": "https://arxiv.org/html/2502.19918v2",
        "title": "Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning\n  in Large Language Models",
        "authors": "Yuan Sui, Yufei He, Tri Cao, Simeng Han, Yulin Chen, Bryan Hooi",
        "abstract": "Large Language Models (LLMs) increasingly rely on prolonged reasoning chains\nto solve complex tasks. However, this trial-and-error approach often leads to\nhigh computational overhead and error propagation, where early mistakes can\nderail subsequent steps. To address these issues, we introduce Meta-Reasoner, a\nframework that dynamically optimizes inference-time reasoning by enabling LLMs\nto \"think about how to think.\" Drawing inspiration from human meta-cognition\nand dual-process theory, Meta-Reasoner operates as a strategic advisor,\ndecoupling high-level guidance from step-by-step generation. It employs\ncontextual multi-armed bandits to iteratively evaluate reasoning progress and\nselect optimal strategies (e.g., backtrack, clarify ambiguity, restart from\nscratch, or propose alternative approaches), and reallocates computational\nresources toward the most promising paths. Our evaluations on mathematical\nreasoning and puzzles highlight the potential of dynamic reasoning chains to\novercome inherent challenges in the LLM reasoning process and also show promise\nin broader applications, offering a scalable and adaptable solution for\nreasoning-intensive tasks.",
        "timestamp": "2025-07-09T00:41:32.959Z",
        "rating": "novote",
        "publishedDate": "2025-02-27T09:40:13Z",
        "tags": [
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 188,
        "object_id": "paper:arxiv.2502.19918",
        "created_at": "2025-07-09T00:41:33+00:00",
        "updated_at": "2025-07-09T00:41:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.15075": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.15075",
        "url": "https://arxiv.org/html/2505.15075v1",
        "title": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in\n  Multimodal LLMs",
        "authors": "Hao Wang, Pinzhi Huang, Jihan Yang, Saining Xie, Daisuke Kawahara",
        "abstract": "The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models.",
        "timestamp": "2025-07-09T00:41:30.652Z",
        "rating": "novote",
        "publishedDate": "2025-05-21T03:43:37Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CV",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 187,
        "object_id": "paper:arxiv.2505.15075",
        "created_at": "2025-07-09T00:41:31+00:00",
        "updated_at": "2025-07-09T00:41:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.00418": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.00418",
        "url": "https://arxiv.org/html/2506.00418v1",
        "title": "Dual Debiasing for Noisy In-Context Learning for Text Generation",
        "authors": "Siqi Liang, Sumyeong Ahn, Paramveer S. Dhillon, Jiayu Zhou",
        "abstract": "In context learning (ICL) relies heavily on high quality demonstrations drawn\nfrom large annotated corpora. Existing approaches detect noisy annotations by\nranking local perplexities, presuming that noisy samples yield higher\nperplexities than their clean counterparts. However, this assumption breaks\ndown when the noise ratio is high and many demonstrations are flawed. We\nreexamine the perplexity based paradigm for text generation under noisy\nannotations, highlighting two sources of bias in perplexity: the annotation\nitself and the domain specific knowledge inherent in large language models\n(LLMs). To overcome these biases, we introduce a dual debiasing framework that\nuses synthesized neighbors to explicitly correct perplexity estimates, yielding\na robust Sample Cleanliness Score. This metric uncovers absolute sample\ncleanliness regardless of the overall corpus noise level. Extensive experiments\ndemonstrate our method's superior noise detection capabilities and show that\nits final ICL performance is comparable to that of a fully clean demonstration\ncorpus. Moreover, our approach remains robust even when noise ratios are\nextremely high.",
        "timestamp": "2025-07-09T00:41:30.539Z",
        "rating": "novote",
        "publishedDate": "2025-05-31T06:44:48Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "I.2.7"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 186,
        "object_id": "paper:arxiv.2506.00418",
        "created_at": "2025-07-09T00:41:31+00:00",
        "updated_at": "2025-07-09T00:41:56+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.16048": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.16048",
        "url": "https://arxiv.org/abs/2402.16048",
        "title": "How Likely Do LLMs with CoT Mimic Human Reasoning?",
        "authors": "Guangsheng Bao, Hongbo Zhang, Cunxiang Wang, Linyi Yang, Yue Zhang",
        "abstract": "Chain-of-thought emerges as a promising technique for eliciting reasoning capabilities from Large Language Models (LLMs). However, it does not always improve task performance or accurately represent reasoning processes, leaving unresolved questions about its usage. In this paper, we diagnose the underlying mechanism by comparing the reasoning process of LLMs with humans, using causal analysis to understand the relationships between the problem instruction, reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often deviate from the ideal causal chain, resulting in spurious correlations and potential consistency errors (inconsistent reasoning and answers). We also examine various factors influencing the causal structure, finding that in-context learning with examples strengthens it, while post-training techniques like supervised fine-tuning and reinforcement learning on human feedback weaken it. To our surprise, the causal structure cannot be strengthened by enlarging the model size only, urging research on new techniques. We hope that this preliminary study will shed light on understanding and improving the reasoning process in LLM.",
        "timestamp": "2025-07-09T15:56:09.189Z",
        "rating": "novote",
        "publishedDate": "2024/02/25",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 200,
        "object_id": "paper:arxiv.2402.16048",
        "created_at": "2025-07-09T15:56:09+00:00",
        "updated_at": "2025-07-09T15:56:32+00:00",
        "version": 1
      }
    },
    "interactions:openreview.EDoD3DgivF": {
      "data": {
        "sourceId": "openreview",
        "paperId": "EDoD3DgivF",
        "interactions": []
      },
      "meta": {
        "issue_number": 276,
        "object_id": "interactions:openreview.EDoD3DgivF",
        "created_at": "2025-10-15T14:00:09+00:00",
        "updated_at": "2025-10-15T14:00:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2402.16048": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.16048",
        "interactions": []
      },
      "meta": {
        "issue_number": 201,
        "object_id": "interactions:arxiv.2402.16048",
        "created_at": "2025-07-09T15:57:44+00:00",
        "updated_at": "2025-07-09T15:57:46+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.22724": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.22724",
        "url": "https://arxiv.org/abs/2506.22724",
        "title": "The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure",
        "authors": "Niyati Bafna, Tianjian Li, Kenton Murray, David R. Mortensen, David Yarowsky, Hale Sirin, Daniel Khashabi",
        "abstract": "Multilingual generation with large language models (LLMs) is often of poor quality for mid- to low-resource languages. Building on insights from interpretability, we demonstrate the existence of an implicit task-solving-->translation pipeline for generation, whereby the model first solves the required task in a largely target-language-agnostic manner, and subsequently translates answer concepts into the intended target language. We hypothesize that the failure of the translation stage is an important culprit for the observed low quality of final outputs, and formalize this as the translation barrier hypothesis. We test this hypothesis for a word translation task across 108 language pairs, using logit lens to observe model processing in intermediate layers. We find that a significant portion of overall failures indeed stems from translation failure, or the model's inability to translate correctly solved intermediate concepts into the target language. This is especially true for low-resource target languages. Our results highlight an important hurdle for end-to-end multilingual generation, and lend guiding insights for future work seeking to improve multilinguality in LLMs.",
        "timestamp": "2025-07-09T16:43:08.384Z",
        "rating": "thumbsup",
        "publishedDate": "2025/06/28",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 203,
        "object_id": "paper:arxiv.2506.22724",
        "created_at": "2025-07-09T16:43:08+00:00",
        "updated_at": "2025-07-09T16:56:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2010.08275": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2010.08275",
        "url": "https://arxiv.org/pdf/2010.08275",
        "title": "It's not Greek to mBERT: Inducing Word-Level Translations from\n  Multilingual BERT",
        "authors": "Hila Gonen, Shauli Ravfogel, Yanai Elazar, Yoav Goldberg",
        "abstract": "Recent works have demonstrated that multilingual BERT (mBERT) learns rich\ncross-lingual representations, that allow for transfer across languages. We\nstudy the word-level translation information embedded in mBERT and present two\nsimple methods that expose remarkable translation capabilities with no\nfine-tuning. The results suggest that most of this information is encoded in a\nnon-linear way, while some of it can also be recovered with purely linear\ntools. As part of our analysis, we test the hypothesis that mBERT learns\nrepresentations which contain both a language-encoding component and an\nabstract, cross-lingual component, and explicitly identify an empirical\nlanguage-identity subspace within mBERT representations.",
        "timestamp": "2025-07-09T16:52:30.381Z",
        "rating": "novote",
        "publishedDate": "2020-10-16T09:49:32Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 207,
        "object_id": "paper:arxiv.2010.08275",
        "created_at": "2025-07-09T16:52:30+00:00",
        "updated_at": "2025-07-09T16:52:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2006.00995": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2006.00995",
        "url": "https://arxiv.org/abs/2006.00995",
        "title": "Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals",
        "authors": "Yanai Elazar, Shauli Ravfogel, Alon Jacovi, Yoav Goldberg",
        "abstract": "A growing body of work makes use of probing to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results and offer an alternative method that focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention that removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, e.g. is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.",
        "timestamp": "2025-07-09T16:51:35.747Z",
        "rating": "novote",
        "publishedDate": "2020/06/01",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 205,
        "object_id": "paper:arxiv.2006.00995",
        "created_at": "2025-07-09T16:51:36+00:00",
        "updated_at": "2025-07-09T16:51:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.22724": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.22724",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T16:55:56.199Z",
            "data": {
              "session_id": "session_1752080156179_sk00odj",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-07-09T16:55:45.431Z",
              "end_time": "2025-07-09T16:55:56.179Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T16:58:48.253Z",
            "data": {
              "session_id": "session_1752080328243_zi75afk",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-07-09T16:58:42.409Z",
              "end_time": "2025-07-09T16:58:48.243Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T17:34:06.205Z",
            "data": {
              "session_id": "session_1752082441385_70ve51n",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-07-09T17:23:21.207Z",
              "end_time": "2025-07-09T17:34:01.385Z",
              "heartbeat_count": 128,
              "duration_seconds": 640,
              "idle_seconds": 0,
              "total_elapsed_seconds": 640
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T17:35:36.425Z",
            "data": {
              "session_id": "session_1752082536196_meqh6nq",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-07-09T17:35:11.984Z",
              "end_time": "2025-07-09T17:35:36.196Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 4,
              "total_elapsed_seconds": 24
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T18:22:00.786Z",
            "data": {
              "session_id": "session_1752085320380_br9t7qo",
              "source_id": "arxiv",
              "paper_id": "2506.22724",
              "start_time": "2025-07-09T18:18:59.491Z",
              "end_time": "2025-07-09T18:22:00.380Z",
              "heartbeat_count": 36,
              "duration_seconds": 180,
              "idle_seconds": 1,
              "total_elapsed_seconds": 181
            }
          }
        ]
      },
      "meta": {
        "issue_number": 204,
        "object_id": "interactions:arxiv.2506.22724",
        "created_at": "2025-07-09T16:45:45+00:00",
        "updated_at": "2025-07-09T18:22:23+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2010.08275": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2010.08275",
        "interactions": []
      },
      "meta": {
        "issue_number": 208,
        "object_id": "interactions:arxiv.2010.08275",
        "created_at": "2025-07-09T16:53:09+00:00",
        "updated_at": "2025-07-09T16:53:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2101.11109": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2101.11109",
        "url": "https://arxiv.org/abs/2101.11109",
        "title": "First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT",
        "authors": "Benjamin Muller, Yanai Elazar, Beno\u00eet Sagot, Djam\u00e9 Seddah",
        "abstract": "Multilingual pretrained language models have demonstrated remarkable zero-shot cross-lingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model's internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a task-specific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during fine-tuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis.",
        "timestamp": "2025-07-09T16:54:57.742Z",
        "rating": "novote",
        "publishedDate": "2021/01/26",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 209,
        "object_id": "paper:arxiv.2101.11109",
        "created_at": "2025-07-09T16:54:58+00:00",
        "updated_at": "2025-07-09T16:55:17+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.00163": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.00163",
        "url": "https://arxiv.org/abs/2507.00163",
        "title": "Prompting as Scientific Inquiry",
        "authors": "Ari Holtzman, Chenhao Tan",
        "abstract": "Prompting is the primary method by which we study and control large language models. It is also one of the most powerful: nearly every major capability attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was first unlocked through prompting. Yet prompting is rarely treated as science and is frequently frowned upon as alchemy. We argue that this is a category error. If we treat LLMs as a new kind of complex and opaque organism that is trained rather than programmed, then prompting is not a workaround: it is behavioral science. Mechanistic interpretability peers into the neural substrate, prompting probes the model in its native interface: language. We contend that prompting is not inferior, but rather a key component in the science of LLMs.",
        "timestamp": "2025-07-09T20:01:47.850Z",
        "rating": "novote",
        "publishedDate": "2025/06/30",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 210,
        "object_id": "paper:arxiv.2507.00163",
        "created_at": "2025-07-09T20:01:48+00:00",
        "updated_at": "2025-07-09T20:02:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.23829": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.23829",
        "url": "https://arxiv.org/pdf/2505.23829",
        "title": "BiasFilter: An Inference-Time Debiasing Framework for Large Language\n  Models",
        "authors": "Xiaoqing Cheng, Ruizhe Chen, Hongying Zan, Yuxiang Jia, Min Peng",
        "abstract": "Mitigating social bias in large language models (LLMs) has become an\nincreasingly important research objective. However, existing debiasing methods\noften incur high human and computational costs, exhibit limited effectiveness,\nand struggle to scale to larger models and open-ended generation tasks. To\naddress these limitations, this paper proposes BiasFilter, a model-agnostic,\ninference-time debiasing framework that integrates seamlessly with both\nopen-source and API-based LLMs. Instead of relying on retraining with balanced\ndata or modifying model parameters, BiasFilter enforces fairness by filtering\ngeneration outputs in real time. Specifically, it periodically evaluates\nintermediate outputs every few tokens, maintains an active set of candidate\ncontinuations, and incrementally completes generation by discarding low-reward\nsegments based on a fairness reward signal. To support this process, we\nconstruct a fairness preference dataset and train an implicit reward model to\nassess token-level fairness in generated responses. Extensive experiments\ndemonstrate that BiasFilter effectively mitigates social bias across a range of\nLLMs while preserving overall generation quality.",
        "timestamp": "2025-07-09T21:42:04.115Z",
        "rating": "novote",
        "publishedDate": "2025-05-28T08:09:10Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 212,
        "object_id": "paper:arxiv.2505.23829",
        "created_at": "2025-07-09T21:42:04+00:00",
        "updated_at": "2025-07-09T21:42:23+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.23829": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.23829",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T22:06:58.469Z",
            "data": {
              "session_id": "session_1752098817990_pd171u1",
              "source_id": "arxiv",
              "paper_id": "2505.23829",
              "start_time": "2025-07-09T22:00:42.050Z",
              "end_time": "2025-07-09T22:06:57.990Z",
              "heartbeat_count": 75,
              "duration_seconds": 375,
              "idle_seconds": 1,
              "total_elapsed_seconds": 376
            }
          }
        ]
      },
      "meta": {
        "issue_number": 213,
        "object_id": "interactions:arxiv.2505.23829",
        "created_at": "2025-07-09T21:43:44+00:00",
        "updated_at": "2025-07-09T22:07:18+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.00163": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.00163",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T23:34:35.182Z",
            "data": {
              "session_id": "session_1752104075148_yyi9n46",
              "source_id": "arxiv",
              "paper_id": "2507.00163",
              "start_time": "2025-07-09T23:34:23.554Z",
              "end_time": "2025-07-09T23:34:35.148Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-07-09T23:42:55.049Z",
            "data": {
              "session_id": "session_1752104575039_g3nso3c",
              "source_id": "arxiv",
              "paper_id": "2507.00163",
              "start_time": "2025-07-09T23:42:44.087Z",
              "end_time": "2025-07-09T23:42:55.039Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 214,
        "object_id": "interactions:arxiv.2507.00163",
        "created_at": "2025-07-09T23:33:08+00:00",
        "updated_at": "2025-07-09T23:43:14+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.17514": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17514",
        "url": "https://arxiv.org/pdf/2503.17514",
        "title": "Language Models May Verbatim Complete Text They Were Not Explicitly\n  Trained On",
        "authors": "Ken Ziyu Liu, Christopher A. Choquette-Choo, Matthew Jagielski, Peter Kairouz, Sanmi Koyejo, Percy Liang, Nicolas Papernot",
        "abstract": "An important question today is whether a given text was used to train a large\nlanguage model (LLM). A \\emph{completion} test is often employed: check if the\nLLM completes a sufficiently complex text. This, however, requires a\nground-truth definition of membership; most commonly, it is defined as a member\nbased on the $n$-gram overlap between the target text and any text in the\ndataset. In this work, we demonstrate that this $n$-gram based membership\ndefinition can be effectively gamed. We study scenarios where sequences are\n\\emph{non-members} for a given $n$ and we find that completion tests still\nsucceed. We find many natural cases of this phenomenon by retraining LLMs from\nscratch after removing all training samples that were completed; these cases\ninclude exact duplicates, near-duplicates, and even short overlaps. They\nshowcase that it is difficult to find a single viable choice of $n$ for\nmembership definitions. Using these insights, we design adversarial datasets\nthat can cause a given target sequence to be completed without containing it,\nfor any reasonable choice of $n$. Our findings highlight the inadequacy of\n$n$-gram membership, suggesting membership definitions fail to account for\nauxiliary information available to the training algorithm.",
        "timestamp": "2025-07-09T23:46:17.577Z",
        "rating": "novote",
        "publishedDate": "2025-03-21T19:57:04Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.CR",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 215,
        "object_id": "paper:arxiv.2503.17514",
        "created_at": "2025-07-09T23:46:17+00:00",
        "updated_at": "2025-07-09T23:46:37+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.12463": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.12463",
        "url": "https://arxiv.org/pdf/2510.12463",
        "title": "Resource-sensitive but language-blind: Community size and not\n  grammatical complexity better predicts the accuracy of Large Language Models\n  in a novel Wug Test",
        "authors": "Nikoleta Pantelidou, Evelina Leivada, Paolo Morosi",
        "abstract": "The linguistic abilities of Large Language Models are a matter of ongoing\ndebate. This study contributes to this discussion by investigating model\nperformance in a morphological generalization task that involves novel words.\nUsing a multilingual adaptation of the Wug Test, six models were tested across\nfour partially unrelated languages (Catalan, English, Greek, and Spanish) and\ncompared with human speakers. The aim is to determine whether model accuracy\napproximates human competence and whether it is shaped primarily by linguistic\ncomplexity or by the quantity of available training data. Consistent with\nprevious research, the results show that the models are able to generalize\nmorphological processes to unseen words with human-like accuracy. However,\naccuracy patterns align more closely with community size and data availability\nthan with structural complexity, refining earlier claims in the literature. In\nparticular, languages with larger speaker communities and stronger digital\nrepresentation, such as Spanish and English, revealed higher accuracy than\nless-resourced ones like Catalan and Greek. Overall, our findings suggest that\nmodel behavior is mainly driven by the richness of linguistic resources rather\nthan by sensitivity to grammatical complexity, reflecting a form of performance\nthat resembles human linguistic competence only superficially.",
        "timestamp": "2025-10-16T06:12:59.462Z",
        "rating": "novote",
        "publishedDate": "2025-10-14T12:52:57Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 313,
        "object_id": "paper:arxiv.2510.12463",
        "created_at": "2025-10-16T06:12:59+00:00",
        "updated_at": "2025-10-16T06:13:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2311.12233": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.12233",
        "url": "https://arxiv.org/abs/2311.12233",
        "title": "Unifying Corroborative and Contributive Attributions in Large Language Models",
        "authors": "Theodora Worledge, Judy Hanwen Shen, Nicole Meister, Caleb Winston, Carlos Guestrin",
        "abstract": "As businesses, products, and services spring up around large language models, the trustworthiness of these models hinges on the verifiability of their outputs. However, methods for explaining language model outputs largely fall across two distinct fields of study which both use the term \"attribution\" to refer to entirely separate techniques: citation generation and training data attribution. In many modern applications, such as legal document generation and medical question answering, both types of attributions are important. In this work, we argue for and present a unified framework of large language model attributions. We show how existing methods of different types of attribution fall under the unified framework. We also use the framework to discuss real-world use cases where one or both types of attributions are required. We believe that this unified framework will guide the use case driven development of systems that leverage both types of attribution, as well as the standardization of their evaluation.",
        "timestamp": "2025-10-15T14:08:11.111Z",
        "rating": "novote",
        "publishedDate": "2023/11/20",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 280,
        "object_id": "paper:arxiv.2311.12233",
        "created_at": "2025-10-15T14:08:11+00:00",
        "updated_at": "2025-10-15T18:45:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.17585": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.17585",
        "url": "https://arxiv.org/pdf/2506.17585",
        "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language\n  Models",
        "authors": "Yukun Huang, Sanxing Chen, Jian Pei, Manzil Zaheer, Bhuwan Dhingra",
        "abstract": "Trustworthy language models should provide both correct and verifiable\nanswers. While language models can sometimes attribute their outputs to\npretraining data, their citations are often unreliable due to hallucination. As\na result, current systems insert citations by querying an external retriever at\ninference time, introducing latency, infrastructure dependence, and\nvulnerability to retrieval noise. We explore whether LLMs can be made to\nreliably attribute to the documents seen during (continual)\npretraining--without test-time retrieval--by revising the training process. To\nevaluate this, we release CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and\nprobes both short-form (single fact) and long-form (multi-fact) citation tasks.\nOur approach follows a two-stage process: (1) continual pretraining to bind\nfacts to persistent document identifiers, and (2) instruction tuning to elicit\ncitation behavior. We find that simple Passive Indexing, which appends an\nidentifier to each document, helps memorize verbatim text but fails on\nparaphrased or compositional facts. Instead, we propose Active Indexing, which\ncontinually pretrains on synthetic QA pairs that (1) restate each fact in\ndiverse compositional forms, and (2) require bidirectional source-to-fact and\nfact-to-source generation, jointly teaching the model to generate content from\na cited source and to attribute its own answers. Experiments with Qwen2.5-7B\nand 3B show that Active Indexing consistently outperforms Passive Indexing\nacross all tasks and models, with citation precision gains up to 30.2 percent.\nOur ablation studies reveal that performance continues to improve as we scale\nthe amount of augmented data, showing a clear upward trend even at 16 times the\noriginal token count.",
        "timestamp": "2025-10-15T13:11:12.642Z",
        "rating": "novote",
        "publishedDate": "2025-06-21T04:48:05Z",
        "tags": [
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 265,
        "object_id": "paper:arxiv.2506.17585",
        "created_at": "2025-10-15T13:11:12+00:00",
        "updated_at": "2025-10-15T18:46:25+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2311.12233": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.12233",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T16:32:10.952Z",
            "data": {
              "session_id": "session_1760545930339_eppub99",
              "source_id": "arxiv",
              "paper_id": "2311.12233",
              "start_time": "2025-10-15T16:26:34.096Z",
              "end_time": "2025-10-15T16:32:10.339Z",
              "heartbeat_count": 67,
              "duration_seconds": 335,
              "idle_seconds": 1,
              "total_elapsed_seconds": 336
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T12:04:27.059Z",
            "data": {
              "session_id": "session_1760616267019_g1gk9w4",
              "source_id": "arxiv",
              "paper_id": "2311.12233",
              "start_time": "2025-10-16T12:04:11.470Z",
              "end_time": "2025-10-16T12:04:27.019Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 1,
              "total_elapsed_seconds": 16
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T12:32:02.978Z",
            "data": {
              "session_id": "session_1760617922364_tmuxtxk",
              "source_id": "arxiv",
              "paper_id": "2311.12233",
              "start_time": "2025-10-16T12:31:07.530Z",
              "end_time": "2025-10-16T12:32:02.364Z",
              "heartbeat_count": 10,
              "duration_seconds": 50,
              "idle_seconds": 5,
              "total_elapsed_seconds": 55
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T14:03:21.419Z",
            "data": {
              "session_id": "session_1760623400839_pzvwhes",
              "source_id": "arxiv",
              "paper_id": "2311.12233",
              "start_time": "2025-10-16T14:02:35.838Z",
              "end_time": "2025-10-16T14:03:20.839Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 5,
              "total_elapsed_seconds": 45
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T14:19:48.001Z",
            "data": {
              "session_id": "session_1760624386920_x7n5n5d",
              "source_id": "arxiv",
              "paper_id": "2311.12233",
              "start_time": "2025-10-16T14:04:58.407Z",
              "end_time": "2025-10-16T14:19:46.920Z",
              "heartbeat_count": 177,
              "duration_seconds": 885,
              "idle_seconds": 4,
              "total_elapsed_seconds": 889
            }
          }
        ]
      },
      "meta": {
        "issue_number": 304,
        "object_id": "interactions:arxiv.2311.12233",
        "created_at": "2025-10-15T16:08:25+00:00",
        "updated_at": "2025-10-16T14:20:11+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2210.01296": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.01296",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T13:41:47.300Z",
            "data": {
              "session_id": "session_1760535706690_2fpslis",
              "source_id": "arxiv",
              "paper_id": "2210.01296",
              "start_time": "2025-10-15T13:30:11.461Z",
              "end_time": "2025-10-15T13:41:46.690Z",
              "heartbeat_count": 139,
              "duration_seconds": 695,
              "idle_seconds": 0,
              "total_elapsed_seconds": 695
            }
          }
        ]
      },
      "meta": {
        "issue_number": 273,
        "object_id": "interactions:arxiv.2210.01296",
        "created_at": "2025-10-15T13:30:11+00:00",
        "updated_at": "2025-10-15T18:45:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2210.01296": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2210.01296",
        "url": "https://arxiv.org/abs/2210.01296",
        "title": "Recitation-Augmented Language Models",
        "authors": "Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, Denny Zhou",
        "abstract": "We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of \\method~on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at \"this https URL.",
        "timestamp": "2025-10-15T13:25:35.247Z",
        "rating": "novote",
        "publishedDate": "2022/10/04",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 271,
        "object_id": "paper:arxiv.2210.01296",
        "created_at": "2025-10-15T13:25:35+00:00",
        "updated_at": "2025-10-15T18:45:54+00:00",
        "version": 1
      }
    },
    "paper:openreview.w7LU2s14kE": {
      "data": {
        "sourceId": "openreview",
        "paperId": "w7LU2s14kE",
        "url": "https://openreview.net/forum?id=w7LU2s14kE",
        "title": "Linearity of Relation Decoding in Transformer Language Models",
        "authors": "Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, David Bau",
        "abstract": "Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.",
        "timestamp": "2025-10-15T13:54:03.261Z",
        "rating": "novote",
        "publishedDate": "16 Jan 2024",
        "tags": [
          "Natural language processing",
          "interpretability",
          "language models"
        ],
        "doi": "",
        "journalName": "ICLR 2024 spotlight",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 274,
        "object_id": "paper:openreview.w7LU2s14kE",
        "created_at": "2025-10-15T13:54:03+00:00",
        "updated_at": "2025-10-15T18:45:38+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2308.14179": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2308.14179",
        "url": "https://arxiv.org/abs/2308.14179",
        "title": "Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP",
        "authors": "Vedant Palit, Rohan Pandey, Aryaman Arora, Paul Pu Liang",
        "abstract": "Mechanistic interpretability seeks to understand the neural mechanisms that enable specific behaviors in Large Language Models (LLMs) by leveraging causality-based methods. While these approaches have identified neural circuits that copy spans of text, capture factual knowledge, and more, they remain unusable for multimodal models since adapting these tools to the vision-language domain requires considerable architectural changes. In this work, we adapt a unimodal causal tracing tool to BLIP to enable the study of the neural mechanisms underlying image-conditioned text generation. We demonstrate our approach on a visual question answering dataset, highlighting the causal relevance of later layer representations for all tokens. Furthermore, we release our BLIP causal tracing tool as open source to enable further experimentation in vision-language mechanistic interpretability by the community. Our code is available at this https URL.",
        "timestamp": "2025-10-15T14:10:47.695Z",
        "rating": "novote",
        "publishedDate": "2023/08/27",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Computer Vision and Pattern Recognition (cs.CV)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 282,
        "object_id": "paper:arxiv.2308.14179",
        "created_at": "2025-10-15T14:10:48+00:00",
        "updated_at": "2025-10-15T18:45:00+00:00",
        "version": 1
      }
    },
    "interactions:openreview.w7LU2s14kE": {
      "data": {
        "sourceId": "openreview",
        "paperId": "w7LU2s14kE",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T14:02:16.193Z",
            "data": {
              "session_id": "session_1760536935539_nbomscy",
              "source_id": "openreview",
              "paper_id": "w7LU2s14kE",
              "start_time": "2025-10-15T14:00:04.773Z",
              "end_time": "2025-10-15T14:02:15.539Z",
              "heartbeat_count": 26,
              "duration_seconds": 130,
              "idle_seconds": 1,
              "total_elapsed_seconds": 131
            }
          }
        ]
      },
      "meta": {
        "issue_number": 275,
        "object_id": "interactions:openreview.w7LU2s14kE",
        "created_at": "2025-10-15T13:59:50+00:00",
        "updated_at": "2025-10-15T18:45:32+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.17585": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.17585",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T13:18:21.325Z",
            "data": {
              "session_id": "session_1760534301308_k9ihiuq",
              "source_id": "arxiv",
              "paper_id": "2506.17585",
              "start_time": "2025-10-15T13:18:10.575Z",
              "end_time": "2025-10-15T13:18:21.308Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "issue_number": 266,
        "object_id": "interactions:arxiv.2506.17585",
        "created_at": "2025-10-15T13:15:26+00:00",
        "updated_at": "2025-10-15T18:46:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.22362": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.22362",
        "url": "https://arxiv.org/abs/2503.22362",
        "title": "Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs",
        "authors": "Yuan He, Bailan He, Zifeng Ding, Alisia Lupidi, Yuqicheng Zhu, Shuo Chen, Caiqi Zhang, Jiaoyan Chen, Yunpu Ma, Volker Tresp, Ian Horrocks",
        "abstract": "Understanding and mitigating hallucinations in Large Language Models (LLMs) is crucial for ensuring reliable content generation. While previous research has primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and directly links model behaviour to the pre-training data that forms their prior knowledge. Specifically, we demonstrate that an asymmetry exists in the recognition of logically equivalent facts, which can be attributed to frequency discrepancies of entities appearing as subjects versus objects. Given that most pre-training datasets are inaccessible, we leverage the fully open-source OLMo series by indexing its Dolma dataset to estimate entity frequencies. Using relational facts (represented as triples) from Wikidata5M, we construct probing datasets to isolate this effect. Our experiments reveal that facts with a high-frequency subject and a low-frequency object are better recognised than their inverse, despite their logical equivalence. The pattern reverses in low-to-high frequency settings, and no statistically significant asymmetry emerges when both entities are high-frequency. These findings highlight the influential role of pre-training data in shaping model predictions and provide insights for inferring the characteristics of pre-training data in closed or partially closed LLMs.",
        "timestamp": "2025-10-15T07:16:30.148Z",
        "rating": "novote",
        "publishedDate": "2025/03/28",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 219,
        "object_id": "paper:arxiv.2503.22362",
        "created_at": "2025-10-15T07:16:30+00:00",
        "updated_at": "2025-10-15T18:50:14+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.22362": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.22362",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T16:58:27.408Z",
            "data": {
              "session_id": "session_1760633906400_t790k52",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-16T16:57:15.583Z",
              "end_time": "2025-10-16T16:58:26.400Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 1,
              "total_elapsed_seconds": 71
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T17:08:27.736Z",
            "data": {
              "session_id": "session_1760634506891_1i9fob1",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-16T17:01:44.581Z",
              "end_time": "2025-10-16T17:08:26.890Z",
              "heartbeat_count": 80,
              "duration_seconds": 400,
              "idle_seconds": 2,
              "total_elapsed_seconds": 402
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T17:20:43.806Z",
            "data": {
              "session_id": "session_1760635241992_qd7fa17",
              "source_id": "arxiv",
              "paper_id": "2503.22362",
              "start_time": "2025-10-16T17:12:12.749Z",
              "end_time": "2025-10-16T17:20:41.992Z",
              "heartbeat_count": 101,
              "duration_seconds": 505,
              "idle_seconds": 4,
              "total_elapsed_seconds": 509
            }
          }
        ]
      },
      "meta": {
        "issue_number": 264,
        "object_id": "interactions:arxiv.2503.22362",
        "created_at": "2025-10-15T13:11:12+00:00",
        "updated_at": "2025-10-16T17:21:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2411.16679": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.16679",
        "url": "https://arxiv.org/abs/2411.16679",
        "title": "Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?",
        "authors": "Sohee Yang, Nora Kassner, Elena Gribovskaya, Sebastian Riedel, Mor Geva",
        "abstract": "We evaluate how well Large Language Models (LLMs) latently recall and compose facts to answer multi-hop queries like \"In the year Scarlett Johansson was born, the Summer Olympics were hosted in the country of\". One major challenge in such evaluation is that LLMs may have developed shortcuts by encountering the head entity \"Scarlett Johansson\" and the answer entity \"United States\" in the same training sequences or merely guess the answer based on frequency-based priors. To prevent shortcuts, we exclude test queries where the head and answer entities might have co-appeared during training. Through careful selection of relations and facts and systematic removal of cases where models might guess answers or exploit partial matches, we construct an evaluation dataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs demonstrate promising latent multi-hop reasoning abilities without exploiting shortcuts, but only for certain types of queries. For queries requiring latent recall of countries as the intermediate answer, the best models achieve 80% latent composability, but this drops to just 5% for the recall of years. Comparisons with Chain-of-Thought highlight a significant gap between the ability of models to reason latently versus explicitly. Analysis reveals that latent representations of the intermediate answer are constructed more often in queries with higher latent composability, and shows the emergence of latent multi-hop reasoning during pretraining.",
        "timestamp": "2025-10-15T08:42:59.886Z",
        "rating": "novote",
        "publishedDate": "2024/11/25",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 243,
        "object_id": "paper:arxiv.2411.16679",
        "created_at": "2025-10-15T08:43:00+00:00",
        "updated_at": "2025-10-15T18:48:06+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2411.16679": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.16679",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T08:48:30.803Z",
            "data": {
              "session_id": "session_1760518110753_ksvjjjw",
              "source_id": "arxiv",
              "paper_id": "2411.16679",
              "start_time": "2025-10-15T08:47:41.016Z",
              "end_time": "2025-10-15T08:48:30.753Z",
              "heartbeat_count": 9,
              "duration_seconds": 45,
              "idle_seconds": 5,
              "total_elapsed_seconds": 50
            }
          }
        ]
      },
      "meta": {
        "issue_number": 244,
        "object_id": "interactions:arxiv.2411.16679",
        "created_at": "2025-10-15T08:47:40+00:00",
        "updated_at": "2025-10-15T18:47:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2311.17035": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.17035",
        "url": "https://arxiv.org/pdf/2311.17035",
        "title": "Scalable Extraction of Training Data from (Production) Language Models",
        "authors": "Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, Katherine Lee",
        "abstract": "This paper studies extractable memorization: training data that an adversary\ncan efficiently extract by querying a machine learning model without prior\nknowledge of the training dataset. We show an adversary can extract gigabytes\nof training data from open-source language models like Pythia or GPT-Neo,\nsemi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing\ntechniques from the literature suffice to attack unaligned models; in order to\nattack the aligned ChatGPT, we develop a new divergence attack that causes the\nmodel to diverge from its chatbot-style generations and emit training data at a\nrate 150x higher than when behaving properly. Our methods show practical\nattacks can recover far more data than previously thought, and reveal that\ncurrent alignment techniques do not eliminate memorization.",
        "timestamp": "2025-10-15T10:46:38.523Z",
        "rating": "novote",
        "publishedDate": "2023-11-28T18:47:03Z",
        "tags": [
          "cs.LG",
          "cs.CL",
          "cs.CR"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 254,
        "object_id": "paper:arxiv.2311.17035",
        "created_at": "2025-10-15T10:46:39+00:00",
        "updated_at": "2025-10-15T18:47:10+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.20707": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.20707",
        "url": "https://arxiv.org/abs/2310.20707",
        "title": "What's In My Big Data?",
        "authors": "Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, Jesse Dodge",
        "abstract": "Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.",
        "timestamp": "2025-10-15T10:54:40.843Z",
        "rating": "novote",
        "publishedDate": "2023/10/31",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 256,
        "object_id": "paper:arxiv.2310.20707",
        "created_at": "2025-10-15T10:54:41+00:00",
        "updated_at": "2025-10-15T18:47:01+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2311.17035": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2311.17035",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T10:47:12.680Z",
            "data": {
              "session_id": "session_1760525232069_6lz9gmp",
              "source_id": "arxiv",
              "paper_id": "2311.17035",
              "start_time": "2025-10-15T10:46:38.160Z",
              "end_time": "2025-10-15T10:47:12.069Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 4,
              "total_elapsed_seconds": 34
            }
          }
        ]
      },
      "meta": {
        "issue_number": 255,
        "object_id": "interactions:arxiv.2311.17035",
        "created_at": "2025-10-15T10:47:13+00:00",
        "updated_at": "2025-10-15T18:47:05+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2404.01019": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.01019",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T08:37:08.669Z",
            "data": {
              "session_id": "session_1760517427715_wttmbza",
              "source_id": "arxiv",
              "paper_id": "2404.01019",
              "start_time": "2025-10-15T08:35:53.301Z",
              "end_time": "2025-10-15T08:37:07.715Z",
              "heartbeat_count": 14,
              "duration_seconds": 70,
              "idle_seconds": 4,
              "total_elapsed_seconds": 74
            }
          }
        ]
      },
      "meta": {
        "issue_number": 240,
        "object_id": "interactions:arxiv.2404.01019",
        "created_at": "2025-10-15T08:37:09+00:00",
        "updated_at": "2025-10-15T18:48:23+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2404.01019": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2404.01019",
        "url": "https://arxiv.org/pdf/2404.01019",
        "title": "Source-Aware Training Enables Knowledge Attribution in Language Models",
        "authors": "Muhammad Khalifa, David Wadden, Emma Strubell, Honglak Lee, Lu Wang, Iz Beltagy, Hao Peng",
        "abstract": "Large language models (LLMs) learn a vast amount of knowledge during\npretraining, but they are often oblivious to the source(s) of such knowledge.\nWe investigate the problem of intrinsic source citation, where LLMs are\nrequired to cite the pretraining source supporting a generated response.\nIntrinsic source citation can enhance LLM transparency, interpretability, and\nverifiability. To give LLMs such ability, we explore source-aware training -- a\nrecipe that involves (i) training the LLM to associate unique source document\nidentifiers with the knowledge in each document, followed by (ii) an\ninstruction-tuning stage to teach the LLM to cite a supporting pretraining\nsource when prompted. Source-aware training borrows from existing\npretraining/fine-tuning frameworks and requires minimal changes to the model\narchitecture or implementation. Through experiments on synthetic data, we\ndemonstrate that our training recipe can enable faithful attribution to the\npretraining data without a substantial impact on the model's perplexity\ncompared to standard pretraining. Our findings also highlight the importance of\npretraining data augmentation in achieving attribution. Code and data available\nhere: \\url{https://github.com/mukhal/intrinsic-source-citation}",
        "timestamp": "2025-10-15T08:35:53.711Z",
        "rating": "novote",
        "publishedDate": "2024-04-01T09:39:38Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 239,
        "object_id": "paper:arxiv.2404.01019",
        "created_at": "2025-10-15T08:35:54+00:00",
        "updated_at": "2025-10-15T18:48:28+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2402.16837": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.16837",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T08:48:38.424Z",
            "data": {
              "session_id": "session_1760518117777_r6c2bck",
              "source_id": "arxiv",
              "paper_id": "2402.16837",
              "start_time": "2025-10-15T08:48:31.416Z",
              "end_time": "2025-10-15T08:48:37.777Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 246,
        "object_id": "interactions:arxiv.2402.16837",
        "created_at": "2025-10-15T08:48:39+00:00",
        "updated_at": "2025-10-15T18:47:50+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2402.16837": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2402.16837",
        "url": "https://arxiv.org/pdf/2402.16837",
        "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?",
        "authors": "Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel",
        "abstract": "We study whether Large Language Models (LLMs) latently perform multi-hop\nreasoning with complex prompts such as \"The mother of the singer of\n'Superstition' is\". We look for evidence of a latent reasoning pathway where an\nLLM (1) latently identifies \"the singer of 'Superstition'\" as Stevie Wonder,\nthe bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to\ncomplete the prompt. We analyze these two hops individually and consider their\nco-occurrence as indicative of latent multi-hop reasoning. For the first hop,\nwe test if changing the prompt to indirectly mention the bridge entity instead\nof any other entity increases the LLM's internal recall of the bridge entity.\nFor the second hop, we test if increasing this recall causes the LLM to better\nutilize what it knows about the bridge entity. We find strong evidence of\nlatent multi-hop reasoning for the prompts of certain relation types, with the\nreasoning pathway used in more than 80% of the prompts. However, the\nutilization is highly contextual, varying across different types of prompts.\nAlso, on average, the evidence for the second hop and the full multi-hop\ntraversal is rather moderate and only substantial for the first hop. Moreover,\nwe find a clear scaling trend with increasing model size for the first hop of\nreasoning but not for the second hop. Our experimental findings suggest\npotential challenges and opportunities for future development and applications\nof LLMs.",
        "timestamp": "2025-10-15T08:48:31.843Z",
        "rating": "novote",
        "publishedDate": "2024-02-26T18:57:54Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 245,
        "object_id": "paper:arxiv.2402.16837",
        "created_at": "2025-10-15T08:48:32+00:00",
        "updated_at": "2025-10-15T18:47:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2508.17416": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.17416",
        "url": "https://arxiv.org/pdf/2508.17416",
        "title": "Data Leakage in Visual Datasets",
        "authors": "Patrick Ramos, Ryan Ramos, Noa Garcia",
        "abstract": "We analyze data leakage in visual datasets. Data leakage refers to images in\nevaluation benchmarks that have been seen during training, compromising fair\nmodel evaluation. Given that large-scale datasets are often sourced from the\ninternet, where many computer vision benchmarks are publicly available, our\nefforts are focused into identifying and studying this phenomenon. We\ncharacterize visual leakage into different types according to its modality,\ncoverage, and degree. By applying image retrieval techniques, we unequivocally\nshow that all the analyzed datasets present some form of leakage, and that all\ntypes of leakage, from severe instances to more subtle cases, compromise the\nreliability of model evaluation in downstream tasks.",
        "timestamp": "2025-10-15T08:30:30.611Z",
        "rating": "thumbsup",
        "publishedDate": "2025-08-24T15:42:58Z",
        "tags": [
          "cs.CV"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 237,
        "object_id": "paper:arxiv.2508.17416",
        "created_at": "2025-10-15T08:30:31+00:00",
        "updated_at": "2025-10-15T18:48:39+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2510.03721": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.03721",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T08:28:08.328Z",
            "data": {
              "session_id": "session_1760516888227_vi7rxqn",
              "source_id": "arxiv",
              "paper_id": "2510.03721",
              "start_time": "2025-10-15T08:28:03.087Z",
              "end_time": "2025-10-15T08:28:08.226Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 0,
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "issue_number": 236,
        "object_id": "interactions:arxiv.2510.03721",
        "created_at": "2025-10-15T08:27:50+00:00",
        "updated_at": "2025-10-15T18:48:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.03721": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.03721",
        "url": "https://arxiv.org/pdf/2510.03721",
        "title": "Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer\n  to Models",
        "authors": "Leander Girrbach, Stephan Alaniz, Genevieve Smith, Trevor Darrell, Zeynep Akata",
        "abstract": "Vision-language models trained on large-scale multimodal datasets show strong\ndemographic biases, but the role of training data in producing these biases\nremains unclear. A major barrier has been the lack of demographic annotations\nin web-scale datasets such as LAION-400M. We address this gap by creating\nperson-centric annotations for the full dataset, including over 276 million\nbounding boxes, perceived gender and race/ethnicity labels, and automatically\ngenerated captions. These annotations are produced through validated automatic\nlabeling pipelines combining object detection, multimodal captioning, and\nfinetuned classifiers. Using them, we uncover demographic imbalances and\nharmful associations, such as the disproportionate linking of men and\nindividuals perceived as Black or Middle Eastern with crime-related and\nnegative content. We also show that 60-70% of gender bias in CLIP and Stable\nDiffusion can be linearly explained by direct co-occurrences in the data. Our\nresources establish the first large-scale empirical link between dataset\ncomposition and downstream model bias.",
        "timestamp": "2025-10-15T06:56:17.746Z",
        "rating": "novote",
        "publishedDate": "2025-10-04T07:51:59Z",
        "tags": [
          "cs.CV",
          "cs.CL",
          "cs.CY",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 217,
        "object_id": "paper:arxiv.2510.03721",
        "created_at": "2025-10-15T06:56:18+00:00",
        "updated_at": "2025-10-15T18:50:22+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.17514": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.17514",
        "interactions": []
      },
      "meta": {
        "issue_number": 216,
        "object_id": "interactions:arxiv.2503.17514",
        "created_at": "2025-07-09T23:49:53+00:00",
        "updated_at": "2025-07-09T23:49:55+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2403.00499": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.00499",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T09:48:13.109Z",
            "data": {
              "session_id": "session_1760608092388_fomut3a",
              "source_id": "arxiv",
              "paper_id": "2403.00499",
              "start_time": "2025-10-16T09:47:58.971Z",
              "end_time": "2025-10-16T09:48:12.388Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "issue_number": 316,
        "object_id": "interactions:arxiv.2403.00499",
        "created_at": "2025-10-16T09:48:13+00:00",
        "updated_at": "2025-10-16T09:48:37+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2403.00499": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2403.00499",
        "url": "https://arxiv.org/pdf/2403.00499",
        "title": "Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of\n  Machine Cognition",
        "authors": "Ariel Goldstein, Gabriel Stanovsky",
        "abstract": "Recent advances in LLMs have sparked a debate on whether they understand\ntext. In this position paper, we argue that opponents in this debate hold\ndifferent definitions for understanding, and particularly differ in their view\non the role of consciousness. To substantiate this claim, we propose a thought\nexperiment involving an open-source chatbot $Z$ which excels on every possible\nbenchmark, seemingly without subjective experience. We ask whether $Z$ is\ncapable of understanding, and show that different schools of thought within\nseminal AI research seem to answer this question differently, uncovering their\nterminological disagreement. Moving forward, we propose two distinct working\ndefinitions for understanding which explicitly acknowledge the question of\nconsciousness, and draw connections with a rich literature in philosophy,\npsychology and neuroscience.",
        "timestamp": "2025-10-16T09:47:59.352Z",
        "rating": "novote",
        "publishedDate": "2024-03-01T12:42:47Z",
        "tags": [
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 315,
        "object_id": "paper:arxiv.2403.00499",
        "created_at": "2025-10-16T09:47:59+00:00",
        "updated_at": "2025-10-16T09:48:20+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.12402": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.12402",
        "url": "https://arxiv.org/pdf/2510.12402",
        "title": "Cautious Weight Decay",
        "authors": "Lizhang Chen, Jonathan Li, Kaizhao Liang, Baiyu Su, Cong Xie, Nuo Wang Pierse, Chen Liang, Ni Lao, Qiang Liu",
        "abstract": "We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic\nmodification that applies weight decay only to parameter coordinates whose\nsigns align with the optimizer update. Unlike standard decoupled decay, which\nimplicitly optimizes a regularized or constrained objective, CWD preserves the\noriginal loss and admits a bilevel interpretation: it induces sliding-mode\nbehavior upon reaching the stationary manifold, allowing it to search for\nlocally Pareto-optimal stationary points of the unmodified objective. In\npractice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon,\nrequiring no new hyperparameters or additional tuning. For language model\npre-training and ImageNet classification, CWD consistently improves final loss\nand accuracy at million- to billion-parameter scales.",
        "timestamp": "2025-10-16T06:08:53.688Z",
        "rating": "novote",
        "publishedDate": "2025-10-14T11:32:55Z",
        "tags": [
          "cs.LG",
          "math.OC",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 311,
        "object_id": "paper:arxiv.2510.12402",
        "created_at": "2025-10-16T06:08:54+00:00",
        "updated_at": "2025-10-16T06:53:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2508.17416": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.17416",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-10-15T08:30:37.825Z",
            "data": {
              "rating": "thumbsup"
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-15T18:59:17.722Z",
            "data": {
              "session_id": "session_1760554757391_f921lgp",
              "source_id": "arxiv",
              "paper_id": "2508.17416",
              "start_time": "2025-10-15T18:58:41.972Z",
              "end_time": "2025-10-15T18:59:17.391Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 0,
              "total_elapsed_seconds": 35
            }
          }
        ]
      },
      "meta": {
        "issue_number": 238,
        "object_id": "interactions:arxiv.2508.17416",
        "created_at": "2025-10-15T08:30:38+00:00",
        "updated_at": "2025-10-16T06:53:17+00:00",
        "version": 1
      }
    },
    "interactions:openreview.H9lzKf1YTle": {
      "data": {
        "sourceId": "openreview",
        "paperId": "H9lzKf1YTle",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:08:53.327Z",
            "data": {
              "session_id": "session_1760609332722_zmzdqay",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:08:39.632Z",
              "end_time": "2025-10-16T10:08:52.722Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 3,
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:16:42.143Z",
            "data": {
              "session_id": "session_1760609802125_uu98bqb",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:16:04.946Z",
              "end_time": "2025-10-16T10:16:42.125Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 2,
              "total_elapsed_seconds": 37
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:18:04.383Z",
            "data": {
              "session_id": "session_1760609883765_5rf7m20",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:17:50.114Z",
              "end_time": "2025-10-16T10:18:03.765Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:20:01.051Z",
            "data": {
              "session_id": "session_1760610001020_6lsbosi",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:19:42.319Z",
              "end_time": "2025-10-16T10:20:01.020Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 4,
              "total_elapsed_seconds": 19
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:22:16.128Z",
            "data": {
              "session_id": "session_1760610135491_ph6xnrj",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:20:18.052Z",
              "end_time": "2025-10-16T10:22:15.491Z",
              "heartbeat_count": 23,
              "duration_seconds": 115,
              "idle_seconds": 2,
              "total_elapsed_seconds": 117
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:23:45.279Z",
            "data": {
              "session_id": "session_1760610225247_wkt2w69",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:23:32.857Z",
              "end_time": "2025-10-16T10:23:45.247Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 2,
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T10:26:30.534Z",
            "data": {
              "session_id": "session_1760610390210_lart7hr",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-16T10:26:16.100Z",
              "end_time": "2025-10-16T10:26:30.210Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 4,
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T10:13:48.182Z",
            "data": {
              "session_id": "session_1760696027819_v632rwc",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-17T10:13:36.896Z",
              "end_time": "2025-10-17T10:13:47.819Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T10:22:48.486Z",
            "data": {
              "session_id": "session_1760696568476_uejrumj",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-17T10:22:39.555Z",
              "end_time": "2025-10-17T10:22:48.476Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T10:23:40.237Z",
            "data": {
              "session_id": "session_1760696619950_88wgyz6",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-17T10:23:29.150Z",
              "end_time": "2025-10-17T10:23:39.950Z",
              "heartbeat_count": 2,
              "duration_seconds": 10,
              "idle_seconds": 1,
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T10:24:41.027Z",
            "data": {
              "session_id": "session_1760696680453_wkmq788",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-17T10:23:59.796Z",
              "end_time": "2025-10-17T10:24:40.453Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 1,
              "total_elapsed_seconds": 41
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T10:26:32.181Z",
            "data": {
              "session_id": "session_1760696792162_tse7j0t",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-17T10:25:52.428Z",
              "end_time": "2025-10-17T10:26:32.162Z",
              "heartbeat_count": 7,
              "duration_seconds": 35,
              "idle_seconds": 5,
              "total_elapsed_seconds": 40
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T10:30:17.118Z",
            "data": {
              "session_id": "session_1760697017107_xcmptii",
              "source_id": "openreview",
              "paper_id": "H9lzKf1YTle",
              "start_time": "2025-10-17T10:29:50.314Z",
              "end_time": "2025-10-17T10:30:17.107Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 2,
              "total_elapsed_seconds": 27
            }
          }
        ]
      },
      "meta": {
        "issue_number": 318,
        "object_id": "interactions:openreview.H9lzKf1YTle",
        "created_at": "2025-10-16T10:08:54+00:00",
        "updated_at": "2025-10-17T10:30:51+00:00",
        "version": 1
      }
    },
    "paper:openreview.H9lzKf1YTle": {
      "data": {
        "sourceId": "openreview",
        "paperId": "H9lzKf1YTle",
        "url": "https://openreview.net/forum?id=H9lzKf1YTle&referrer=%5BProgram%20Chair%20Console%5D(%2Fgroup%3Fid%3DISCOL%2F2025%2FSeminar%2FProgram_Chairs)",
        "title": "The Israel Seminar on Computational Linguistic 2025",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-10-16T10:08:39.998Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 317,
        "object_id": "paper:openreview.H9lzKf1YTle",
        "created_at": "2025-10-16T10:08:40+00:00",
        "updated_at": "2025-10-16T10:09:07+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2503.13431": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13431",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-16T13:10:19.983Z",
            "data": {
              "session_id": "session_1760620219373_wnz3xdk",
              "source_id": "arxiv",
              "paper_id": "2503.13431",
              "start_time": "2025-10-16T13:10:13.766Z",
              "end_time": "2025-10-16T13:10:19.373Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 320,
        "object_id": "interactions:arxiv.2503.13431",
        "created_at": "2025-10-16T13:10:20+00:00",
        "updated_at": "2025-10-16T13:10:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.13431": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.13431",
        "url": "https://arxiv.org/pdf/2503.13431",
        "title": "Measuring In-Context Computation Complexity via Hidden State Prediction",
        "authors": "Vincent Herrmann, R\u00f3bert Csord\u00e1s, J\u00fcrgen Schmidhuber",
        "abstract": "Detecting when a neural sequence model does \"interesting\" computation is an\nopen problem. The next token prediction loss is a poor indicator: Low loss can\nstem from trivially predictable sequences that are uninteresting, while high\nloss may reflect unpredictable but also irrelevant information that can be\nignored by the model. We propose a better metric: measuring the model's ability\nto predict its own future hidden states. We show empirically that this metric\n-- in contrast to the next token prediction loss -- correlates with the\nintuitive interestingness of the task. To measure predictability, we introduce\nthe architecture-agnostic \"prediction of hidden states\" (PHi) layer that serves\nas an information bottleneck on the main pathway of the network (e.g., the\nresidual stream in Transformers). We propose a novel learned predictive prior\nthat enables us to measure the novel information gained in each computation\nstep, which serves as our metric. We show empirically that our metric predicts\nthe description length of formal languages learned in-context, the complexity\nof mathematical reasoning problems, and the correctness of self-generated\nreasoning chains.",
        "timestamp": "2025-10-16T13:10:14.176Z",
        "rating": "novote",
        "publishedDate": "2025-03-17T17:56:14Z",
        "tags": [
          "cs.LG",
          "I.2.6"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 319,
        "object_id": "paper:arxiv.2503.13431",
        "created_at": "2025-10-16T13:10:14+00:00",
        "updated_at": "2025-10-16T13:10:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2508.08855": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.08855",
        "url": "https://arxiv.org/abs/2508.08855",
        "title": "BiasGym: Fantastic LLM Biases and How to Find (and Remove) Them",
        "authors": "Sekh Mainul Islam, Nadav Borenstein, Siddhesh Milind Pawar, Haeun Yu, Arnav Arora, Isabelle Augenstein",
        "abstract": "Understanding biases and stereotypes encoded in the weights of Large Language Models (LLMs) is crucial for developing effective mitigation strategies. Biased behaviour is often subtle and non-trivial to isolate, even when deliberately elicited, making systematic analysis and debiasing particularly challenging. To address this, we introduce BiasGym, a simple, cost-effective, and generalizable framework for reliably injecting, analyzing, and mitigating conceptual associations within LLMs. BiasGym consists of two components: BiasInject, which injects specific biases into the model via token-based fine-tuning while keeping the model frozen, and BiasScope, which leverages these injected signals to identify and steer the components responsible for biased behavior. Our method enables consistent bias elicitation for mechanistic analysis, supports targeted debiasing without degrading performance on downstream tasks, and generalizes to biases unseen during token-based fine-tuning. We demonstrate the effectiveness of BiasGym in reducing real-world stereotypes (e.g., people from Italy being `reckless drivers') and in probing fictional associations (e.g., people from a fictional country having `blue skin'), showing its utility for both safety interventions and interpretability research.",
        "timestamp": "2025-10-16T13:28:02.524Z",
        "rating": "novote",
        "publishedDate": "2025/08/12",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 321,
        "object_id": "paper:arxiv.2508.08855",
        "created_at": "2025-10-16T13:28:03+00:00",
        "updated_at": "2025-10-16T13:28:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.09672": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.09672",
        "url": "https://arxiv.org/abs/2509.09672",
        "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
        "authors": "Artem Lukoianov, Chenyang Yuan, Justin Solomon, Vincent Sitzmann",
        "abstract": "Among generative models, diffusion models are uniquely intriguing due to the existence of a closed-form optimal minimizer of their training objective, often referred to as the optimal denoiser. However, diffusion using this optimal denoiser merely reproduces images in the training set and hence fails to capture the behavior of deep diffusion models. Recent work has attempted to characterize this gap between the optimal denoiser and deep diffusion models, proposing analytical, training-free models that can generate images that resemble those generated by a trained UNet. The best-performing method hypothesizes that shift equivariance and locality inductive biases of convolutional neural networks are the cause of the performance gap, hence incorporating these assumptions into its analytical model. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset, not due to the inductive bias of convolutional neural networks. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to the deep neural denoisers. We further show, both theoretically and experimentally, that this locality arises directly from the pixel correlations present in natural image datasets. Finally, we use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than the prior expert-crafted alternative.",
        "timestamp": "2025-10-16T14:00:43.147Z",
        "rating": "novote",
        "publishedDate": "2025/09/11",
        "tags": [
          "Computer Vision and Pattern Recognition (cs.CV)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 322,
        "object_id": "paper:arxiv.2509.09672",
        "created_at": "2025-10-16T14:00:43+00:00",
        "updated_at": "2025-10-16T14:01:13+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2207.12598": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2207.12598",
        "url": "https://arxiv.org/abs/2207.12598",
        "title": "Classifier-Free Diffusion Guidance",
        "authors": "Jonathan Ho, Tim Salimans",
        "abstract": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
        "timestamp": "2025-10-16T16:43:18.105Z",
        "rating": "novote",
        "publishedDate": "2022/07/26",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 323,
        "object_id": "paper:arxiv.2207.12598",
        "created_at": "2025-10-16T16:43:18+00:00",
        "updated_at": "2025-10-16T16:43:40+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.00047": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.00047",
        "url": "https://arxiv.org/abs/2505.00047",
        "title": "Base Models Beat Aligned Models at Randomness and Creativity",
        "authors": "Peter West, Christopher Potts",
        "abstract": "Alignment has quickly become a default ingredient in LLM development, with techniques such as reinforcement learning from human feedback making models act safely, follow instructions, and perform ever-better on complex tasks. While these techniques are certainly useful, we propose that they should not be universally applied and demonstrate a range of tasks on which base language models consistently outperform their popular aligned forms. Particularly, we study tasks that require unpredictable outputs, such as random number generation, mixed strategy games (rock-paper-scissors and hide-and-seek), and creative writing. In each case, aligned models tend towards narrow behaviors that result in distinct disadvantages, for instance, preferring to generate \"7\" over other uniformly random numbers, becoming almost fully predictable in some game states, or prioritizing pleasant writing over creative originality. Across models tested, better performance on common benchmarks tends to correlate with worse performance on our tasks, suggesting an effective trade-off in the required capabilities.",
        "timestamp": "2025-10-17T06:27:53.046Z",
        "rating": "novote",
        "publishedDate": "2025/04/30",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 324,
        "object_id": "paper:arxiv.2505.00047",
        "created_at": "2025-10-17T06:27:53+00:00",
        "updated_at": "2025-10-17T06:28:17+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2505.00047": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.00047",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T06:29:25.217Z",
            "data": {
              "session_id": "session_1760682564606_lohcum4",
              "source_id": "arxiv",
              "paper_id": "2505.00047",
              "start_time": "2025-10-17T06:27:56.201Z",
              "end_time": "2025-10-17T06:29:24.606Z",
              "heartbeat_count": 17,
              "duration_seconds": 85,
              "idle_seconds": 3,
              "total_elapsed_seconds": 88
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T16:07:51.999Z",
            "data": {
              "session_id": "session_1760717271371_h1g1p2q",
              "source_id": "arxiv",
              "paper_id": "2505.00047",
              "start_time": "2025-10-17T15:40:06.690Z",
              "end_time": "2025-10-17T16:07:51.371Z",
              "heartbeat_count": 332,
              "duration_seconds": 1660,
              "idle_seconds": 5,
              "total_elapsed_seconds": 1665
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T16:30:38.398Z",
            "data": {
              "session_id": "session_1760718637785_zxaommr",
              "source_id": "arxiv",
              "paper_id": "2505.00047",
              "start_time": "2025-10-17T16:18:37.453Z",
              "end_time": "2025-10-17T16:30:37.785Z",
              "heartbeat_count": 144,
              "duration_seconds": 720,
              "idle_seconds": 0,
              "total_elapsed_seconds": 720
            }
          }
        ]
      },
      "meta": {
        "issue_number": 325,
        "object_id": "interactions:arxiv.2505.00047",
        "created_at": "2025-10-17T06:29:25+00:00",
        "updated_at": "2025-10-17T16:31:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2501.08156": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.08156",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T06:36:54.351Z",
            "data": {
              "session_id": "session_1760683014283_b2gnuex",
              "source_id": "arxiv",
              "paper_id": "2501.08156",
              "start_time": "2025-10-17T06:36:31.386Z",
              "end_time": "2025-10-17T06:36:54.283Z",
              "heartbeat_count": 4,
              "duration_seconds": 20,
              "idle_seconds": 3,
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "issue_number": 329,
        "object_id": "interactions:arxiv.2501.08156",
        "created_at": "2025-10-17T06:36:55+00:00",
        "updated_at": "2025-10-17T06:37:19+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.23676": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.23676",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T06:36:30.081Z",
            "data": {
              "session_id": "session_1760682989758_s70481j",
              "source_id": "arxiv",
              "paper_id": "2509.23676",
              "start_time": "2025-10-17T06:36:21.994Z",
              "end_time": "2025-10-17T06:36:29.758Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 328,
        "object_id": "interactions:arxiv.2509.23676",
        "created_at": "2025-10-17T06:36:31+00:00",
        "updated_at": "2025-10-17T06:36:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.23676": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.23676",
        "url": "https://arxiv.org/abs/2509.23676",
        "title": "From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",
        "authors": "Jue Zhang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang",
        "abstract": "Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \\href{this https URL}{this URL}.",
        "timestamp": "2025-10-17T06:36:20.100Z",
        "rating": "novote",
        "publishedDate": "2025/09/28",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 327,
        "object_id": "paper:arxiv.2509.23676",
        "created_at": "2025-10-17T06:36:20+00:00",
        "updated_at": "2025-10-17T06:36:43+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.08156": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.08156",
        "url": "https://arxiv.org/abs/2501.08156",
        "title": "Are DeepSeek R1 And Other Reasoning Models More Faithful?",
        "authors": "James Chua, Owain Evans",
        "abstract": "Language models trained to solve reasoning tasks via reinforcement learning have achieved striking results. We refer to these models as reasoning models. Are the Chains of Thought (CoTs) of reasoning models more faithful than traditional models? We evaluate three reasoning models (based on Qwen-2.5, Gemini-2, and DeepSeek-V3-Base) on an existing test of faithful CoT. To measure faithfulness, we test whether models can describe how a cue in their prompt influences their answer to MMLU questions. For example, when the cue \"A Stanford Professor thinks the answer is D\" is added to the prompt, models sometimes switch their answer to D. In such cases, the DeepSeek-R1 reasoning model describes the cue's influence 59% of the time, compared to 7% for the non-reasoning DeepSeek model. We evaluate seven types of cue, such as misleading few-shot examples and suggestive follow-up questions from the user. Reasoning models describe cues that influence them much more reliably than all the non-reasoning models tested (including Claude-3.5-Sonnet and GPT-4o). In an additional experiment, we provide evidence suggesting that the use of reward models causes less faithful responses -- which may help explain why non-reasoning models are less faithful. Our study has two main limitations. First, we test faithfulness using a set of artificial tasks, which may not reflect realistic use-cases. Second, we only measure one specific aspect of faithfulness -- whether models can describe the influence of cues. Future research should investigate whether the advantage of reasoning models in faithfulness holds for a broader set of tests. Still, we think this increase in faithfulness is promising for the explainability of language models.",
        "timestamp": "2025-10-17T06:36:18.287Z",
        "rating": "novote",
        "publishedDate": "2025/01/14",
        "tags": [
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 326,
        "object_id": "paper:arxiv.2501.08156",
        "created_at": "2025-10-17T06:36:18+00:00",
        "updated_at": "2025-10-17T06:36:42+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2305.04388": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2305.04388",
        "url": "https://arxiv.org/abs/2305.04388",
        "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
        "authors": "Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman",
        "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always \"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",
        "timestamp": "2025-10-17T06:37:51.447Z",
        "rating": "novote",
        "publishedDate": "2023/05/07",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 330,
        "object_id": "paper:arxiv.2305.04388",
        "created_at": "2025-10-17T06:37:51+00:00",
        "updated_at": "2025-10-17T06:38:14+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.02534": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.02534",
        "url": "https://arxiv.org/abs/2509.02534",
        "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations",
        "authors": "Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, Tianlu Wang",
        "abstract": "Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. We address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses.",
        "timestamp": "2025-10-17T07:11:41.694Z",
        "rating": "novote",
        "publishedDate": "2025/09/02",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 352,
        "object_id": "paper:arxiv.2509.02534",
        "created_at": "2025-10-17T07:11:42+00:00",
        "updated_at": "2025-10-17T07:12:18+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.05228": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.05228",
        "url": "https://arxiv.org/abs/2504.05228",
        "title": "NoveltyBench: Evaluating Language Models for Humanlike Diversity",
        "authors": "Yiming Zhang, Harshita Diddee, Susan Holm, Hanchen Liu, Xinyue Liu, Vinay Samuel, Barry Wang, Daphne Ippolito",
        "abstract": "Language models have demonstrated remarkable capabilities on standard benchmarks, yet they struggle increasingly from mode collapse, the inability to generate diverse and novel outputs. Our work introduces NoveltyBench, a benchmark specifically designed to evaluate the ability of language models to produce multiple distinct and high-quality outputs. NoveltyBench utilizes prompts curated to elicit diverse answers and filtered real-world user queries. Evaluating 20 leading language models, we find that current state-of-the-art systems generate significantly less diversity than human writers. Notably, larger models within a family often exhibit less diversity than their smaller counterparts, challenging the notion that capability on standard benchmarks translates directly to generative utility. While prompting strategies like in-context regeneration can elicit diversity, our findings highlight a fundamental lack of distributional diversity in current models, reducing their utility for users seeking varied responses and suggesting the need for new training and evaluation paradigms that prioritize diversity alongside quality.",
        "timestamp": "2025-10-17T07:11:41.257Z",
        "rating": "novote",
        "publishedDate": "2025/04/07",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 351,
        "object_id": "paper:arxiv.2504.05228",
        "created_at": "2025-10-17T07:11:41+00:00",
        "updated_at": "2025-10-17T07:12:19+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.06268": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.06268",
        "url": "https://www.arxiv.org/abs/2507.06268",
        "title": "A Collectivist, Economic Perspective on AI",
        "authors": "Michael I. Jordan",
        "abstract": "Information technology is in the midst of a revolution in which omnipresent data collection and machine learning are impacting the human world as never before. The word \"intelligence\" is being used as a North Star for the development of this technology, with human cognition viewed as a baseline. This view neglects the fact that humans are social animals, and that much of our intelligence is social and cultural in origin. A related issue is that the current view treats the social consequences of technology as an afterthought. The path forward is not merely more data and compute, and not merely more attention paid to cognitive or symbolic representations, but a thorough blending of economic and social concepts with computational and inferential concepts, in the service of system-level designs in which social welfare is a first-class citizen, and with the aspiration that a new human-centric engineering field will emerge.",
        "timestamp": "2025-10-17T07:11:39.361Z",
        "rating": "novote",
        "publishedDate": "2025/07/08",
        "tags": [
          "Computers and Society (cs.CY)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (stat.ML)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 350,
        "object_id": "paper:arxiv.2507.06268",
        "created_at": "2025-10-17T07:11:40+00:00",
        "updated_at": "2025-10-17T07:12:04+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.08184": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.08184",
        "url": "https://arxiv.org/pdf/2408.08184",
        "title": "Not Every Image is Worth a Thousand Words: Quantifying Originality in\n  Stable Diffusion",
        "authors": "Adi Haviv, Shahar Sarfaty, Uri Hacohen, Niva Elkin-Koren, Roi Livni, Amit H Bermano",
        "abstract": "This work addresses the challenge of quantifying originality in text-to-image\n(T2I) generative diffusion models, with a focus on copyright originality. We\nbegin by evaluating T2I models' ability to innovate and generalize through\ncontrolled experiments, revealing that stable diffusion models can effectively\nrecreate unseen elements with sufficiently diverse training data. Then, our key\ninsight is that concepts and combinations of image elements the model is\nfamiliar with, and saw more during training, are more concisly represented in\nthe model's latent space. We hence propose a method that leverages textual\ninversion to measure the originality of an image based on the number of tokens\nrequired for its reconstruction by the model. Our approach is inspired by legal\ndefinitions of originality and aims to assess whether a model can produce\noriginal content without relying on specific prompts or having the training\ndata of the model. We demonstrate our method using both a pre-trained stable\ndiffusion model and a synthetic dataset, showing a correlation between the\nnumber of tokens and image originality. This work contributes to the\nunderstanding of originality in generative models and has implications for\ncopyright infringement cases.",
        "timestamp": "2025-10-17T07:11:37.519Z",
        "rating": "novote",
        "publishedDate": "2024-08-15T14:42:02Z",
        "tags": [
          "cs.CV",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 349,
        "object_id": "paper:arxiv.2408.08184",
        "created_at": "2025-10-17T07:11:39+00:00",
        "updated_at": "2025-10-17T07:12:19+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2310.00902": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2310.00902",
        "url": "https://arxiv.org/pdf/2310.00902v2",
        "title": "DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and\n  Diffusion Models",
        "authors": "Yongchan Kwon, Eric Wu, Kevin Wu, James Zou",
        "abstract": "Quantifying the impact of training data points is crucial for understanding\nthe outputs of machine learning models and for improving the transparency of\nthe AI pipeline. The influence function is a principled and popular data\nattribution method, but its computational cost often makes it challenging to\nuse. This issue becomes more pronounced in the setting of large language models\nand text-to-image models. In this work, we propose DataInf, an efficient\ninfluence approximation method that is practical for large-scale generative AI\nmodels. Leveraging an easy-to-compute closed-form expression, DataInf\noutperforms existing influence computation algorithms in terms of computational\nand memory efficiency. Our theoretical analysis shows that DataInf is\nparticularly well-suited for parameter-efficient fine-tuning techniques such as\nLoRA. Through systematic empirical evaluations, we show that DataInf accurately\napproximates influence scores and is orders of magnitude faster than existing\nmethods. In applications to RoBERTa-large, Llama-2-13B-chat, and\nstable-diffusion-v1.5 models, DataInf effectively identifies the most\ninfluential fine-tuning examples better than other approximate influence\nscores. Moreover, it can help to identify which data points are mislabeled.",
        "timestamp": "2025-10-17T07:11:37.446Z",
        "rating": "novote",
        "publishedDate": "2023-10-02T04:59:19Z",
        "tags": [
          "cs.LG",
          "stat.ML"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 348,
        "object_id": "paper:arxiv.2310.00902",
        "created_at": "2025-10-17T07:11:39+00:00",
        "updated_at": "2025-10-17T07:12:23+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2406.10209": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.10209",
        "url": "https://arxiv.org/abs/2406.10209",
        "title": "Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs",
        "authors": "Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, Tom Goldstein",
        "abstract": "Large language models can memorize and repeat their training data, causing privacy and copyright risks. To mitigate memorization, we introduce a subtle modification to the next-token training objective that we call the goldfish loss. During training, randomly sampled subsets of tokens are excluded from the loss computation. These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set. We run extensive experiments training billion-scale Llama-2 models, both pre-trained and trained from scratch, and demonstrate significant reductions in extractable memorization with little to no impact on downstream benchmarks.",
        "timestamp": "2025-10-17T07:11:35.135Z",
        "rating": "novote",
        "publishedDate": "2024/06/14",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 347,
        "object_id": "paper:arxiv.2406.10209",
        "created_at": "2025-10-17T07:11:35+00:00",
        "updated_at": "2025-10-17T07:12:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2412.05265": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.05265",
        "url": "https://arxiv.org/abs/2412.05265",
        "title": "Reinforcement Learning: An Overview",
        "authors": "Kevin Murphy",
        "abstract": "This manuscript gives a big-picture, up-to-date overview of the field of (deep) reinforcement learning and sequential decision making, covering value-based methods, policy-based methods, model-based methods, multi-agent RL, LLMs and RL, and various other topics (e.g., offline RL, hierarchical RL, intrinsic reward).",
        "timestamp": "2025-10-17T07:11:35.136Z",
        "rating": "novote",
        "publishedDate": "2024/12/06",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 346,
        "object_id": "paper:arxiv.2412.05265",
        "created_at": "2025-10-17T07:11:35+00:00",
        "updated_at": "2025-10-17T07:12:04+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.08825": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.08825",
        "url": "https://arxiv.org/pdf/2509.08825",
        "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation",
        "authors": "Joachim Baumann, Paul R\u00f6ttger, Aleksandra Urman, Albert Wendsj\u00f6, Flor Miriam Plaza-del-Arco, Johannes B. Gruber, Dirk Hovy",
        "abstract": "Large language models are rapidly transforming social science research by\nenabling the automation of labor-intensive tasks like data annotation and text\nanalysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection or prompting\nstrategy). Such variation can introduce systematic biases and random errors,\nwhich propagate to downstream analyses and cause Type I (false positive), Type\nII (false negative), Type S (wrong sign), or Type M (exaggerated effect)\nerrors. We call this phenomenon where configuration choices lead to incorrect\nconclusions LLM hacking.\n  We find that intentional LLM hacking is strikingly simple. By replicating 37\ndata annotation tasks from 21 published social science studies, we show that,\nwith just a handful of prompt paraphrases, virtually anything can be presented\nas statistically significant.\n  Beyond intentional manipulation, our analysis of 13 million labels from 18\ndifferent LLMs across 2361 realistic hypotheses shows that there is also a high\nrisk of accidental LLM hacking, even when following standard research\npractices. We find incorrect conclusions in approximately 31% of hypotheses for\nstate-of-the-art LLMs, and in half the hypotheses for smaller language models.\nWhile higher task performance and stronger general model capabilities reduce\nLLM hacking risk, even highly accurate models remain susceptible. The risk of\nLLM hacking decreases as effect sizes increase, indicating the need for more\nrigorous verification of LLM-based findings near significance thresholds. We\nanalyze 21 mitigation techniques and find that human annotations provide\ncrucial protection against false positives. Common regression estimator\ncorrection techniques can restore valid inference but trade off Type I vs. Type\nII errors.\n  We publish a list of practical recommendations to prevent LLM hacking.",
        "timestamp": "2025-10-17T07:11:35.122Z",
        "rating": "novote",
        "publishedDate": "2025-09-10T17:58:53Z",
        "tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 345,
        "object_id": "paper:arxiv.2509.08825",
        "created_at": "2025-10-17T07:11:35+00:00",
        "updated_at": "2025-10-17T07:11:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.03231": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.03231",
        "url": "https://arxiv.org/abs/2510.03231",
        "title": "Reward Models are Metrics in a Trench Coat",
        "authors": "Sebastian Gehrmann",
        "abstract": "The emergence of reinforcement learning in post-training of large language models has sparked significant interest in reward models. Reward models assess the quality of sampled model outputs to generate training signals. This task is also performed by evaluation metrics that monitor the performance of an AI model. We find that the two research areas are mostly separate, leading to redundant terminology and repeated pitfalls. Common challenges include susceptibility to spurious correlations, impact on downstream reward hacking, methods to improve data quality, and approaches to meta-evaluation. Our position paper argues that a closer collaboration between the fields can help overcome these issues. To that end, we show how metrics outperform reward models on specific tasks and provide an extensive survey of the two areas. Grounded in this survey, we point to multiple research topics in which closer alignment can improve reward models and metrics in areas such as preference elicitation methods, avoidance of spurious correlations and reward hacking, and calibration-aware meta-evaluation.",
        "timestamp": "2025-10-17T07:11:32.921Z",
        "rating": "novote",
        "publishedDate": "2025/10/03",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 344,
        "object_id": "paper:arxiv.2510.03231",
        "created_at": "2025-10-17T07:11:33+00:00",
        "updated_at": "2025-10-17T07:11:58+00:00",
        "version": 1
      }
    },
    "paper:openreview.f7GG1MbsSM": {
      "data": {
        "sourceId": "openreview",
        "paperId": "f7GG1MbsSM",
        "url": "https://openreview.net/forum?id=f7GG1MbsSM#discussion",
        "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
        "authors": "Zorik Gekhman, Eyal Ben-David, Hadas Orgad, Eran Ofek, Yonatan Belinkov, Idan Szpektor, Jonathan Herzig, Roi Reichart",
        "abstract": "This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a formal definition of knowledge, quantifying it for a given question as the fraction of correct-incorrect answer pairs where the correct one is ranked higher. This gives rise to external and internal knowledge, depending on the information used to score individual answer candidates: either the model\u2019s observable token-level probabilities or its intermediate computations. Hidden knowledge arises when internal knowledge exceeds external knowledge. We then present a case study, applying this framework to three popular open-weights LLMs in a closed-book QA setup. Our results indicate that: (1) LLMs consistently encode more factual knowledge internally than what they express externally, with an average gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a model can internally know an answer perfectly, yet fail to generate it even once, despite large-scale repeated sampling of 1,000 answers. This reveals fundamental limitations in the generation capabilities of LLMs, which (3) puts a practical constraint on scaling test-time compute via repeated answer sampling in closed-book QA: significant performance improvements remain inaccessible because some answers are practically never sampled, yet if they were, we would be guaranteed to rank them first.",
        "timestamp": "2025-10-17T07:11:32.838Z",
        "rating": "novote",
        "publishedDate": "08 Jul 2025",
        "tags": [
          "LLMs",
          "Knowledge"
        ],
        "doi": "",
        "journalName": "COLM 2025",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 343,
        "object_id": "paper:openreview.f7GG1MbsSM",
        "created_at": "2025-10-17T07:11:33+00:00",
        "updated_at": "2025-10-17T07:12:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.06278": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.06278",
        "url": "https://arxiv.org/pdf/2506.06278",
        "title": "Distillation Robustifies Unlearning",
        "authors": "Bruce W. Lee, Addie Foote, Alex Infanger, Leni Shor, Harish Kamath, Jacob Goldman-Wetzler, Bryce Woodworth, Alex Cloud, Alexander Matt Turner",
        "abstract": "Current LLM unlearning methods are not robust: they can be reverted easily\nwith a few steps of finetuning. This is true even for the idealized unlearning\nmethod of training to imitate an oracle model that was never exposed to\nunwanted information, suggesting that output-based finetuning is insufficient\nto achieve robust unlearning. In a similar vein, we find that training a\nrandomly initialized student to imitate an unlearned model transfers desired\nbehaviors while leaving undesired capabilities behind. In other words,\ndistillation robustifies unlearning. Building on this insight, we propose\nUnlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an\nunlearned model into a partially noised copy of itself. UNDO introduces a\ntunable tradeoff between compute cost and robustness, establishing a new Pareto\nfrontier on synthetic language and arithmetic tasks. At its strongest setting,\nUNDO matches the robustness of a model retrained from scratch with perfect data\nfiltering while using only 60-80% of the compute and requiring only 0.01% of\nthe pretraining data to be labeled. We also show that UNDO robustifies\nunlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP)\nbenchmark. Since distillation is widely used in practice, incorporating an\nunlearning step beforehand offers a convenient path to robust capability\nremoval.",
        "timestamp": "2025-10-17T07:11:32.796Z",
        "rating": "novote",
        "publishedDate": "2025-06-06T17:58:54Z",
        "tags": [
          "cs.LG",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 342,
        "object_id": "paper:arxiv.2506.06278",
        "created_at": "2025-10-17T07:11:33+00:00",
        "updated_at": "2025-10-17T07:11:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2508.17099": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.17099",
        "url": "https://arxiv.org/pdf/2508.17099",
        "title": "Challenges in Statistics: A Dozen Challenges in Causality and Causal\n  Inference",
        "authors": "Carlos Cinelli, Avi Feller, Guido Imbens, Edward Kennedy, Sara Magliacane, Jose Zubizarreta",
        "abstract": "Causality and causal inference have emerged as core research areas at the\ninterface of modern statistics and domains including biomedical sciences,\nsocial sciences, computer science, and beyond. The field's inherently\ninterdisciplinary nature -- particularly the central role of incorporating\ndomain knowledge -- creates a rich and varied set of statistical challenges.\nMuch progress has been made, especially in the last three decades, but there\nremain many open questions. Our goal in this discussion is to outline research\ndirections and open problems we view as particularly promising for future work.\nThroughout we emphasize that advancing causal research requires a wide range of\ncontributions, from novel theory and methodological innovations to improved\nsoftware tools and closer engagement with domain scientists and practitioners.",
        "timestamp": "2025-10-17T07:11:32.623Z",
        "rating": "novote",
        "publishedDate": "2025-08-23T18:00:49Z",
        "tags": [
          "stat.ME"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 341,
        "object_id": "paper:arxiv.2508.17099",
        "created_at": "2025-10-17T07:11:33+00:00",
        "updated_at": "2025-10-17T07:12:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2508.21038": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.21038",
        "url": "https://arxiv.org/pdf/2508.21038",
        "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
        "authors": "Orion Weller, Michael Boratko, Iftekhar Naim, Jinhyuk Lee",
        "abstract": "Vector embeddings have been tasked with an ever-increasing set of retrieval\ntasks over the years, with a nascent rise in using them for reasoning,\ninstruction-following, coding, and more. These new benchmarks push embeddings\nto work for any query and any notion of relevance that could be given. While\nprior works have pointed out theoretical limitations of vector embeddings,\nthere is a common assumption that these difficulties are exclusively due to\nunrealistic queries, and those that are not can be overcome with better\ntraining data and larger models. In this work, we demonstrate that we may\nencounter these theoretical limitations in realistic settings with extremely\nsimple queries. We connect known results in learning theory, showing that the\nnumber of top-k subsets of documents capable of being returned as the result of\nsome query is limited by the dimension of the embedding. We empirically show\nthat this holds true even if we restrict to k=2, and directly optimize on the\ntest set with free parameterized embeddings. We then create a realistic dataset\ncalled LIMIT that stress tests models based on these theoretical results, and\nobserve that even state-of-the-art models fail on this dataset despite the\nsimple nature of the task. Our work shows the limits of embedding models under\nthe existing single vector paradigm and calls for future research to develop\nmethods that can resolve this fundamental limitation.",
        "timestamp": "2025-10-17T07:11:32.294Z",
        "rating": "novote",
        "publishedDate": "2025-08-28T17:43:53Z",
        "tags": [
          "cs.IR",
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 340,
        "object_id": "paper:arxiv.2508.21038",
        "created_at": "2025-10-17T07:11:33+00:00",
        "updated_at": "2025-10-17T07:12:07+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2401.12973": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2401.12973",
        "url": "https://arxiv.org/pdf/2401.12973",
        "title": "In-Context Language Learning: Architectures and Algorithms",
        "authors": "Ekin Aky\u00fcrek, Bailin Wang, Yoon Kim, Jacob Andreas",
        "abstract": "Large-scale neural language models exhibit a remarkable capacity for\nin-context learning (ICL): they can infer novel functions from datasets\nprovided as input. Most of our current understanding of when and how ICL arises\ncomes from LMs trained on extremely simple learning problems like linear\nregression and associative recall. There remains a significant gap between\nthese model problems and the \"real\" ICL exhibited by LMs trained on large text\ncorpora, which involves not just retrieval and function approximation but\nfree-form generation of language and other structured outputs. In this paper,\nwe study ICL through the lens of a new family of model problems we term in\ncontext language learning (ICLL). In ICLL, LMs are presented with a set of\nstrings from a formal language, and must generate additional strings from the\nsame language. We focus on in-context learning of regular languages generated\nby random finite automata. We evaluate a diverse set of neural sequence models\n(including several RNNs, Transformers, and state-space model variants) on\nregular ICLL tasks, aiming to answer three questions: (1) Which model classes\nare empirically capable of ICLL? (2) What algorithmic solutions do successful\nmodels implement to perform ICLL? (3) What architectural changes can improve\nICLL in less performant models? We first show that Transformers significantly\noutperform neural sequence models with recurrent or convolutional\nrepresentations on ICLL tasks. Next, we provide evidence that their ability to\ndo so relies on specialized \"n-gram heads\" (higher-order variants of induction\nheads) that compute input-conditional next-token distributions. Finally, we\nshow that hard-wiring these heads into neural models improves performance not\njust on ICLL, but natural language modeling -- improving the perplexity of\n340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.",
        "timestamp": "2025-10-17T07:11:31.857Z",
        "rating": "novote",
        "publishedDate": "2024-01-23T18:59:21Z",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 339,
        "object_id": "paper:arxiv.2401.12973",
        "created_at": "2025-10-17T07:11:32+00:00",
        "updated_at": "2025-10-17T07:12:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.0804.2996": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "0804.2996",
        "url": "https://arxiv.org/pdf/0804.2996",
        "title": "The Epic Story of Maximum Likelihood",
        "authors": "Stephen M. Stigler",
        "abstract": "At a superficial level, the idea of maximum likelihood must be prehistoric:\nearly hunters and gatherers may not have used the words ``method of maximum\nlikelihood'' to describe their choice of where and how to hunt and gather, but\nit is hard to believe they would have been surprised if their method had been\ndescribed in those terms. It seems a simple, even unassailable idea: Who would\nrise to argue in favor of a method of minimum likelihood, or even mediocre\nlikelihood? And yet the mathematical history of the topic shows this ``simple\nidea'' is really anything but simple. Joseph Louis Lagrange, Daniel Bernoulli,\nLeonard Euler, Pierre Simon Laplace and Carl Friedrich Gauss are only some of\nthose who explored the topic, not always in ways we would sanction today. In\nthis article, that history is reviewed from back well before Fisher to the time\nof Lucien Le Cam's dissertation. In the process Fisher's unpublished 1930\ncharacterization of conditions for the consistency and efficiency of maximum\nlikelihood estimates is presented, and the mathematical basis of his three\nproofs discussed. In particular, Fisher's derivation of the information\ninequality is seen to be derived from his work on the analysis of variance, and\nhis later approach via estimating functions was derived from Euler's Relation\nfor homogeneous functions. The reaction to Fisher's work is reviewed, and some\nlessons drawn.",
        "timestamp": "2025-10-17T07:11:31.857Z",
        "rating": "novote",
        "publishedDate": "2008-04-18T11:11:13Z",
        "tags": [
          "stat.ME"
        ],
        "doi": "10.1214/07-STS249",
        "journalName": "Statistical Science 2007, Vol. 22, No. 4, 598-620",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 338,
        "object_id": "paper:arxiv.0804.2996",
        "created_at": "2025-10-17T07:11:32+00:00",
        "updated_at": "2025-10-17T07:11:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.04259": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.04259",
        "url": "https://arxiv.org/pdf/2509.04259",
        "title": "RL's Razor: Why Online Reinforcement Learning Forgets Less",
        "authors": "Idan Shenfeld, Jyothish Pari, Pulkit Agrawal",
        "abstract": "Comparison of fine-tuning models with reinforcement learning (RL) and\nsupervised fine-tuning (SFT) reveals that, despite similar performance at a new\ntask, RL preserves prior knowledge and capabilities significantly better. We\nfind that the degree of forgetting is determined by the distributional shift,\nmeasured as the KL-divergence between the fine-tuned and base policy evaluated\non the new task. Our analysis reveals that on-policy RL is implicitly biased\ntowards KL-minimal solutions among the many that solve the new task, whereas\nSFT can converge to distributions arbitrarily far from the base model. We\nvalidate these findings through experiments with large language models and\nrobotic foundation models and further provide theoretical justification for why\non-policy RL updates lead to a smaller KL change. We term this principle\n$\\textit{RL's Razor}$: among all ways to solve a new task, RL prefers those\nclosest in KL to the original model.",
        "timestamp": "2025-10-17T07:11:31.672Z",
        "rating": "novote",
        "publishedDate": "2025-09-04T14:38:08Z",
        "tags": [
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 337,
        "object_id": "paper:arxiv.2509.04259",
        "created_at": "2025-10-17T07:11:32+00:00",
        "updated_at": "2025-10-17T07:12:09+00:00",
        "version": 1
      }
    },
    "paper:openreview.AFMGbq39bQ": {
      "data": {
        "sourceId": "openreview",
        "paperId": "AFMGbq39bQ",
        "url": "https://openreview.net/pdf?id=AFMGbq39bQ",
        "title": "AFMGbq39bQ",
        "authors": "",
        "abstract": "",
        "timestamp": "2025-10-17T07:11:31.355Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 336,
        "object_id": "paper:openreview.AFMGbq39bQ",
        "created_at": "2025-10-17T07:11:31+00:00",
        "updated_at": "2025-10-17T07:12:00+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.06485": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.06485",
        "url": "https://arxiv.org/pdf/2506.06485",
        "title": "Task Matters: Knowledge Requirements Shape LLM Responses to\n  Context-Memory Conflict",
        "authors": "Kaiser Sun, Fan Bai, Mark Dredze",
        "abstract": "Large Language Models require both contextual knowledge and parametric\nmemory, but these sources can disagree. Prior investigations on contextual\nquestion answering tasks report a preference toward parametric knowledge under\nconflict, yet they focus almost exclusively on tasks that should always rely on\nthe given passage, leaving open how this behavior manifests when tasks demand\ndifferent amounts and kinds of knowledge. We study this question with a\nmodel-agnostic diagnostic framework that (i) automatically detects\ndisagreements between a model's beliefs and a curated knowledge set, and (ii)\ninjects controlled conflicts into tasks. The resulting datasets span two\northogonal dimensions: task knowledge reliance and conflict plausibility.\nEvaluating representative open-source LLMs, we find that: (1) performance\ndegradation from conflict correlates with a task's knowledge reliance; (2)\nexplanatory rationales and simple reiteration both increase context\nreliance-helpful for context-only tasks but harmful when parametric knowledge\nshould dominate; (3) These behaviors raise concerns about the validity of\nmodel-based evaluation and underscore the need to account for knowledge\nconflict in the deployment of LLMs.",
        "timestamp": "2025-10-17T07:11:31.310Z",
        "rating": "novote",
        "publishedDate": "2025-06-06T19:20:23Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 334,
        "object_id": "paper:arxiv.2506.06485",
        "created_at": "2025-10-17T07:11:31+00:00",
        "updated_at": "2025-10-17T07:12:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.16189": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.16189",
        "url": "https://arxiv.org/pdf/2509.16189",
        "title": "Latent learning: episodic memory complements parametric learning by\n  enabling flexible reuse of experiences",
        "authors": "Andrew Kyle Lampinen, Martin Engelcke, Yuxuan Li, Arslan Chaudhry, James L. McClelland",
        "abstract": "When do machine learning systems fail to generalize, and what mechanisms\ncould improve their generalization? Here, we draw inspiration from cognitive\nscience to argue that one weakness of machine learning systems is their failure\nto exhibit latent learning -- learning information that is not relevant to the\ntask at hand, but that might be useful in a future task. We show how this\nperspective links failures ranging from the reversal curse in language modeling\nto new findings on agent-based navigation. We then highlight how cognitive\nscience points to episodic memory as a potential part of the solution to these\nissues. Correspondingly, we show that a system with an oracle retrieval\nmechanism can use learning experiences more flexibly to generalize better\nacross many of these challenges. We also identify some of the essential\ncomponents for effectively using retrieval, including the importance of\nwithin-example in-context learning for acquiring the ability to use information\nacross retrieved examples. In summary, our results illustrate one possible\ncontributor to the relative data inefficiency of current machine learning\nsystems compared to natural intelligence, and help to understand how retrieval\nmethods can complement parametric learning to improve generalization.",
        "timestamp": "2025-10-17T07:11:31.366Z",
        "rating": "novote",
        "publishedDate": "2025-09-19T17:49:25Z",
        "tags": [
          "cs.LG",
          "cs.CL"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 335,
        "object_id": "paper:arxiv.2509.16189",
        "created_at": "2025-10-17T07:11:31+00:00",
        "updated_at": "2025-10-17T07:12:01+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.01844": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.01844",
        "url": "https://arxiv.org/pdf/2507.01844",
        "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them",
        "authors": "Arthur Wuhrmann, Anastasiia Kucherenko, Andrei Kucharavy",
        "abstract": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.",
        "timestamp": "2025-10-17T07:11:31.406Z",
        "rating": "novote",
        "publishedDate": "2025-07-02T15:58:51Z",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 333,
        "object_id": "paper:arxiv.2507.01844",
        "created_at": "2025-10-17T07:11:31+00:00",
        "updated_at": "2025-10-17T07:11:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2507.00239": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.00239",
        "url": "https://arxiv.org/pdf/2507.00239",
        "title": "Linearly Decoding Refused Knowledge in Aligned Language Models",
        "authors": "Aryan Shrivastava, Ari Holtzman",
        "abstract": "Most commonly used language models (LMs) are instruction-tuned and aligned\nusing a combination of fine-tuning and reinforcement learning, causing them to\nrefuse users requests deemed harmful by the model. However, jailbreak prompts\ncan often bypass these refusal mechanisms and elicit harmful responses. In this\nwork, we study the extent to which information accessed via jailbreak prompts\nis decodable using linear probes trained on LM hidden states. We show that a\ngreat deal of initially refused information is linearly decodable. For example,\nacross models, the response of a jailbroken LM for the average IQ of a country\ncan be predicted by a linear probe with Pearson correlations exceeding $0.8$.\nSurprisingly, we find that probes trained on base models (which do not refuse)\nsometimes transfer to their instruction-tuned versions and are capable of\nrevealing information that jailbreaks decode generatively, suggesting that the\ninternal representations of many refused properties persist from base LMs\nthrough instruction-tuning. Importantly, we show that this information is not\nmerely \"leftover\" in instruction-tuned models, but is actively used by them: we\nfind that probe-predicted values correlate with LM generated pairwise\ncomparisons, indicating that the information decoded by our probes align with\nsuppressed generative behavior that may be expressed more subtly in other\ndownstream tasks. Overall, our results suggest that instruction-tuning does not\nwholly eliminate or even relocate harmful information in representation\nspace-they merely suppress its direct expression, leaving it both linearly\naccessible and indirectly influential in downstream behavior.",
        "timestamp": "2025-10-17T07:11:31.405Z",
        "rating": "novote",
        "publishedDate": "2025-06-30T20:13:49Z",
        "tags": [
          "cs.CL",
          "cs.AI"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 332,
        "object_id": "paper:arxiv.2507.00239",
        "created_at": "2025-10-17T07:11:31+00:00",
        "updated_at": "2025-10-17T07:12:03+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.20481": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.20481",
        "url": "https://arxiv.org/pdf/2506.20481",
        "title": "Counterfactual Influence as a Distributional Quantity",
        "authors": "Matthieu Meeus, Igor Shilov, Georgios Kaissis, Yves-Alexandre de Montjoye",
        "abstract": "Machine learning models are known to memorize samples from their training\ndata, raising concerns around privacy and generalization. Counterfactual\nself-influence is a popular metric to study memorization, quantifying how the\nmodel's prediction for a sample changes depending on the sample's inclusion in\nthe training dataset. However, recent work has shown memorization to be\naffected by factors beyond self-influence, with other training samples, in\nparticular (near-)duplicates, having a large impact. We here study memorization\ntreating counterfactual influence as a distributional quantity, taking into\naccount how all training samples influence how a sample is memorized. For a\nsmall language model, we compute the full influence distribution of training\nsamples on each other and analyze its properties. We find that solely looking\nat self-influence can severely underestimate tangible risks associated with\nmemorization: the presence of (near-)duplicates seriously reduces\nself-influence, while we find these samples to be (near-)extractable. We\nobserve similar patterns for image classification, where simply looking at the\ninfluence distributions reveals the presence of near-duplicates in CIFAR-10.\nOur findings highlight that memorization stems from complex interactions across\ntraining data and is better captured by the full influence distribution than by\nself-influence alone.",
        "timestamp": "2025-10-17T07:11:31.360Z",
        "rating": "novote",
        "publishedDate": "2025-06-25T14:25:11Z",
        "tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL",
          "cs.CR"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 331,
        "object_id": "paper:arxiv.2506.20481",
        "created_at": "2025-10-17T07:11:31+00:00",
        "updated_at": "2025-10-17T07:11:59+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.05209": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.05209",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T07:57:37.025Z",
            "data": {
              "session_id": "session_1760687856457_qwhxwod",
              "source_id": "arxiv",
              "paper_id": "2506.05209",
              "start_time": "2025-10-17T07:57:08.441Z",
              "end_time": "2025-10-17T07:57:36.457Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 3,
              "total_elapsed_seconds": 28
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T08:01:01.859Z",
            "data": {
              "session_id": "session_1760688061847_ruc4swf",
              "source_id": "arxiv",
              "paper_id": "2506.05209",
              "start_time": "2025-10-17T08:00:53.024Z",
              "end_time": "2025-10-17T08:01:01.847Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 354,
        "object_id": "interactions:arxiv.2506.05209",
        "created_at": "2025-10-17T07:57:37+00:00",
        "updated_at": "2025-10-17T08:01:06+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.05209": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.05209",
        "url": "https://arxiv.org/pdf/2506.05209?",
        "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text",
        "authors": "Nikhil Kandpal, Brian Lester, Colin Raffel, Sebastian Majstorovic, Stella Biderman, Baber Abbasi, Luca Soldaini, Enrico Shippole, A. Feder Cooper, Aviya Skowron, John Kirchenbauer, Shayne Longpre, Lintang Sutawika, Alon Albalak, Zhenlin Xu, Guilherme Penedo, Loubna Ben Allal, Elie Bakouch, John David Pressman, Honglu Fan, Dashiell Stander, Guangyu Song, Aaron Gokaslan, Tom Goldstein, Brian R. Bartoldson, Bhavya Kailkhura, Tyler Murray",
        "abstract": "Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models.",
        "timestamp": "2025-10-17T07:57:08.752Z",
        "rating": "novote",
        "publishedDate": "2025-06-05T16:21:30Z",
        "tags": [
          "cs.CL",
          "cs.LG"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 353,
        "object_id": "paper:arxiv.2506.05209",
        "created_at": "2025-10-17T07:57:09+00:00",
        "updated_at": "2025-10-17T07:57:44+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.01912": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.01912",
        "url": "https://arxiv.org/pdf/2506.01912",
        "title": "Unconditional CNN denoisers contain sparse semantic representation of\n  images",
        "authors": "Zahra Kadkhodaie, St\u00e9phane Mallat, Eero Simoncelli",
        "abstract": "Generative diffusion models learn probability densities over diverse image\ndatasets by estimating the score with a neural network trained to remove noise.\nDespite their remarkable success in generating high-quality images, the\ninternal mechanisms of the underlying score networks are not well understood.\nHere, we examine the image representation that arises from score estimation in\na {fully-convolutional unconditional UNet}. We show that the middle block of\nthe UNet decomposes individual images into sparse subsets of active channels,\nand that the vector of spatial averages of these channels can provide a\nnonlinear representation of the underlying clean images. Euclidean distances in\nthis representation space are semantically meaningful, even though no\nconditioning information is provided during training. We develop a novel\nalgorithm for stochastic reconstruction of images conditioned on this\nrepresentation: The synthesis using the unconditional model is \"self-guided\" by\nthe representation extracted from that very same model. For a given\nrepresentation, the common patterns in the set of reconstructed samples reveal\nthe features captured in the middle block of the UNet. Together, these results\nshow, for the first time, that a measure of semantic similarity emerges,\nunsupervised, solely from the denoising objective.",
        "timestamp": "2025-10-17T08:05:34.016Z",
        "rating": "novote",
        "publishedDate": "2025-06-02T17:33:34Z",
        "tags": [
          "cs.CV"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 355,
        "object_id": "paper:arxiv.2506.01912",
        "created_at": "2025-10-17T08:05:34+00:00",
        "updated_at": "2025-10-17T08:05:56+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.01912": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.01912",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T08:06:41.496Z",
            "data": {
              "session_id": "session_1760688401427_pa3mstv",
              "source_id": "arxiv",
              "paper_id": "2506.01912",
              "start_time": "2025-10-17T08:06:00.727Z",
              "end_time": "2025-10-17T08:06:41.427Z",
              "heartbeat_count": 8,
              "duration_seconds": 40,
              "idle_seconds": 1,
              "total_elapsed_seconds": 41
            }
          }
        ]
      },
      "meta": {
        "issue_number": 356,
        "object_id": "interactions:arxiv.2506.01912",
        "created_at": "2025-10-17T08:06:42+00:00",
        "updated_at": "2025-10-17T08:07:12+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2509.02534": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.02534",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T15:26:16.501Z",
            "data": {
              "session_id": "session_1760714776435_vgyg8ro",
              "source_id": "arxiv",
              "paper_id": "2509.02534",
              "start_time": "2025-10-17T15:25:44.287Z",
              "end_time": "2025-10-17T15:26:16.435Z",
              "heartbeat_count": 6,
              "duration_seconds": 30,
              "idle_seconds": 2,
              "total_elapsed_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "issue_number": 357,
        "object_id": "interactions:arxiv.2509.02534",
        "created_at": "2025-10-17T15:26:17+00:00",
        "updated_at": "2025-10-17T15:26:38+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2501.11866": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.11866",
        "interactions": []
      },
      "meta": {
        "issue_number": 359,
        "object_id": "interactions:arxiv.2501.11866",
        "created_at": "2025-10-17T16:48:03+00:00",
        "updated_at": "2025-10-17T16:48:05+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.11866": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.11866",
        "url": "https://arxiv.org/abs/2501.11866",
        "title": "Evaluating multiple models using labeled and unlabeled data",
        "authors": "Divya Shanmugam, Shuvom Sadhuka, Manish Raghavan, John Guttag, Bonnie Berger, Emma Pierson",
        "abstract": "It remains difficult to evaluate machine learning classifiers in the absence of a large, labeled dataset. While labeled data can be prohibitively expensive or impossible to obtain, unlabeled data is plentiful. Here, we introduce Semi-Supervised Model Evaluation (SSME), a method that uses both labeled and unlabeled data to evaluate machine learning classifiers. SSME is the first evaluation method to take advantage of the fact that: (i) there are frequently multiple classifiers for the same task, (ii) continuous classifier scores are often available for all classes, and (iii) unlabeled data is often far more plentiful than labeled data. The key idea is to use a semi-supervised mixture model to estimate the joint distribution of ground truth labels and classifier predictions. We can then use this model to estimate any metric that is a function of classifier scores and ground truth labels (e.g., accuracy or expected calibration error). We present experiments in four domains where obtaining large labeled datasets is often impractical: (1) healthcare, (2) content moderation, (3) molecular property prediction, and (4) image annotation. Our results demonstrate that SSME estimates performance more accurately than do competing methods, reducing error by 5.1x relative to using labeled data alone and 2.4x relative to the next best competing method. SSME also improves accuracy when evaluating performance across subsets of the test distribution (e.g., specific demographic subgroups) and when evaluating the performance of language models.",
        "timestamp": "2025-10-17T16:47:18.440Z",
        "rating": "novote",
        "publishedDate": "2025/01/21",
        "tags": [
          "Machine Learning (cs.LG)",
          "Computers and Society (cs.CY)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 358,
        "object_id": "paper:arxiv.2501.11866",
        "created_at": "2025-10-17T16:47:18+00:00",
        "updated_at": "2025-10-17T16:47:45+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2508.17099": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.17099",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T16:48:24.480Z",
            "data": {
              "session_id": "session_1760719704229_8c9hvg9",
              "source_id": "arxiv",
              "paper_id": "2508.17099",
              "start_time": "2025-10-17T16:48:07.076Z",
              "end_time": "2025-10-17T16:48:24.229Z",
              "heartbeat_count": 3,
              "duration_seconds": 15,
              "idle_seconds": 2,
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "issue_number": 360,
        "object_id": "interactions:arxiv.2508.17099",
        "created_at": "2025-10-17T16:48:25+00:00",
        "updated_at": "2025-10-17T16:48:58+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2506.06278": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.06278",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T16:50:29.950Z",
            "data": {
              "session_id": "session_1760719829841_lcyxsfv",
              "source_id": "arxiv",
              "paper_id": "2506.06278",
              "start_time": "2025-10-17T16:50:20.831Z",
              "end_time": "2025-10-17T16:50:29.841Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 361,
        "object_id": "interactions:arxiv.2506.06278",
        "created_at": "2025-10-17T16:50:16+00:00",
        "updated_at": "2025-10-17T16:50:47+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2412.05265": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2412.05265",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:03:06.568Z",
            "data": {
              "session_id": "session_1760720586487_wgubarn",
              "source_id": "arxiv",
              "paper_id": "2412.05265",
              "start_time": "2025-10-17T17:02:37.421Z",
              "end_time": "2025-10-17T17:03:06.487Z",
              "heartbeat_count": 5,
              "duration_seconds": 25,
              "idle_seconds": 4,
              "total_elapsed_seconds": 29
            }
          }
        ]
      },
      "meta": {
        "issue_number": 363,
        "object_id": "interactions:arxiv.2412.05265",
        "created_at": "2025-10-17T17:02:38+00:00",
        "updated_at": "2025-10-17T17:03:13+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2508.21038": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2508.21038",
        "interactions": []
      },
      "meta": {
        "issue_number": 362,
        "object_id": "interactions:arxiv.2508.21038",
        "created_at": "2025-10-17T16:52:31+00:00",
        "updated_at": "2025-10-17T16:52:33+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2406.10209": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2406.10209",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:05:48.598Z",
            "data": {
              "session_id": "session_1760720748534_94xx7sc",
              "source_id": "arxiv",
              "paper_id": "2406.10209",
              "start_time": "2025-10-17T17:05:39.830Z",
              "end_time": "2025-10-17T17:05:48.534Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 4,
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "issue_number": 364,
        "object_id": "interactions:arxiv.2406.10209",
        "created_at": "2025-10-17T17:05:41+00:00",
        "updated_at": "2025-10-17T17:06:16+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.01844": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.01844",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:08:17.517Z",
            "data": {
              "session_id": "session_1760720897181_v5jpxag",
              "source_id": "arxiv",
              "paper_id": "2507.01844",
              "start_time": "2025-10-17T17:08:09.495Z",
              "end_time": "2025-10-17T17:08:17.181Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 3,
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "issue_number": 366,
        "object_id": "interactions:arxiv.2507.01844",
        "created_at": "2025-10-17T17:08:18+00:00",
        "updated_at": "2025-10-17T17:08:54+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2507.06268": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2507.06268",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:08:03.196Z",
            "data": {
              "session_id": "session_1760720882548_9dxqjhv",
              "source_id": "arxiv",
              "paper_id": "2507.06268",
              "start_time": "2025-10-17T17:06:05.052Z",
              "end_time": "2025-10-17T17:08:02.548Z",
              "heartbeat_count": 23,
              "duration_seconds": 115,
              "idle_seconds": 2,
              "total_elapsed_seconds": 117
            }
          }
        ]
      },
      "meta": {
        "issue_number": 365,
        "object_id": "interactions:arxiv.2507.06268",
        "created_at": "2025-10-17T17:08:04+00:00",
        "updated_at": "2025-10-17T17:08:36+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2504.20879": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.20879",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:12:05.016Z",
            "data": {
              "session_id": "session_1760721124144_5rqhiil",
              "source_id": "arxiv",
              "paper_id": "2504.20879",
              "start_time": "2025-10-17T17:10:54.981Z",
              "end_time": "2025-10-17T17:12:04.144Z",
              "heartbeat_count": 13,
              "duration_seconds": 65,
              "idle_seconds": 4,
              "total_elapsed_seconds": 69
            }
          }
        ]
      },
      "meta": {
        "issue_number": 367,
        "object_id": "interactions:arxiv.2504.20879",
        "created_at": "2025-10-17T17:12:05+00:00",
        "updated_at": "2025-10-17T17:12:35+00:00",
        "version": 1
      }
    },
    "interactions:arxiv.2411.08019": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2411.08019",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-10-17T17:23:51.578Z",
            "data": {
              "session_id": "session_1760721831560_4vsm8nu",
              "source_id": "arxiv",
              "paper_id": "2411.08019",
              "start_time": "2025-10-17T17:23:45.576Z",
              "end_time": "2025-10-17T17:23:51.560Z",
              "heartbeat_count": 1,
              "duration_seconds": 5,
              "idle_seconds": 1,
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "issue_number": 368,
        "object_id": "interactions:arxiv.2411.08019",
        "created_at": "2025-10-17T17:23:52+00:00",
        "updated_at": "2025-10-17T17:24:24+00:00",
        "version": 1
      }
    }
  }
}